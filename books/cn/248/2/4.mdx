---
---
---
title: 数据创造奇迹：量变到质变
---




从某种意义上讲，2005年是大数据元年，虽然大部分人感受不到数据带来的变化，但是一项科研成果却让全世界从事机器翻译的人感到震惊，那就是之前在机器翻译领域从来没有技术积累、不为人所知的Google，以巨大的优势打败了全世界所有机器翻译研究团队，一跃成为这个领域的领头羊。

故事要从这一年的2月说起。全世界拿了美国政府机器翻译科研经费的研究机构，不论是大学还是公司，照例都要参加由美国国家标准与技术研究所（National Institute of Standards and Technologies，简称NIST）主持的测评和交流，而且需要介绍自己研究方法的细节。当然，没有拿美国政府机构的研究团队也可以参加，但是没有义务披露太多的细节。这一年的测评从2月开始，Google的机器翻译团队是第一次参加这个测评，其他的团队要么过去曾经取得过很好的成绩，比如德国亚琛工学院，要么研究的历史非常长，比如IBM和SYSTRAN，因此在测试之前谁也没有关注Google团队的表现。

当年4月，测评的结果出来了，让除了Google以外的所有人大吃一惊，在所有4项测评中，之前从来没有做过机器翻译的Google均比其他研究团队同类的系统领先了一大截。表2.1是2005年NIST评比的结果，表中所给的数据是机器翻译结果和人工翻译结果之间的BLEU分数。关于BLEU分数，简单地讲，它反映了两种翻译结果的一致性，因此这个分数越高越好。当然，并非BLEU分数要达到100%才算翻译完全正确，因为人和人之间的BLEU分数大约只有50%左右。明确了评分标准，我们不妨看两项评比的结果：从阿拉伯语到英语的翻译，Google系统的得分为51.31%，领先第二名将近5%，而提高这5个百分点在过去需要研究5-10年的时间26；而在中文到英语的翻译中，Google 51.37%的得分比第二名领先了17%，这个差距已经超出了一代人的水平。



表2.12005年美国国家标准与技术研究所（NIST）对全世界多种机器翻译系统的评比结果

至于为什么Google能够做到这一点，其中一个原因行业里的人都知道，就是Google花重金请到了当时世界上水平最高的机器翻译专家弗朗兹·奥科（Franz Och）博士。事实上参加测评的系统中，有两个可以说是Google系统的姊妹系统，那就是亚琛工学院的系统和南加州大学的系统，前者是奥科读博士时写的，后者是奥科做研究教授时写的。但是，奥科是2004年7月才正式到Google上班的27，这一年的7月到第二年的2月，奥科也只能赶时间把他过去的工作在Google重新演练一下，根本不可能做实质性的改进。那么为什么Google的系统要比它的姊妹系统好很多呢？

根据NIST的要求，大家在测评结果出来后，一般是在5月到7月之间，要开一次研讨会，交流各自的研究方法。以前，大家的兴趣在于相互讨论，当然在学术界，老朋友们也通过这种方式见见面，联络一下感情。但是这一次大家的目的非常明确，就是看看Google的秘密武器到底是什么。



图2.9 Google翻译的发明人奥科博士

这一年的7月，大家来到NIST所在的弗吉尼亚州北部开会交流经验，奥科则是这次会议的焦点人物。大家都想听他的秘诀，但是这个秘诀一讲出来就不值钱了，他用的还是两年前的方法，但是用了比其他研究所多几千倍甚至上万倍的数据。其实，在和自然语言处理有关的领域，科学家们都清楚数据的重要性，但是在过去，不同研究组之间能使用的数据通常只相差两三倍，对结果即使有些影响，也差不了很多。但是，当奥科用了上万倍的数据时，量变的积累就导致了质变的发生。奥科能训练出一个六元模型，而当时大部分研究团队的数据量只够训练三元模型28。简单地讲，一个好的三元模型可以准确地构造英语句子中的短语和简单的句子成分之间的搭配，而六元模型则可以构造整个从句和复杂的句子成分之间的搭配，相当于将这些片段从一种语言到另一种语言直接对译过去了。不难想象，如果一个系统对大部分句子在很长的片段上直译，那么其准确性相比那些在词组单元做翻译的系统要准确得多。在Google之前，不是没有人想到五元或者六元模型，但是如果没有充足的数据，那么训练出来的五元或六元模型准确性非常差，对翻译没有任何帮助。

从表2.1中可以看到，采用传统人工智能方法的SYSTRAN公司和那些采用数据驱动的系统相比，差距之大已经不在一个时代了，因此从2005年NIST测评之后，它就逐渐退出了历史舞台，如今已经没有多少人知道它了。在那次测评之后，其他大学和研究所则把大部分精力都用到收集数据上了。在第二年的测评中，所有研究组都使用了比前一年至少多100倍的数据，它们和Google的差距迅速缩小。

如今在很多与”智能”有关的研究领域，比如图像识别和自然语言理解，如果所采用的方法无法利用数据量的优势，会被认为是落伍的。

数据驱动方法从20世纪70年代开始起步，在八九十年代得到缓慢但稳步的发展。进入21世纪后，由于互联网的出现，使得可用的数据量剧增，数据驱动方法的优势越来越明显，最终完成了从量变到质变的飞跃。如今很多需要类似人类智能才能做的事情，计算机已经可以胜任了，这得益于数据量的增加。

全世界各个领域数据不断向外扩展，渐渐形成了另外一个特点，那就是很多数据开始出现交叉，各个维度的数据从点和线渐渐连成了网，或者说，数据之间的关联性极大地增强，在这样的背景下，就出现了大数据。