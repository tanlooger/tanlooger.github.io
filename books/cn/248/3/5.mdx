---
---
---
title: 大数据的本质
---




有了信息论这样一个工具和方法论，我们便很容易认清大数据的本质了。首先我们必须承认世界的不确定性，这样我们就不会采用确定性的思维方式去面对一个不确定性的世界。当我们了解到信息或者说数据能够消除不确定性之后，便能理解为什么大数据的出现能够解决那些智能的问题，因为很多智能问题从根本上来讲无非是消除不确定性的问题。对于前面提到的大数据的三个特征，即数据量大、多维度和完备性，我们可以从信息论出发，对它们的重要性和必要性一一做出解释。在这个基础之上，我们就能够讲清楚大数据的本质。

先谈谈数据量的问题。在过去，由于数据量不够，即使使用了数据，依然不足以消除不确定性，因此数据的作用其实很有限，很多人忽视它的重要性是必然的。在那种情况下，哪个领域先积攒下足够多的数据，它的研究进展就显得快一些。具体到机器智能方面，语音识别是最早获得比较多数据的领域，因此数据驱动的方法从这个领域产生也就不足为奇了。

关于大数据多维度的重要性问题，可以从两个角度来看待它。第一个视角是前面提及的“互信息”，为了获得相关性通常需要多个维度的信息。比如我们要统计“央行调整利息”和“股市波动”的相关性，只有历史上央行调整利息一个维度的信息显然是不够的，需要上述两个维度的信息同时出现。第二个视角是所谓的“交叉验证”，我们不妨看这样一个例子：夏天的时候，如果我们感觉很闷热，就知道可能要下雨了。也就是说，“空气湿度较高”和“24小时内要下雨”之间的互信息较大。但是，这件事并非很确定，因为有些时候湿度大却没有下雨。不过，如果结合气压信息、云图信息等其他维度的信息，也能验证“24小时内要下雨”这件事，那么预测的准确性就要大很多。因此，大数据多维度的重要性，也是有信息论做理论基础的。

最后，我们从信息论的角度来看看数据完备性的重要性。在说明这件事情之前，我们还需要介绍信息论里一个重要的概念——交叉熵，这个概念并非由香农提出的，而是由库尔贝克等人提出的，因此在英文里更多地被称为库尔贝克～莱伯勒距离（Kullback～Leibler Divergence），它可以反映两个信息源之间的一致性，或者两种概率模型之间的一致性。当两个数据源完全一致时，它们的交叉熵等于零，当它们相差很大时，交叉熵也很大。所有采用数据驱动的方法，建立模型所使用的数据和使用模型的数据之间需要有一致性，也就是盖洛普所讲的代表性，否则这种方法就会失效，而交叉熵就是对这种代表性或者一致性的一种精确的量化度量。

回过头来讲大数据的完备性。在过去，使用任何基于概率统计的模型都会有很多小概率事件覆盖不到，这在过去被认为是数据驱动方法的死穴。很多学科把这种现象称为“黑天鹅效应”51。在大数据出来之前，这件事是无法避免的，就连提出数据驱动方法的鼻祖贾里尼克也认为，不论统计数据量多大，都会有漏网的情况。这些漏网的情况反映到交叉熵时，它的值会达到无穷大，也就是说数据驱动方法在这个时候就失效了。

怎样才能防止出现很多漏网的情况呢？这就要求大数据的完备性了。在大数据时代，在某个领域里获得数据的完备性还是可能的。比如在过去把全国所有人的面孔收集全是一件不可想象的事情，但是今天这件事情完全能做到。当数据的完备性具备了之后，就相当于训练模型的数据集合和使用这个模型的测试集合是同一个集合，或者是高度重复的，这样，它们的交叉熵近乎零。在这种情况下，就不会出现覆盖不了很多小概率事件的灾难。这样数据驱动才具有普遍性，而不再是时灵时不灵的方法论。

由此可见，大数据的科学基础是信息论，它的本质就是利用信息消除不确定性。虽然人类使用信息由来已久，但是到了大数据时代，量变带来质变，以至于人们忽然发现，采用信息论的思维方式可以让过去很多难题迎刃而解。