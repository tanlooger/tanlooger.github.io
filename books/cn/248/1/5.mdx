---
---
---
title: 数学模型：数据驱动方法的基础
---




在上面统计电影观众分布的例子中，我们大致可以估计出4个年龄组观众的人数分布情况。现在的问题是这个估计是否可信，因为毕竟抽样有很大的随机性。从概率论一诞生人们就有这种担忧，人们希望能够从理论上证明当观察到的数据量足够多了以后，随机性和噪声的影响可以忽略不计。19世纪的俄国数学家切比雪夫（Chebyshev，1821~1894）对这个问题给出了肯定的回答。他给出了这样一个不等式，也称作切比雪夫不等式：



其中X是一个随机变量，E(X)是该变量的数学期望值，n是实验次数（或者是样本数），ε是误差，σ2是方差。这个公式的含义是，当样本数足够多时，一个随机变量（比如观察到的各个年龄段观众的比例）和它的数学期望值（比如真实情况下所有看电影的观众中不同年龄段的比例）之间的误差可以任意小（小于不等式右边的数值）。

将切比雪夫不等式应用到我们这个例子中，我们大致可以计算出4个年龄组的人分别占到观众人数的20%、27%、24%和29%左右，误差小于5%（在统计中也称为置信度大于95%）。但是如果我们要想将4个年龄段观众的准确率提高到小数点后一位数，那么我们大约需要10倍的数据，即两万个左右样本。

用抽样数据来估计一个概率分布是一类非常简单的问题，用统计数据做一做加减乘除即可。但是在大多数复杂的应用中，需要通过数据建立起一个数学模型，以便在实际应用中使用。要建立数学模型就要解决两个问题，首先是采用什么样的模型，其次是模型的参数是多少。

模型的选择不是一件容易的事情，通常简单的模型未必和真实情况相匹配，一个典型的例子就是，无论支持地心说的托勒密，还是提出日心说的哥白尼，都假定行星运动轨迹的基本模型是最简单的圆，而不是更准确的椭圆。由此可见，如果一开始模型选得不好，那么以后修修补补就很困难。因此，在过去，无论在理论上还是工程上，大家都把主要的精力放在寻找模型上。

有了模型之后，第二步就是要找到模型的参数，以便让模型至少和以前观察到的数据相吻合。这一点在过去的被重视程度远不如找模型。但是今天它又有了一个比较时髦而高深的词——机器学习。

鉴于完美的模型未必存在，即使存在，找到它也非常不容易，而且费时间，因此就有人考虑是否能通过用很多简单不完美的模型凑在一起，起到完美模型的效果呢？比如说，是否可以通过很多很多圆互相嵌套在一起，建立一个地心说模型，和牛顿推演出的日心说模型16一样准确呢？如今这个答案是肯定的，从理论上讲，只要找到足够多的具有代表性的样本（数据），就可以运用数学找到—个模型或者一组模型的组合，使得它和真实情况非常接近。

这种思路在现实生活中已经被用到。比如美国和苏联在设计飞机、航天器和其他武器上的理念和方法就不同。苏联拥有大量数学功底非常深厚的设计人员，但是缺乏高性能的计算机和大量的数据，因此其科学家喜欢寻找比较准确但是复杂的数学模型；而美国的设计人员相比之下数学功底平平，但是美国的计算机拥有强大的计算能力和更多的数据，因此其科学家喜欢用很多简单的模型来替代一个复杂的模型。这两个国家做出的东西可谓各有千秋，但从结果来看，似乎美国的更胜一筹。

在工程上，采用多而简单的模型常常比一个精确的模型成本更低，也被使用得更普遍。比如在光学仪器的设计上，一个完美的镜头里面的透镜其实不应该是球面镜，因为那样边缘的图像会变形，只有采用抛物面或者其他复杂曲面，才能使得整个画面都清晰。但是这些非球面透镜的加工需要技艺高超的技工。德国因为拥有最好的技工，因此敢于在镜头设计上采用非球面透镜，这样整个光学仪器就非常小巧。而日本缺乏这种水平的技工，但是善于用机器加工，因此日本人在设计光学仪器时，就用好几个球面透镜来取代一个非球面透镜，这样的光学仪器虽然显得笨重，但是容易大规模生产，而且成本非常低。“二战”后，日本超过德国成为全球光学仪器（包括相机）第一大制造国。

回到数学模型上，其实只要数据量足够，就可以用若干个简单的模型取代一个复杂的模型。这种方法被称为数据驱动方法，因为它是先有大量的数据，而不是预设的模型，然后用很多简单的模型去契合数据（Fit Data）。虽然这种数据驱动方法在数据量不足时找到的一组模型可能和真实的模型存在一定的偏差，但是在误差允许的范围内，单从结果上看和精确的模型是等效的17，这在数学上是有根据的。从原理上讲，这类似于前面提到的切比雪夫大数定律。

当然，数据驱动方法要想成功，除了数据量大之外，还要有一个前提，那就是样本必须非常具有代表性，这在任何统计学教科书里就是一句话，但是在现实生活中要做到是非常难的。我们在后面的章节中将会看到，这在大数据出现之前，其实都没有做得很好。

在今天的IT领域中，越来越多的问题可以用数据驱动方法来解决。具体讲，就是当我们对一个问题暂时不能用简单而准确的方法解决时，我们可以根据以往的历史数据，构造很多近似的模型来逼近真实情况，这实际上是用计算量和数据量来换取研究的时间。这种方法不仅仅是经验论，它在数学上是有严格保障的。

数据驱动方法最大的优势在于，它可以在最大程度上得益于计算机技术的进步。尽管数据驱动方法在一开始数据量不足、计算能力不够时，可能显得有些粗糙，但是随着时间的推移，摩尔定律保证了计算能力和数据量以一个指数级增长的速度递增，数据驱动方法可以非常准确。相比之下，很多其他方法的改进需要靠理论的突破，因此改进起来周期非常长。在过去的30年里，计算机变得越来越聪明，这并非是因为我们对特定问题的认识有了多大的提高，而是因为在很大程度上我们靠的是数据量的增加。

可以用来说明数据驱动方法对机器智能产生作用的最佳案例，恐怕要数2016年在计算机行业最热门的事件——Google的AlphaGo计算机战胜天才围棋选手李世石了。AlphaGo在围棋方面有很高的智能，来源于它对能找到的全部几十万盘人类高手对弈的分析总结。这么多的对弈是任何人类高手一辈子也学习不完的。在总结了几十万盘的数据后，AlphaGo得到了一个统计模型，对于在不同的局势下该如何行棋有一个比人类更为准确的估计。这就是AlphaGo显得很聪明的原因。

关于数据驱动方法，我们在后面的章节里还会详细介绍，它是大数据的基础，也是智能革命的核心，更重要的是，它带来一种新的思维方式。