---
---
---
title: 机械感知
---




在通过化学侦测能力一次解决了觅食、逃命与求偶这三项问题之后，神经系统立刻又面临了更艰难的一道关卡。虽然化学侦测能力可以让神经系统知道附近哪里有食物、周遭是否有掠食者，以及是否存在可能的异性配偶，但是神经系统能不能更快速地侦测到猎物、掠食者与配偶，还有能否更有效率地驱动身体去快速抓到猎物、逃离掠食者，以及趋近配偶，那就另当别论了。换句话说，侦测到目标其实只是基本要求，能不能快速地侦测到目标，以及侦测到目标之后能不能快速地趋近或逃离目标，才是能否在进化中脱颖而出的关键。

这一道进化关卡，又是折煞了无数的生命。直到一种能够增进身体运动功能，并同时能够侦测远端信息的能力出现之后，才终于突破难关。这种侦测能力，就是“机械感知能力”。

机械感知，是通过受器来侦测机械式的能量变化，例如身体中的本体感觉系统可以侦测身体的位置，皮肤上的触觉系统可以侦测接触到的压力变化，内耳前庭的平衡系统可以侦测重力与身体的移动方式。这些机械感知能力多半与身体的知觉与运动协调有关，有利于神经系统更有效率地感知并调节身体的运动。

多数的机械感知能力中，仍然属于近距离感知能力，因为各种机械式的变化大都必须要与身体上的受器近距离直接接触才行。但是其中有一种机械感知能力却因为能“感知到远方事物”而与众不同，这项能力可以侦测空气中的振动能量，并为生物带来极大的生存繁衍优势，此能力就是“听觉”。

听觉

在原始的生存环境中，生物与环境之间的互动，大多是经由肉体直接触碰或是化学物质传递的。在这种近距离接触的互动中，触觉、嗅觉和味觉就足以应付绝大多数的情境。虽然嗅觉和味觉也可以侦测到较远一点的事物以及稍早之前遗留下的味道，但是这两种方式主要还是用在应付周遭的当下变化，而无法快速针对远方或未来的情境提前做出反应，如果想要快速对远距离信息做出判断和反应，上述这些感官知觉就会出现黔驴技穷的局面。

面对着无法取得远距离信息的困境，神经系统也逐渐进化出听觉以及稍后会介绍的视觉这两种“远距离感知能力”，以弥补其他感官知觉的不足。

早期的听觉功能

在进化早期，听觉信息主要是通过毛细胞（Hair Cells）来侦测周遭物质的波动，再把信息传入后脑。此时的听觉信息，主要是用来帮助生物体找出外在掠食者、猎物或其他物体的位置。这种毛细胞后来在两栖类、爬虫类与鸟类身上进化成基底乳头（Basilar Papilla），在人类身上则进化成耳蜗中的柯蒂氏体（Corti）。

早期的听觉信息，主要是用来帮助生物快速逃离危险。在现今的各种生物身上，我们都还可以看到由听觉所诱发的各种反射式逃跑行为。例如有些蛾类在飞行中听到蝙蝠的叫声时，会突然向下坠落以躲避蝙蝠的捕食。有些鱼类和蝌蚪，也会在侦测到水中声波振动时迅速逃开。这些由声音所启动的逃生行为，主要都是通过后脑以及脊索的反射式反应来完成的。

但是，这种由听觉所诱发的反射式反应，显然过于僵化。如果掠食者知道猎物每次听到声音后都只会向下急坠，久而久之，掠食者就会直接往下方进行追击而成功捕获猎物。因此，只靠听觉诱发反射式的反应显然太过死板。

面对这样的生存压力，听觉生物不得不把配备升级，于是乎，听觉信息也开始传入中脑、视丘和大脑皮质，除了通过听觉快速启动逃生行为之外，生物也逐渐发展出利用声音来“听音辨位”以及“分辨声源物”的能力。

作为可能被猎食的对象，听音辨位和分辨声源物是两项非常重要的求生能力。如果可以正确地辨别声音的方位，就可以顺利地往反方向逃离，这样才不会因为盲目逃窜而不小心把自己送入虎口。同样的，正确地分辨声源物，也可以帮助生物分辨声音的来源到底是否真的具有威胁性，如此一来，才不会稍有风声鸟鸣就以为是大敌来袭，也才不至于浪费能量去逃避根本没有危险的声音。

掠食者也同样可以受益于听音辨位和分辨声源物，如果可以通过分析方位和声源物来辨识猎物，就可以更准确地判断猎物的方向，并且事先评估发出声音之猎物值不值得花力气去捕捉。

如何听音辨位？

在描述头颅周围的三度空间位置时，主要涉及了三个不同的轴，第一个是地平经度（Azimuth），第二个是垂直高度（Elevation），第三个是距离（Distance）。地平经度可以用来描述和头部位于同一个水平面上的各种不同方位角。比方说如果我们的头颅面对正北方，那正前方的方位角就是零度，而右手边的正东方就是九十度。垂直高度是用来描述不同高度的水平面。距离则是用来描述在同一个水平面上声源至头颅中央的物理长度。

人类的大脑在辨别声源的方位时，会使用不同的计算方式来推测不同轴上的声源位置。

比方说在计算声源的地平经度时，大脑主要会分析“双耳时间差”以及“双耳强度差”。之所以会有“双耳时间差”以及“双耳强度差”，是因为双耳在空间中的位置约有17厘米的差距，此差异会导致不同地平经度上的声音传到双耳时出现短暂的“时间差”，较靠近声源的耳朵会先接收到声音，而且由于双耳中间有着一个会吸收声波的头颅，因此不同地平经度上的声音传到双耳时也会出现“强度差”，即较靠近声源的耳朵会接收到比较强的声音。大脑在接收到“双耳时间差”以及“双耳强度差”后，就可以据此来反推出声源的位置。

在人类身上，“双耳时间差”以及“双耳强度差”只能用来判断声源的地平经度，但不能用来判断声源的垂直高度，因为声源的垂直高度变化并不会产生明显的“双耳时间差”以及“双耳强度差”。因此，如果想要判断音源的垂直高度，就得依靠不同的计算方法。

那么人类是怎么计算音源的垂直高度呢？人类判断声源垂直高度的方法，其实是通过一个大家意想不到的特殊设计来完成。大家知道自己的耳朵上为什么会有奇形怪状的皱褶吗？我们通常都会说那是因为可以方便收集声音。但是事实上，耳廓上奇怪皱褶的主要功能之一，就是用来判断音源的垂直高度。不同垂直高度的音源，会因为耳廓皱褶的反射而出现不同的变化，大脑就可以借此来判定音源的垂直高度。实验发现，如果我们通过人为的方式改变受试者的耳廓皱褶，他们对音源的垂直高度判断就会出现错误。大家如果有兴趣亲自尝试的话，可以闭上眼睛，并用手扭曲自己的耳廓，然后请朋友在你的头颅上方或下方拍手或弹指，此时你将会发现音源的垂直高度变得很难判断（幸好我们的大脑有很强的认知弹性和学习力，因此只要经过几次试误学习后，我们很快就又能够正确辨别音源的垂直高度）。

虽然人类无法使用“双耳时间差”以及“双耳强度差”来判断音源的垂直高度，但是猫头鹰却可以，因为猫头鹰的两耳高度差异非常的巨大。对猫头鹰来说，判断音源的垂直高度是一项攸关生死的狩猎能力，因此在进化的过程中，它们的双耳在脸上的高低差异已经变得非常明显，这个双耳高低差异，使得不同垂直高度的音源也会产生“双耳时间差”以及“双耳强度差”。例如当猫头鹰的左耳比较低时，下方的音源就会因为比较靠近左耳而比较早到达左耳，而且到达左耳时的音量也会比较强。

关于音源的距离，则是通过声音的大小和声音中含有的高低音频率多寡来判断。比方说，同一个音源距离我们越远时，我们接收到的音量就会越小，而且其中的高频声音也会越少（因为低音频的能量在传递时比较不容易耗损）。

通过这三种简单的听觉定位方式，我们就可建构出一张三维的空间地图。大脑这项强大的认知能力，可以帮助生物快速辨别出远方同类、猎物或掠食者的方位，对于生存繁衍极度重要。

沟通和语言能力

在拥有了基本的听觉感知能力后，听觉生物们很快就面临到一项自我挑战：能否主动使用听觉来进行沟通。如果生物可以主动发出声音让同类听见，那么求偶等沟通信息就会迅速地传递到远方，与嗅觉相比，听觉的信息传递效率可以说是完胜。相反的，如果无法主动发出声音来把信息传递给同类，那么不但不利于求偶，当掠食者出现时，也将会因为无法有效地警告同类而导致全族覆没的下场。

在这样不进则退的进化压力之下，几乎所有拥有听觉的生物都发展出了通过声音来沟通的能力。

比方说节肢动物常使用摩擦鸣声（Stridulation）来彼此沟通，例如蟋蟀可以摩擦翅膀，雄蝉可以摩擦腹部的发音膜，一些甲虫（如独角仙）可以摩擦翅膀和腹部来发声，一些多毛的大蜘蛛也可以通过摩擦脚上的刺毛来发出声音。

在脊椎动物身上，更是可以见到多元的发声沟通方式。比方说，大家可能知道硬骨鱼身上的鱼鳔能用来帮助鱼儿沉浮，而且鱼鳔也很好吃。但是大家可能不晓得，有很多种鱼都可以通过快速收缩鱼鳔来发出声响。还有响尾蛇可以振动尾巴上累积的脱壳来发声，有一些鹤和猫头鹰可以通过快速夹动鸟喙以制造节律（Bill Clacking），而大猩猩则也可以通过击胸来发出巨大声响。

除了以上的特殊发声方式之外，最为人所熟知的应该就是口器发声（Vocalization）。鸟类、蝙蝠、海豹、鲸鱼、猴子等几乎所有的动物，都能够使用口器发声来进行各式各样的沟通，包括求偶、警示、食物定位以及社交学习等。

在使用听觉与口器发声的各种沟通方式中，最令人赞叹的进化极致，应该就是人类的语言能力。人类总共有七千种以上的语言，而更令人震惊的是，只要在适当的时间点让婴儿接触到足够的语言信息，任何一个婴儿都有学会任何一种语言的潜力。语言学家乔姆斯基（Noam Chomsky）认为，这是因为婴儿的大脑天生就内建了一套“普遍语法”（Universal Grammar），根据这套“普遍语法”，婴儿就可以捕捉到或发展出所有人类自然语言中的普遍特质，例如语词内的主词、动词和名词之分。

但是，即使我们承认婴儿的大脑中有普遍语法的存在，但这些普遍语法究竟是怎么帮助婴儿学会语言的呢？我们在第一次听到陌生外语时，听到的感觉就只是一连串字词的无意义声音，婴儿在第一次听到大人说话时，必然也是一样的感觉。究竟婴儿的大脑是如何从这些看似毫无头绪的语音信息中捕捉到规则的呢？

婴儿如何学习语言

最近的研究发现，婴儿大脑的语言学习过程，可能和母语信息中的统计规律息息相关 。比方说，婴儿的大脑似乎会根据母语中各种“音素”（Phoneme）出现的频率来决定哪些音素比较重要，并借此决定该投入多少大脑资源来加以学习。

所谓的“音素”，就是人类可以发出和听到的基本语音元素。婴儿在刚出生时，就已经可以分辨全世界现存的将近八百种音素。但是到了大约六个月和九个月大时，婴儿对元音音素及子音因素的区辨能力就会分别开始“窄化”或“专化”。换言之，婴儿会对自己母语中常听见的音素变得更敏锐，但是对其他不曾听过的语言中的音素就会变得较不敏锐。

而这其中的关键要素，就在于这些音素的出现次数，当某个音素出现的次数越频繁，大脑就越有机会学习和分析该音素，而之后对于该音素的区辨力也就会越强。

同样的，婴儿可能也是通过类似的“统计规律”方法来辨识出一连串语音中每个字词的分隔点。如果大家有机会听到陌生外语，一定会发现一个现象，就是除了完全听不懂意思之外，我们甚至连一连串语音中每个字词的分隔点都抓不到。那为什么在母语中的婴儿，久而久之就会知道每个字词的分隔点在哪呢？

目前的研究显示，婴儿可能就是根据每个音素的“相连概率”来作为判断的标准。

在二十世纪九十年代中期，威斯康星大学麦迪逊分校的语言心理学家萨弗瑞（Jenny Saffran）的团队发现，八个月大婴儿可以通过音节相连的概率来学会类似语言的信息。以“happy baby”这一串语音为例，“hap”这个音节和“py”这个音节很容易在各种说话内容中被连续听到，例如在“happy girl”或“happy dog”这两串语音中也会听到“hap”和“py”这两个音节相连。但相较之下，“hap”和“py”和“ba”这三个音节被连续听到的概率就小很多。久而久之，大脑就会把“hap”和“py”这两个连续语音组合成一个字词：“happy”，而不会把“hap”、“py”和“ba”三个连续语音组合成“happyba”。

在实验中，萨弗瑞让宝宝聆听一连串由计算机合成的无意义语词，这些语词由音节构成，其中有些音节会比较常相连出现。结果发现，宝宝会特别注意到这些虚构语词中时常相连出现的音节，而这种能力就是帮助他们找出可能字词的关键 。

很可惜的是，这段对音素和语言统计规律特别敏感的时期只存在于幼儿大脑中，当我们成年之后再聆听新语言时，就不会再有如此的敏锐度。这也就是为什么长大后才学习第二语言并不容易的原因。

语言的相关脑区

虽然我们目前仍不完全清楚学习语言的认知过程与大脑机制，但是科学家对于语言的相关脑区已经有了初步的理解。

1861年，法国医生布洛卡（Pierre Paul Broca）接触到了一位“无法说话”但却“可以正常理解他人话语”的病人。这位病人名叫勒伯尼（Leborgne），由于他只会发出“唐”（tan）的声音，因此有了“小唐”这样的绰号。小唐过世之后，布洛卡解剖了他的大脑，结果发现他的左脑前额叶下方的运动区域附近受损 。后来布洛卡又解剖了十二位相同症状的病人，也都发现类似的结果。后人因此把这种症状称为“表达型失语症”（expressive aphasia），而这个脑区也因此被后人称为“布洛卡语言区” 。

1974年，德国医生维内基（Carl Wernicke）研究了另外一些“可以顺利说话”但却“无法听懂他人话语”的病人，结果发现他们的左脑颞叶上回的听觉区域附近受损。这种病症后来被称为“接收型失语症”（Receptive Aphasia），而这个脑区后来则被称为“维内基语言区”。

近年来的功能性磁共振造影技术，也更进一步地找出语言的精确相关脑区。比方说，笔者先前在麻省理工学院的同事费多伦科（Evelina Fedorenko）就发现，布洛卡语言区其实可以再被细分成两个不同的子区域，其中一个子区域和语言有关，另一个子区域则和数学与工作记忆有关 。而我们一起合作发表的另一篇论文还发现，和语言有关的脑区并不是只有左脑中的布洛卡区和维内基区，而是还包括了两侧大脑至少十三个区域 。

总而言之，虽然目前仍不完全清楚语言的细部认知过程和确切大脑运作机制，但是愈来愈多的研究已经逐渐开始找出头绪，语言能力秘密大白之日，我们指日可待。