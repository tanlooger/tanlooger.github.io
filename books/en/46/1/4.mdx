---
---
---
title: Chapter 4 By Legislation or Fiat
---



Countrywide credit was the lender at the epicenter of the housing boom, and Angelo Mozilo was the public face of Countrywide. The Bronx-born son of first-generation Italian Americans, Mozilo had gone to work in his father’s butcher shop at the age of twelve and then as a messenger for a Manhattan mortgage lender. By the time he was sixteen he had worked his way up from ferrying paperwork to processing loans, progress testifying either to his exceptional ambition or to the straightforward nature of underwriting in the era of plain-vanilla mortgages.

Mozilo stayed with the same firm through his high school and college years and until it merged with Lomax Realty Securities, headed by industry veteran David Loeb. In the mid-1960s Loeb sent Mozilo to Central Florida, which was in the grips of a real estate boom not unlike that of the 1920s. Observing that the boom was being driven by the influx of space engineers to Cape Canaveral, Mozilo recommended taking a stake in a Brevard County subdevelopment. When the bet paid off, Loeb made Mozilo his sidekick.

When Lomax Securities was bought out in 1968, Loeb and Mozilo set out to create their own mortgage company, which they dubbed Countrywide Credit Industries. Initially the name was indicative more of the partners’ ambition than the reality. The duo worked out of a single office in Anaheim, California, the Inland Empire to the east beckoning as the final frontier. Loeb served as the firm’s strategist, Mozilo as its sales force of one.1

Although the business gained traction, costs showed a troubling tendency to escalate. Recruiting and retaining salesmen required paying generous commissions. Turnover was high, in part because Mozilo was a demanding boss— “a son of a bitch,” as he proudly put it. Loeb therefore proposed eliminating the sales force and using direct advertising to solicit applications, something that had not previously been tried in the mortgage-banking industry. Mozilo, a salesman himself, resisted but eventually agreed to take the plunge.

Attracting business by advertising meant competing on price, which in turn required keeping costs down. The company’s retail offices were standardized, situated in strip malls, and permitted no more than two full-time employees. Gradually the strategy began paying dividends. In the course of the 1970s Countrywide Credit opened four additional offices in California. By 1980 it had forty offices in nine states, no mean feat in a period when mortgage and housing markets were buffeted by interest rates of 20 percent. By the mid-1980s, the 40 offices had grown to 104 and the nine states to twenty-six. By 1992, with nearly 400 branches, Countrywide was the largest mortgage banker in the country and, for that matter, the world.

Reflecting its emphasis on low costs and standardization, Countrywide came to be known as the McDonald’s of mortgage banking. The label reflected the extent to which it successfully reduced the home mortgage to a commodity, the financial equivalent of a hamburger, and the loan officer to the white-collar equivalent of a hamburger flipper. Countrywide was an early adopter of information technology to process applications. By the mid-1990s, fully 70 percent of loans passing through its automated underwriting system required no human intervention. Standardization and the commitment to information technology, together with reliance on temporary employees, allowed the company to ramp up when opportunity beckoned and downsize when demand slackened. Countrywide started reselling the mortgages it originated to Freddie Mac and Fannie Mae almost as soon as the two government-sponsored housing agencies were authorized to purchase mortgages not guaranteed by the US government.2 It diversified into loan servicing, buying the right to service mortgages from other lenders to insulate itself from the ups and downs of loan origination. When interest rates were low, loan origination was big business, but when they were high, prepayments were less common, rendering servicing more profitable. Mozilo referred to this as Countrywide’s “macro hedge.”

Eventually some of these innovations came to be viewed in a less favorable light. That a majority of loan applications were processed without human intervention meant no independent verification of borrowers’ claims of income. Aware that their tenure with the firm was likely to be limited, branch managers focused on originating as many mortgages as possible without due attention to their quality. Loan servicing turned out to provide less insulation from the ups and downs of interest rates and the housing market than Mozilo had posited. But these were problems for the future. By the 1990s, organic expansion had become harder for what was now the largest player in the mortgage banking industry. This led Mozilo to the fateful decision to expand into low-income lending. In 1993 Countrywide launched a program that he ambitiously christened “House America.” Mozilo marketed the initiative as bringing the American dream of home ownership to low-income and minority borrowers. Videos promoting it were narrated by the stentorian Hollywood actor James Earl Jones, who commanded the same vocal authority as William Jennings Bryan.

In practice, House America allowed low-income households to assume a heavier burden of mortgage debt. Countrywide adopted “flexible underwriting practices,” reprogramming its automated underwriting systems to approve mortgages for individuals and households lacking the well-documented employment and credit histories needed to obtain conventional loans. “Flexible underwriting practices” was code for down payments of as little as 3 percent, offered in return for a higher interest rate.

This was not the birth of subprime lending. Credit for that invention goes to Long Beach Savings & Loan, a small Orange County–based thrift that eventually morphed into Ameriquest Mortgage. But now Countrywide, the leading player in mortgage origination, jumped with both feet onto the bandwagon. Countrywide was emblematic of what came to be known as the shadow banking system of nonbank financial institutions engaged in mortgage underwriting, securitization, and other activities that had once been the preserve of banks and savings and loan associations (S&Ls), the descendants of the building and loans of the 1920s. The growth of shadow banking in turn reflected a process of financial liberalization stretching back to the early 1970s, not incidentally the same point in time when Loeb and Mozilo established their California-based underwriting operation.

The Wall Street crash of 1929 and the banking crises that battered the US economy prompted the adoption of a panoply of new regulations affecting the banking and financial system.3 The Glass-Steagall Act separated investment and commercial banking and prevented deposit-taking commercial banks from engaging in security and insurance underwriting. Also in 1933, the Federal Reserve Board adopted “Regulation Q,” prohibiting banks from paying interest on demand deposits (essentially, checking accounts), while placing ceilings on permissible rates on time and savings accounts.4 The Securities and Exchange Act created a government commission to oversee stock and bond markets. The Commodity Exchange Act extended regulatory authority to futures markets. The result appears, from a distance, as a golden age of financial stability. Between the end of World War II and the 1970s, bank failures were rare. Financial institutions specialized in different types of lending. Banks extended corporate and consumer loans. S&Ls engaged in mortgage lending. Each type of institution was overseen by its respective regulator. The stock market rose and fell, as stock markets do. But when it fell, it did not bring down the financial system and the economy with it.

As time passed, the financial establishment grew restive. Memories of the unstable 1930s faded. The thrift industry, enjoying tax and regulatory advantages, gained market share at the expense of the banks. Deposit taking and lending in London—what came to be known as the Eurodollar market—subjected the banks to additional competition.

A further source of competition, whose existence would have profound implications in 2008, came in the form of money market mutual funds. The first of the breed, the Reserve Fund, was created in 1971 by a pair of failed New York financial consultants, Harry Brown and Bruce Bent.5 Money market funds invested in treasury bills and commercial paper, not corporate, consumer, and mortgage loans in the manner of a bank. Free of Regulation Q ceilings, they were able to offer savers a more attractive combination of liquidity and interest than on bank accounts, but without, it should be noted, the protection of deposit insurance.

The innovation was heralded as a significant step in the direction of financial democracy, given the miserly returns available on bank accounts. The MIT economist Paul Samuelson, himself a Nobel Laureate, proclaimed that Bent and Brown similarly deserved a Nobel Prize for their innovation. The founders characterized their achievement more modestly. “I wish I could say that our ‘invention’ resulted from any brilliance on our part,” Brown later remarked, “but it was actually a combination of the threat of starvation and pure greed that drove us to it.”6

The absence of deposit insurance and of any requirement for money market funds to hold reserves as a buffer against risk was justified on the grounds that fund managers invested only in safe assets and managed their shareholders’ money conservatively. One dollar invested in a money market fund would always be worth $1, or so the argument ran. The presumption did not anticipate the tendency for fund managers to move into riskier investments as Regulation Q was relaxed and competition created pressure to boost yields, something that regulators first failed to notice and then were reluctant to address, given an increasingly powerful mutual fund lobby. The presumption that shares in money market funds would never fall below par did not anticipate the failure of Lehman Brothers, Lehman being a consequential issuer of the kind of high-yielding short-term notes that the managers of money funds found irresistible. Commercial banks, concerned about the erosion of their deposit base, had been lobbying for the removal of Regulation Q for years. Their calls gained urgency with this new competition and even more when inflation rose, undermining their ability to compete for savings. Regulation Q ceilings were finally phased out by the ironically entitled Depository Institutions Deregulatory and Monetary Control Act of 1980—ironic since its passage resulted from the Fed’s loss of monetary control.

The abolition of Regulation Q unleashed a cascade of unintended consequences. A first consequence was to intensify the pressure on S&Ls, which had previously been permitted to offer higher deposit rates than other financial institutions.7 To limit the damage, the Garn–St. Germain Act of 1982 allowed S&Ls to engage in a range of commercial banking activities, those related to consumer lending, for example, above and beyond their traditional remit of taking deposits and extending mortgage loans. Among its other provisions was one authorizing the extension of adjustable rate mortgage loans. President Reagan, on signing the bill, called it the first step in a “comprehensive program of financial deregulation.”8 Little did he know.

Garn–St. Germain helped set the stage for the S&L crisis of the 1980s by allowing thrifts to take on additional risk without at the same time doing anything to restrain them. But equally important was how the provision of additional financial services by the thrifts intensified the pressure on the banks.9 Commercial banks had long been frustrated by their inability to underwrite corporate and municipal bonds. Securities markets now having recovered from the 1930s and World War II, big corporate borrowers took to issuing commercial paper and junk bonds. With these instruments offering corporate borrowers new ways of financing themselves, reducing their dependence on bank credit, bank profits were squeezed.10 The big banks that were the big companies’ traditional interlocutors were hurt the most.

At first, money-center banks, with Citibank (the rebranded National City Bank of Chapter 3) in the vanguard, found a new market in syndicated loans to governments in Latin America and Eastern Europe, in an echo of the foreign lending boom of the 1920s. But by the early 1980s these loans had gone bad. For the regulators to insist that the banks acknowledge their losses was not an option, however; doing so would have bankrupted the Federal Deposit Insurance Corporation. Instead the banks were allowed to earn their way back to health, and regulation was loosened to facilitate their efforts. In December 1986, in response to a petition from J. P. Morgan, Bankers Trust, and Citicorp (the holding company parent of Citibank), the Fed creatively reinterpreted the Glass-Steagall Act to allow commercial banks to derive up to 5 percent of their income from investment banking activities. The investment banking activities in question included underwriting municipal bonds, commercial paper and, fatefully, mortgage-backed securities. In 1987, over the opposition of its deregulation-skeptical, soon-to-be-former chairman, Paul Volcker, the Federal Reserve Board authorized several large banks to further expand their underwriting businesses. If ever there was an illustration of how lame-duck status can weaken a Fed chair, this was it. Under Volcker’s successor, the liberalization-minded Alan Greenspan, the Fed then allowed bank holding companies to derive as much as 25 percent of their revenues from investment banking operations.

By the 1990s, then, Glass-Steagall was already weakened. The fatal blow was struck by the merger wave that swept investment banking and brokerage toward the end of the decade. Morgan Stanley, an investment bank, merged with Dean, Witter, Discover & Co., a brokerage and credit card company, in 1997, while the trust company and derivatives house Bankers Trust acquired Alex. Brown & Sons, an investment and brokerage firm. This consolidation of investment houses, brokers, and insurance companies threatened to further disadvantage the banks, which responded by lobbying even more intensely for the removal of remaining restrictions on their operations.

And if lobbying was not enough, there were other ways of forcing the issue. Citicorp moved in 1998 to purchase Travelers Insurance Group, notwithstanding Glass-Steagall provisions requiring it to sell off Travelers’ insurance business within two years. The merger would allow Travelers to market to Citicorp’s retail customers not just insurance but also its in-house money market funds while giving Citicorp access to an expanded clientele of investors and insurance policyholders. Its main shortcoming was its incompatibility with Glass-Steagall.

The chairmen and co-CEOs of the merged company, John Reed and Sanford Weill, mounted a furious campaign to remove Glass-Steagall’s nettlesome restrictions before the two-year window closed. Weill formed an alliance with David Komansky of Merrill Lynch and Phil Purcell of Morgan Stanley to lobby for change. Their arguments received a sympathetic hearing from the Greenspan Fed and also from the White House, in the person of President Clinton’s advisor for financial reform, Gene Sperling, and from the Treasury Department, especially when Lawrence Summers succeeded Robert Rubin as secretary in mid-1999.11 (Rubin left for an advisory position with none other than Citigroup; he started in October.) They were warmly received in the halls of Congress, where bank lobbyists freely roamed. The main opposition came from Phil Gramm, who complained that no big banks or insurance companies made their headquarters in Texas. Weill lobbied Gramm incessantly; in his 2006 autobiography, Weill reports running into Gramm at a dinner party in 2004 where Gramm remarked, “Congress made a mistake. It should have called the new law the ‘Weill-Gramm-Leach-Bliley Act!”12 Glass-Steagall was finally euthanized by Gramm-Leach-Bliley, which repealed residual restrictions on combining commercial banking, investment banking, and insurance underwriting, in November 1999. Weill proudly mounted a four-foot slab of wood on his office wall, etched with his portrait and the words “The Shatterer of Glass Steagall.” Much later, in 2012, reflecting on the crisis, he acknowledged that removal of the Glass-Steagall restrictions had been a terrible mistake.13 The abolition of Glass-Steagall closed a chapter in US financial history. But it was also indicative of a broader deregulatory trend. Other manifestations included the Riegle-Neal Interstate Banking and Branch Efficiency Act of 1994, which repealed prohibitions on cross-state branching and opened the door to mega-banks. Likewise, the Commodity Futures Modernization Act (CFMA) of 2000 eliminated federal and state regulatory oversight of financial derivatives. CFMA relieved issuers of credit default swaps from having to hold reserves against the possibility that they would actually have to make payments to purchasers of those instruments. Credit default swaps (CDS) had been designed to allow investors in mortgage-backed securities to insure themselves against default on the mortgages in the underlying pool. Now, however, CDS were purchased by buyers who did not also purchase the asset against whose default the insurance was written but simply wished to bet against the housing market. The decision in 2000 to relieve issuers of the obligation to hold reserves against liabilities associated with these contracts would have momentous implications for what followed.

And where deregulation could not be achieved by legislation, it proceeded by fiat. The activist chair of the Commodity Futures Trading Commission, Brooksley Born, was forced out in 1999 by a hostile Fed chairman and treasury secretary after recommending against further deregulation of derivatives. The Securities and Exchange Commission (SEC), under the more accommodating Harvey Pitt and William Donaldson, then loosened its rules for the financial reserves that had to be held by the brokerage units of banks.

Nor was deregulation limited to the United States. For many years European countries had assiduously regulated their banks and securities markets. In 1986, British Prime Minister Margaret Thatcher, already famous for her commitment to deregulation, then turned her attention to financial markets. Having previously reduced top tax rates, liberalized labor markets, and sold off much of the public housing stock, Thatcher launched her “big bang” financial reform, reducing regulatory restrictions with the goal of enhancing London’s position as an international financial center.

Meanwhile the European Union moved to create a single, integrated market in merchandise, labor services, and financial capital. Overregulation, according to the widely accepted diagnosis, was to blame for the slow growth and high unemployment plaguing the continent, and the single market, by prying open the door to cross-border competition, was the pivotal reform that might force European states to lighten that crushing regulatory load. The diagnosis that other sectors and activities were suffering from excessive regulation was extended, for better or worse, to financial services. As a result, the Single European Act, with its goal of establishing a continentwide market by 1992, permitted big European banks to expand into neighboring countries.

At first the banks were slow to respond, and regulators were reluctant to let them. This changed, however, with the establishment of the euro in 1999, which removed exchange-rate risk as a deterrent to cross-border business. As the competition to provide financial services intensified, European banks levered up their bets. Banks in Northern Europe, where interest rates were low, found it impossible to resist the higher yields on loans to Southern European banks and investments in Southern European bonds. Southern European banks, for their part, welcomed the cheap finance provided by their Northern European counterparts, using it to make speculative real estate loans and buy the bonds of their sovereigns.

The result was explosive growth of banking in Ireland and across Southern Europe. In some countries, the assets and liabilities of the banking system grew to large multiples of gross domestic product. In Ireland, claims on the banking system, at their peak in 2007–08, reached 400 percent of GDP.14 In Cyprus, the liabilities of the banking system peaked out at an extraordinary eight times national income.15 Thus, no one factor explains the deregulation of banking and financial services. Memories of how banks had collapsed in the 1930s faded with time. Foreign competition created pressure to eliminate restrictions on the range of permissible bank activities. Financial innovation, from development of new lending instruments to establishment of money market mutual funds, undermined the effectiveness of existing regulation. The dilemma for policy makers was whether to extend existing regulation to these new entities and markets or to relax restrictions on banks and other incumbents complaining that the playing field was tilted against them. A range of arguments militated in favor of the latter. Banks pointed to advances in technology making it easier to use data from one business to benefit another. Computers made it possible to share information and products across activities in the manner of Citibank and Travelers Insurance, rendering the regulatory walls separating banking from insurance more irritating and, arguably, less efficient.16 Automated credit-scoring techniques like those pioneered by Countrywide Credit encouraged not just routinization of bank lending but also securitization of mortgages, loans, and credits, creating another argument for allowing lenders to branch into underwriting. Commercial banks could cite the experience of the 1990s, when their limited forays into investment banking enhanced profitability without causing noticeable problems.17

Academics like the aforementioned Paul Samuelson, together with Eugene Fama of Chicago and Robert Merton of Harvard, meanwhile provided theoretical models of the efficiency of freely functioning financial markets. In reality, their models were only an intellectual point of departure. They identified the restrictive conditions under which asset prices incorporate all the information needed for market efficiency. It was not long before researchers had built up a catalog of empirical anomalies that were hard to square with the efficient-markets view. The fathers of this efficient-markets theory may have understood its limitations, but this was not universally true of policy makers and others who made use of it to justify their positions. In particular, the efficient-markets view found a ready reception from the likes of Chairman Greenspan.

But the role of ideology extended beyond the halls of the Fed and the person of its chairman. In 1992, the Democratic Party moved in a business-friendly direction in an effort to regain the political middle ground. Responding to twelve years of Republican control of the White House, a party traditionally opposed to financial deregulation now embraced Bill Clinton’s “third way” of balanced budgets, private-public partnerships, and finance for growth. Political scientists Sandra Suarez and Robin Kolodny emphasize the role of this ideological convergence between Left and Right in setting the stage for financial deregulation. Where the 1992 Democratic Party platform might have been expected to at least express reservations about the concessions extended to financial institutions, it was notably silent on the question of deregulation.18 The Riegle-Neal, Gramm-Leach-Bliley, and Commodity Futures Modernization Acts were all signed into law by a president affiliated with a party that had once, but no longer, opposed deregulation of the financial sector—the same party that was responsible during the presidency of Franklin Delano Roosevelt for putting in place the elements of modern financial regulation. The result of these measures was a massive increase in the size, complexity, and leverage of US financial institutions. After having remained stable for more than two decades, the share of the financial-services industry in GDP more than doubled from 4 percent in the early 1970s to 8.3 percent in 2006.19 Some of this growth was natural recovery from the turbulent 1930s and post–World War II years. It can be seen as the financial sector reasserting its role in helping to allocate resources in a complex modern economy. But the remainder, and especially the breakneck financialization of the years leading up to the crisis, is not adequately explained by standard models of the efficiency advantages of a well-functioning financial sector.

Moreover, the growth of the sector was financed to a considerable extent not with equity—not by banks raising more capital—but with debt. The debt in question was incurred by borrowing for a fixed, typically short term from corporations, mutual funds, state and municipal governments, government agencies, and not least other banks. Large banks had the best access to this so-called wholesale money market.20 Having diversified their business and invested in internal controls, they could argue that they were in the best position to manage the risk of relying on borrowed funds.

Large banks were also in the best position to create the special purpose vehicles used to shift risky assets off balance sheet, minimizing the amount of capital the parent institution had to raise. They were further incentivized to reduce their capital ratios and increase their leverage by the knowledge that they were systemically significant. Because they were too big to fail, they were apt to be bailed out in the event of trouble. This in turn encouraged them to take on additional leverage and risk.

And what was true of banks in the United States was similarly true of banks elsewhere, notably in Europe. Although regulatory preferences and subsidies were also extended to small banks specializing in activities like mortgage lending, in country after country it was the large institutions that expanded their balance sheets and raised their leverage most dramatically.

The extreme cases were broker-dealers like Bear Stearns and Lehman Brothers, whose traditional business was trading securities on behalf of their customers. Historically, these firms had maintained large reserves and limited the riskiness of their investment portfolios. Under pressure from commercial bank competitors, they now moved from one extreme to the other. In 2007 the typical US commercial bank had a leverage ratio on the order of 12 to 1, measured as the unadorned ratio of assets to shareholders’ equity. Lehman Brothers, by comparison, had a leverage ratio of 30, Bear Stearns 33.21 A leverage ratio of 33 meant that a decline in asset values of just 3 percent could wipe out shareholders’ equity and therefore the firm itself if it was forced to acknowledge those losses.22 As subsequent events would reveal, this was a tenuous position for any financial institution.

How this extraordinary situation was allowed to develop became a key question in the wake of subsequent events. The answer starts with the decline of the private partnership model of investment banking. Traditionally, the New York Stock Exchange had banned public listing of investment banks as too risky. Instead, investment houses were organized as private partnerships or closely held corporations owned and operated by a handful of partners whose interests were not easily bought and sold. The partners thus had a stake in the long-term survival of the institution. By tradition, they sat together around a table in the “partners’ room,” literally keeping an eye on one another. Peer pressure and close oversight thus served as deterrents to excessive risk taking.

Over time, technological change—development of expensive new computer technology to process transactions, for example—heightened the advantages of scale and made the private partnership model, where the size of the bank was limited by the capital resources of the partners, problematic. It doesn’t take much effort to imagine whose lobbying caused the ban on public listing to be removed in 1970. (Answer: the investment banks.) Merrill Lynch was the first big broker-dealer to go public in 1971, followed by Bear Stearns, Morgan Stanley, Lehman Brothers, and Goldman Sachs, the four other members of what collectively came to be known as “the Big Five.”

Now the CEO, as head of a public company, and those who worked for him, answered (if at all) to the chief risk officer. Management’s interest in the firm was neither illiquid nor long-term. If their risky bets paid off, they earned enormous bonuses. And if big payoffs today were followed by big losses tomorrow, there was no provision for clawing back yesterday’s bonuses (a practice that regulators and shareholders sought to change only after 2008). In principle, the board of directors, representing the shareholders, was supposed to push back against excessive risk taking. But outside directors had limited information and, in many cases, limited ability to assess it. In practice, no one was watching the store.

Regulators, for their part, were no better positioned to restrain risk taking and leverage. They took their cue from the banks rather than the other way around. The SEC loosened capital requirements for broker-dealers in 2004 in response to similar action by the European Union and lobbying by the Big Five, whose members feared losing ground to their foreign rivals. For thirty years, US broker-dealers had been required to apply what was known as the “net capital rule,” which obliged them, like their commercial bank brethren, to limit their leverage to 12 to 1. The SEC’s 2004 decision now allowed them to use their internal models to estimate, or in practice underestimate, the riskiness of their investments. The broker-dealers reduced their capital cushions accordingly.

The Big Five were also leaders in using special purpose vehicles (SPVs) to shift assets off balance sheet, where they would be free of capital requirements. SPVs were robot firms with no employees or physical location. They existed solely to securitize a bank’s mortgage claims, credit card interest due, and other receivables and to sell the resulting instruments on to other investors. If an SPV was unable to pay interest on its securities because of defaults on the underlying pool of residential mortgages, then that was the security holders’ problem, or so it was argued. The sponsoring bank was not legally obliged to provide additional resources so that the SPV could meet its commitments. This was the rationale for exempting the banks from having to hold capital to back the obligations of their SPVs.

But everyone knew who was at fault when a special purpose vehicle ran off the road. The blame rested not with the vehicle but with its driver, in this case the parent bank. Failure to lend support could therefore damage the parent’s reputation and impair its access to capital markets.23 In the event of defaults on the pool of underlying mortgages, responsibility for making good the difference reverted to the sponsoring financial institution. The off-balance-sheet liability migrated back onto the sponsor’s balance sheet.24 Just why regulators should have allowed a parent firm transferring pools of mortgages to an SPV to hold less capital in this light is, to put it mildly, unclear.25 The growth of SPVs was just one manifestation of the larger process of asset securitization. Rather than holding mortgage loans, student loans, auto loans, and corporate loans on their balance sheets, where they had to be funded, banks pooled their loans and transformed them into securities to be sold on to other investors. The pool was split into tranches, with the senior tranche receiving first claim on the cash flow from the underlying loans. The junior tranches received payment only after the senior tranche was serviced. The resulting securities were known as collateralized debt obligations, or CDOs. The sequential payments were referred to, more prosaically, as the “cash-flow waterfall.” The presumption was that the senior tranche was safe in the absence of extraordinary events. This façade of security allowed the senior tranche to obtain an AAA rating and be sold off to pension funds and insurance companies, whose mandates allowed them to invest only in high-rated paper.

Mortgage securitization was no new phenomenon. As we saw in Chapter 1, the “guaranteed mortgage participation certificates” of the 1920s, where the title or insurance company issuing the mortgage-backed security guaranteed the purchaser a specified return, bore more than a passing resemblance to the senior tranche of the mortgage securitizations of the early 2000s. But now the process achieved a scale and complexity not seen before. CDOs were tranched a second time and transformed into securities known as “CDOs squared.” “CDOs cubed” were not long in following.

CDOs backed by pools of loans then gave way to “synthetic CDOs,” whose payment streams were backed not by actual mortgage loans but by portfolios of credit default swaps. Credit default swaps, recall, are insurance contracts that pay out in case of a specified credit event, like a default on a mortgage bond. In practice they were issued by many of the same investment banks active in the securitization business. And they were backed by nothing more than the promise of the issuer to pay in the event that the default in question occurred.

By 2005, the face value of CDOs exceeded $1.5 trillion by one estimate.26 The “one estimate” qualification is important, since in truth no one really knew the value of CDOs outstanding, much less who held them. Similarly for credit default swaps. One survey conducted by the International Swaps and Derivatives Association suggested that there were $17 trillion of CDS outstanding in 2005. But no one knew for sure. The result was an enormous increase in the flow of credit into US financial markets, and into the housing market in particular. Mortgage and nonmortgage debt had risen in lockstep for three decades.27 Starting in 2000–01, however, nonmortgage debt as a share of GDP leveled off, while the growth of mortgage debt rose explosively. At the peak in 2006, private mortgage debt was more than half again as high as private nonmortgage debt. Something peculiar was evidently happening in mortgage and financial markets.

Associated with this tsunami of finance was a run-up in home prices unlike anything seen since Florida in the 1920s. Housing prices nationwide, adjusted for inflation, had been essentially trendless from the 1950s through the 1990s. Starting in 1999 they shot up, rising by two-thirds in real terms in just seven years.28 As in the 1920s, the increase was strongest in certain frenzied pockets, Florida and this time Arizona and California.29 The bubble then fed on itself, as bubbles do. More home purchases meant higher property prices, which encouraged bank and nonbank lenders to lend against the collateral of more highly valued homes. This meant additional purchases, still-higher prices, and more collateral against which to borrow. Subprime borrowers, with unprecedented access to credit, purchased homes they could afford only if the property appreciated, allowing them to refinance and extract equity from the investment.

A growing number of homes were purchased with little down payment if any, given a fresh coat of paint, and put back on the market. Tales of individuals of modest means buying multiple properties, the hallmark of a speculative market, became widespread. The Discovery Home Channel began broadcasting a program called “Flip That House.” Each episode told the story of an individual or group that purchased a run-down property for little or no money down, gave it a fresh coat of paint, and sold it for a substantial profit. The typical episode glossed over details like closing costs and the need to purchase title insurance, much less the consequences for highly leveraged real estate speculators of a decline in housing prices. Their omission reflected more than the intrinsic limitations of the thirty-minute format.