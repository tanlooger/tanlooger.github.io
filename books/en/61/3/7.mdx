---
---
---
title: CHAPTER 7 THE BEGINNINGS OF HUMAN HISTORY
---







THE EVOLUTION OF HUMAN LANGUAGE




Many features contributed to the unique evolutionary package that is our species. But the previous chapter argued that the most critical was the appearance of symbolic language, which released the new and uniquely potent adaptive mechanism of collective learning. So, to understand when human history really began, we have to understand when and how humans acquired their aptitude for symbolic language.

This is murky territory, for language leaves no direct signs in the fossil record; our attempts to understand the evolution of human language depend on ambiguous hints in the fossil record, padded out with a heavy wadding of theory. Not surprisingly, experts disagree even on the fundamental question of when human language first appeared. Henry Plotkin writes:



Some put it as recently as 100,000 years ago or less, a few put it back beyond two million years from the present, and the majority go for somewhere in the region of 200,000 to 250,000 years ago. It is extremely unlikely to have occurred instantaneously, if one defines instantaneously as either a single miraculous mutation or a period of time less than about 1,000 years… . It most likely was smeared out over tens of thousands, perhaps a few hundred thousand years.1





Currently, it is common to suppose, building on the insights of the linguist Noam Chomsky, that language, like some other distinctive human abilities, depends on the evolution of particular “modules” or “organs” within the brain that contain the programs for particular skills. Human brains, it is argued, have a generalized computing capacity that is extremely powerful. But they also contain specialized modules for language and for many other skills, perhaps including social skills, technological skills, and ecological or environmental knowledge. Such theories are tempting, particularly in the case of language. Human infants acquire language with a speed and fluency that is incompatible with any process of learning by trial and error and that has no parallel among our closest relatives, the chimps. In some sense, it seems, the human capacity for language must be hardwired into our brains, and it must have been wired in quite recently, in evolutionary terms. If so, those interested in hominine evolution must try to explain how a language module evolved.2

Steven Mithen has proposed that a number of once discrete brain modules, some of which may have been present in the earliest hominines, merged quite suddenly—perhaps within the last hundred thousand years—in a sort of linguistic “big bang.”3 But exactly how this might have happened remains unclear. There are other difficulties with the “Swiss army knife” view of the human brain. Human brains are certainly different in significant ways from those of apes (not just in their size), but it has proved impossible to locate a distinct “language” module. Language skills appear to be distributed through many different parts of the mind, and their location differs even between individuals. Language seems to be a product of networks of interactions between different parts of the brain, rather than the work of any one language area.4

In The Symbolic Species, Terrence Deacon has offered an account of the evolution of human language that does not rely on the idea of specialized modules. His argument begins with the use of symbols, the most distinctive feature of human languages. Representations of the external world can exist in three distinct forms. The simplest two depend on the detection of similarities (which Deacon calls “icons”) or correlations (“indices”) between events and things.5 Iconic similarities enable organisms as simple as bacteria to react in one way to all manifestations of warmth or light, and in another way to cold or darkness. On the other hand, Pavlov’s dogs learned that there was a correlation between eating and the sound of bells because the two regularly occurred together. As a result, they linked the two phenomena despite the absence of any iconic similarity. Both these ways of learning depend on one-to-one correspondences between internal and external events. However, “symbols,” the third form of representation, refer not just to the outer world but also to whole collections of icons and indices, so they can be used to create much more complicated inner maps of reality.

But symbolic thinking is tricky. It can be done only if iconic and indexical forms of representation can be held, as it were, in the background, while other parts of the mind distill their conceptual essence into a symbolic form of some kind. According to Deacon, “The problem for symbol discovery is to shift attention from the concrete to the abstract; from separate indexical links between signs and objects to an organized set of relations between signs. In order to bring the logic of token-token relationships to the fore, a high degree of redundancy is important” (p. 402; and see chap. 3, passim). This intellectual maneuver requires a lot of computing power. Deacon’s argument makes clear the size of the hurdle that had to be surmounted before symbolic thinking was possible, and this helps explain why symbolic modes of representation are apparently confined to modern human beings, with their exceptionally large brains.

Large brains are not enough, however. Symbolic language also requires many other intellectual and physiological skills. These include a capacity to quickly make and process symbolic gestures or sounds and to understand rapid sequences of symbolic sounds uttered by others. How and why could such a coherent and complex set of skills develop together in the comparatively short period of a few million years? Deacon’s answer is that they emerged through a process of co-evolution during which hominines evolved to take increasing advantage of rudimentary forms of symbolic communication, while languages themselves evolved to accommodate, with increasing delicacy and precision, the changing abilities and peculiarities of the hominine brain. Such changes probably involved some type of Baldwinian evolution, in which slight behavioral modifications gave a significant reproductive advantage to those individuals most skilled at these new behaviors. That advantage, in turn, would create powerful selective pressures in favor of these skills; in this way, what started out as a purely behavioral development may eventually have been inscribed both in the genetic code of our species and in the deep structures of human languages.6 Rudimentary forms of symbolic communication may have appeared first as a result of minor behavioral changes similar to those observable in modern chimpanzees in experimental situations. But once they became habitual, these new forms of communication may have created new selective pressures by enhancing the reproductive chances of those individuals who were, for genetic reasons, most adept at them.

This discussion suggests that the initial steps toward a symbolic language probably began a long time ago to allow time for the evolution of the many behavioral and genetic changes that have made modern language possible. It also suggests that the first steps required brains little different from those of modern chimps. But these initial steps were probably followed by evolutionary changes whose most evident feature (at least in the fossil record) would have been expansion in the size and importance of the prefrontal cortex, the front part of the brain. Finally, it is only at a later stage in human evolution that we should look for direct evidence of efficient symbolic communication. Deacon’s account of the extreme difficulty of symbolic communication suggests that once that threshold was crossed, we might expect a sudden change in the quality and nature of human communication—something along the lines of Steven Mithen’s linguistic big bang.

The first steps toward symbolic language may have involved a combination of gestures and sounds. Under experimental conditions, chimpanzees can be taught to use signs symbolically, even if their capacity to symbolize remains limited, and australopithecines may have been as competent linguistically as modern chimps.7 But if we could observe australopithecines communicating with each other, we might still be uncertain whether this was really “language.” Deacon explains:



The first symbolic systems were almost certainly not full-blown languages, to say the least. We would probably not even recognize them as languages if we encountered them today, though we would recognize them as different in striking ways from the communication of other species. In their earliest forms, it is likely that they lacked both the efficiency and the flexibility that we attribute to modern language… . The first symbol learners probably still carried on most of their social communication through call-and-display behaviors much like those of modern apes and monkeys. Symbolic communication was likely only a small part of social communication, (p. 378)





If this reconstruction is correct, it suggests that australopithecines had a limited ability to live in a symbolic realm, which may have permitted a modest degree of abstract thought and perhaps even a degree of self-consciousness. However, for the most part, we should assume that australopithecines, like most animals with brains, lived in an experiential world dominated by the sensations of the present moment rather than in a psychic world like that of modern humans, within which we can often conjure up what is not present, including the past and future.8

Studies of Homo habilis skulls show that their brains were not merely larger than australopithecine brains; they were also organized differently. In particular, there are hints of the division of labor between left and right sides that, in modern humans, is reflected in “handedness.” This feature, in common with increased brain size, may reflect selection for improved symbolic ability, since the segregation of functions to different parts of the brain may have increased the ability of the brain to process different types of information in parallel.9 Deacon suggests that other skills related to language may have been present in habilis and later hominines:



Homo habilis and Homo erectus would have had greater motor control [than australopithecines] and probably also exhibited some intermediate degree of laryngeal descent as well [thereby increasing the variety of sounds that could be made]. Homo erectus’ speech might have been somewhat less distinctive as well as slower than modern speech, and the speech of Homo habilis would have been even more limited. So, although their speech would not have had either the speed, range, or flexibility of today, it would have at least possessed many of the consonantal features also found in modern speech. (p. 358)





But we should not exaggerate these skills. The relatively high larynx of all early hominines suggests that they could not produce the range of sounds (particularly vowels) used by modern humans. If they spoke, they probably did so with a limited vocabulary of words dominated by consonants. Gestures may still have carried most of the burden of communication. Because they lacked the ability to manipulate symbols with the speed or dexterity of modern humans, their communication would have been limited and slow by modern standards. Most important of all, we do not yet see in the archaeological record any signs of the significantly enhanced adaptive ability associated with collective learning.

It is during the past 500,000 years or so that we begin to find evidence of a more decisive shift toward symbolic language, combined with increased adaptive creativity. Neanderthals had brains as large as humans (see figure 7.1), but studies of the base of Neanderthal skulls suggest that they, too, lacked the capacity to manipulate sounds in the complex ways demanded by modern human languages. And this, combined with the absence of any other unequivocal evidence for extensive symbolic activity among Neanderthals, leads us to believe that Neanderthals did not use a fully developed form of language, though their presence in parts of Ice Age Eurasia indicates that they did have an enhanced capacity to adapt to new environments. However, the rapid growth in brain size among several distinct species of humans in the past 500,000 years suggests that a process of rapid co-evolution was taking place, in which several distinct abilities crucial to symbolic language were evolving together and quite swiftly. These may have included the descent of the larynx (necessary to make possible more complex manipulation of sounds), increasing lateral specialization within the brain, and increasing ability to control the breath and to recognize and analyze sounds rapidly and precisely.10





Figure 7.1. Neanderthal and human skulls. The skull on the left is Neanderthal (from La Ferrassie); the skull on the right is of a modern human (from Cro-Magnon). Modern genetic evidence suggests that humans and Neanderthals are less closely related than was once thought. From Chris Stringer and Clive Gamble, In Search of the Neanderthals (London: Thames and Hudson, 1993), p. 185.





WHEN DOES HUMAN HISTORY BEGIN?




When do we first get evidence for the existence of humans that not only looked like modern humans but also behaved and communicated with each other like modern humans? This is one of the most important questions that a historian can ask, for it is really a question about the beginnings of human history.

In recent years, two rather different answers have been available. The first is now a minority position, though it is still defended vigorously by some scholars, including Milford Wolpoff and Alan Thorne. They argue that humans evolved slowly toward their modern forms throughout Afro-Eurasia, over almost a million years. Thus all the hominine remains found throughout Afro-Eurasia in the past million years should be treated as examples of a single, evolving species with regional variants, some of whose features, including skin color and facial characteristics, survive to the present day. In this view, regional populations continued to interbreed, so they always remained part of a single species.11 If this account is accurate, we must conelude that human history is perhaps a million years old, though its most distinctive features do not become apparent until more recently. There are several difficulties with this approach, however. Above all, the great variety of fossil remains from the past million years, the huge area they cover, and the probability that few individuals traveled large distances make it difficult to see how we can regard these remains as evidence of a single, evolving species.

A second view, which is currently more popular, is that modern humans appeared more abruptly, somewhere in Africa, between 100,000 and 250,000 years ago.12 The crucial evidence for this conclusion is genetic, though it is also compatible with recent fossil finds. Studies of the genetic material of modern humans show that we vary far less than do neighboring populations of gorillas. This suggests that our species is very young—perhaps only 200,000 years old. If we had been around much longer, there would have been time for much more genetic variety to accumulate both within and between regional populations. Furthermore, most of the genetic variety within modern humans occurs within African populations, which suggests that this is where humans have lived longest. Presumably, then, Africa is where modern humans (Homo sapiens) first appeared. Indeed, this theory suggests that for at least half of our history, modern humans lived exclusively in Africa.

This account of the relatively abrupt appearance of our species fits well with what we know of typical patterns of evolution. Modern humans, like many hominine species, may have evolved by a process known to biologists as allopatric speciation. When populations of a species range over a large area, it is common for some groups to become isolated. They may enter a valley or cross a mountain or a river that then cuts them off from other members of their species. If they cease to interbreed with other populations of their species, they will soon begin to diverge genetically from the parent population. If the isolated population is small, and if the ecological conditions of its new home are very different from its old home, it may diverge rapidly, because selective pressures are strong and favorable genetic changes can spread more swiftly in small populations. Besides, a small population is unlikely, for purely statistical reasons, to be entirely typical of the parent population, and in it such deviations can multiply quickly. (This is known as the founder effect.) For all these reasons, new species often evolve rapidly in small populations living at the edge of the range of a parent species. If this is how our species evolved, then all modern humans are descended from a small and isolated group of ancestors who lived in Africa between 100,000 and 200,000 years ago. If they lived in southern Africa, this would indeed place them at the extreme edge of the range of hominine populations of the Middle Paleolithic (the era from 200,000 to 50,000 years ago).

But there is a problem with this theory, too, for most of its supporters agree that evidence of distinctively modern behaviors, including human language, does not appear before the Upper Paleolithic, which began about 50,000 years ago. Archaeological evidence from Eurasia and Australia suggests that some quite decisive changes occurred in human behavior about 50,000 years ago. The markers that archaeologists have taken as signs of modern human behavior are of four main types. First are new ecological adaptations, such as the entry into new types of environment. Second are new technologies, such as the appearance of small, precisely made, and sometimes standardized blades that may have been hafted, as well as the use of new materials such as bone, all of which would presumably have enhanced the capacity to enter new environments. Third are indications of greater social and economic organization, which show up in evidence for networks of exchange extending over large distances, improved ability to hunt large animals, and evidence of an increased capacity for organization and planning. Fourth, and in some ways most important of all, are indirect signs of symbolic activity, such as the appearance of artistic activity of various kinds, which would have accompanied the use of symbolic language. On the basis of evidence for all these types of change, a number of archaeologists and prehistorians have argued that there was a “revolution of the Upper Paleolithic”: a late, and remarkably sudden, flowering of human creative activity, beginning ca. 50,000 years ago, which marks the true beginning of human history.

But why the apparent gap between the appearance of modern humans and the appearance of modern behaviors? This has remained a tantalizing puzzle. It has tempted some scholars to suppose that critical changes may have taken place in the wiring of human brains within the last 100,000 years; in that case, the real beginning of human history should be put later than the genetic evidence might suggest. Recently, however, two American paleontologists, Sally McBrearty and Alison Brooks, have proposed an elegant resolution of these difficulties, based largely on a close analysis of the archaeological evidence from Africa. Their account dovetails neatly with the account of language origins offered in the previous section, as it seems to demonstrate how a process of genetic evolution of the kind familiar to biologists was transformed, about 250,000 years ago, into a process of cultural evolution of the kind familiar to historians. The next section will be based largely on their revised account of early human history in Africa.13

In “The Revolution That Wasn’t,” McBrearty and Brooks have shown that the abrupt changes apparent in the Eurasian and Australian evidence are not seen in evidence from Africa. Here, they argue, evidence of fully human behavior can be found much earlier than the Upper Paleolithic, perhaps from as early as 250,000 years ago, but it appears piecemeal and gradually. Evidence for the use of small blade tools, some of them hafted, as well as for the use of grindstones and pigments appears very early, while evidence for other innovative technologies—including fishing, forms of mining, long-distance exchanges of goods, the use of bone tools, and migrations into new environments—also can be seen earlier than in Eurasia. Neither cultural nor anatomical changes appear in a “big bang”; instead, they evolve more fitfully.



There was no “human revolution” in Africa. Rather, … novel features accrued stepwise. Distinct elements of the social, economic, and subsistence bases changed at different rates and appeared at different times and places. We describe evidence from the African MSA [Middle Stone Age, ca. 250,000–50,000 BP] to support the contention that both human anatomy and human behavior were intermittently transformed from an archaic to a more modern pattern over a period of more than 200,000 years, (p. 458)





Instead of a revolution of the Upper Paleolithic, what is apparent in Africa is a slow process of change that seems to reflect the “fitful expansion of a shared body of knowledge” over many small groups and large areas (p. 531). And this, they argue, is just what should be expected if modern humans lived in small groups and developed these skills community by community.

Furthermore, they argue, the earliest of these changes coincide with the appearance of a new hominine species, recently dubbed Homo helmei, which is so close to modern humans that it may prove necessary to reclassify its members as belonging to our own species, H. sapiens. Remains that are firmly attributable to H. sapiens are certainly present in Africa by 130,000 years ago, and perhaps as early as 190,000 years ago, but there is no sharp discontinuity between the two species (p. 455). All in all, they maintain that in Africa, unlike Eurasia, the genetic evidence and behavioral evidence combine to offer a coherent account of how our species originated and began to display the ecological creativity that is unique to our species.



Both H. helmei and early members of H. sapiens are associated with MSA technology, and thus it is clear that the main behavioral shift leading to modernity lies at the Acheulian-MSA boundary about 250–300 ka [thousand years], not at the MSA-LSA [Later Stone Age] boundary at 50–40 ka as many assume. We have shown here that many sophisticated behaviors are present in the MSA. This implies increased cognitive abilities with the appearance of H. helmei, and behavioral similarities and a close phylogenetic relationship between H. helmei and H. sapiens. It could be argued that the specimens referred here to H. helmei are more correctly attributed to H. sapiens, and that H. helmei should be sunk into H. sapiens. If that is the case, our species has a time depth of ca. 250–300 ka, and its origin coincides with the appearance of MSA technology, (p. 529)





If McBrearty and Brooks are right, we can say that human history began somewhere between 300,000 and 250,000 years ago in Africa.





AFRICAN ORIGINS: THE FIRST 200,000 YEARS




Before about 100,000 years ago, humans were confined to Africa; but within Africa they pioneered new technologies and lifeways and they occupied new environments, including those of the forests and deserts. Only after ca. 60,000 years ago do humans begin traveling into regions that no earlier hominines had settled, including Australia (which required the ability to cross a significant body of water), Ice Age Siberia (which required the ability to adapt to extremely cold conditions), and eventually the Americas.

Evidence for the earliest (and longest) phases of human history in Africa is tantalizingly thin. In principle, we know that once language appeared, each community had its own history that was rich in epic stories, great names, disasters, and triumphs. But because we cannot see these histories, we have to portray the large trends, forgetting about the details that mattered to individuals. There is little we can do about this except to periodically make the imaginative effort to remember that each community did have its own detailed history, which was as vivid and live to community members as any history constructed today on the basis of written sources.

These generalizations are true of the entire period of human history traditionally referred to as prehistory because of the absence of written sources. But they apply with particular force to the earliest eras of human history. Less archaeological work has been done in Africa than in Europe, dating is tricky, and, as always, trying to explain behavior on the basis of archaeological evidence is difficult. Besides, we should expect that in those early days, processes of collective learning would have worked extremely slowly; we should not be looking yet for spectacular displays of technological virtuosity. As McBrearty and Brooks note, “Early modern human populations in late Middle Pleistocene Africa were relatively small and dispersed, change was episodic, and contact among groups intermittent. This resulted in a stepwise progress, a gradual assembling of the modern human adaptation” (p. 529).

Despite these difficulties, McBrearty and Brooks make a strong case for the appearance in Africa, 250,000 years ago, of all the crucial changes once regarded as evidence for a revolution of the Upper Paleolithic (see figure 7.2). The earliest and clearest signs of new behaviors can be seen in changing stone technologies. Most striking is the disappearance, after 250,000 BP, of the Acheulian stone technologies associated with various forms of H. ergaster. In their place there appear new and more delicate types of stone tools. Some may have been hafted so they could be used as spears or projectiles, an innovation that would have permitted safer and more precise hunting of large animals. Traces of the gums used by modern hunters to hold blades in place have been found on at least one early blade, and many early stone blades are shaped in ways that are consistent with hafting.14 In addition, there are signs of the use of small-scale resources such as fish and shellfish. These are technologies that do not appear outside of Africa until after ca. 50,000 BP.





Figure 7.2. Behavioral innovations of the Middle Stone Age in Africa (duration in thousands of years). Adapted with permission from Sally McBrearty and Alison S. Brooks, ‘The Revolution That Wasn’t: A New Interpretation of the Origin of Modern Human Behaviour,” Journal of Human Evolution 39 (2000): 530.



Humans were also adapting to new environments, in particular to desert and forest regions that no earlier hominines had used.15 Evidence of new forms of social organization and of local “cultures’’ appears in the quite distinct stylistic patterns found in stone implements. There is also evidence of complex patterns of exchange, sometimes over several hundred kilometers. Such behavior suggests that though most of the time humans lived in family groups joined together in small bands, they had occasional friendly contacts with other groups—sometimes over large distances. The creation of such networks (Robert Wright describes them as “giant regional brains”)16 marks a radical break with what we know of the social systems of living great apes. It is tempting to interpret it as indirect evidence of improved forms of communication. More direct signs of modern linguistic skills appear in the form of ornamental objects, as well as grindstones apparently used to grind pigments. Both have been found in Africa well before the Upper Paleolithic. These provide the clearest evidence for the existence of symbolic activity, symbolic thought, and therefore symbolic language.

None of these scraps of evidence is unambiguous, but taken together they help us piece together the earliest stages of the process of collective learning that has culminated, 250,000 years later, in the world we know today. And they suggest that this process was linked directly with the appearance of new species of hominines capable of using symbolic language.





SOME RULES OF COLLECTIVE LEARNING




Symbolic language enabled humans, unlike other closely related species, to share information and to learn together. How did this pooling and sharing of knowledge generate the long-term changes that distinguish the history of humans from that of closely related species? In exploring what is distinctive about human history, we will need to focus, above all, on those factors that determined the pace and geography of processes of collective learning. Why was ecological innovation slower in some eras and faster in others? Why was it slower in some regions and faster in others? If, as I have argued earlier, collective learning is the most important distinguishing feature of human history, we clearly need to keep a close eye on these questions.

In practice, of course, processes of collective learning were as unpredictable as any creative process. But some general rules are worth noticing at the outset, as these will suggest which changes were most likely to accelerate or retard the accumulation of ecologically significant knowledge—the types of knowledge that, over time, have given humans their unique power to manipulate the material world. Two factors stand out: the volume and variety of the information being pooled, and the efficiency and speed with which information is shared.

The first critical factor is the size of information networks, or the number of communities and individuals that can share information.17 Intuitively, we should expect the potential synergy of a network of information exchanges to increase at an accelerating rate as the number and diversity of people exchanging information increase.18 It may be easiest to understand this rule in terms of a model network in which there are a number of nodes (vertices, in graph theory; people or communities, for our purposes), but the total intellectual synergy is proportionate to the number of possible links between those nodes (edges, in graph theory). Then the math is easy. The number of possible links between 2 nodes is 1, the number between 3 is 3, and the number between 4 is 6; in general, if the number of nodes is n, the total number of links is (n × (n–1))/2. In reality, not all connections are made. But the important point is that the number of possible connections (and thereby the potential informational synergy of the entire network) increases faster than the number of nodes, and the difference between the two rates increases as the number of nodes increases. So, as networks expand in size, their potential intellectual synergy increases much faster: “larger and denser populations equal faster technological advance.”19

The variety of information being pooled may be as important as the sheer volume. Neighboring communities living similar lifeways may be able to help each other fine-tune technologies and skills, but they are unlikely to introduce radically new ideas. Fundamentally new forms of information are likely to be shared only where communities living different lifeways come into significant contact. To be sure, differences in lifeways often act as a barrier to contact; but sometimes, as in some forms of trade, they do not. Indeed, where dissimilar groups belong to the same information networks, we are most likely to find processes of collective learning leading to significant changes in technologies and lifeways.

This abstract model suggests that it is important to try to describe the size and variety of information networks—the regions over which information can be exchanged. It also suggests another important principle: as the size and variety of information networks grow, we should expect to find not just an accumulation of new knowledge but an acceleration in the accumulation of new knowledge. And at the most general level, this is exactly what we observe over long periods of human history.

The second critical factor is the efficiency with which information is exchanged. To define the size of a region within which information may be exchanged is one thing. But within that region the speed and regularity of exchanges may vary greatly. The efficiency of information exchanges reflects, above all, the nature and regularity of contacts and exchanges between different communities. And these may be shaped by social conventions, geographical factors, and technologies of communication and transportation. Within a single information network, processes of collective learning may be more or less powerful in different regions; it is thus possible to imagine regions in which more information is pooled, in greater variety and in greater concentrations, than in other regions.

These arguments suggest a useful general principle: the size, diversity, and efficiency of information networks should be an important large-scale determinant of rates of ecological innovation. In subsequent chapters, we will attempt to track the changing synergy of processes of collective learning by examining the size and variety of information networks in different parts of the world, as well as the varying efficiency with which information was pooled within those networks.

In the Paleolithic era, the existence of small groups that had limited contact with each other ensured that exchanges of ecological information worked sluggishly. In a single lifetime, each individual was unlikely to encounter more than a few hundred individuals, and most of that lifetime would have been spent in the company of no more than the ten to thirty individuals who belonged to the same family. The amount of information that could be exchanged in such networks was clearly limited; these limitations help explain what seems to us the glacial slowness of technological change in the Paleolithic era, even though by hominine standards, technological change was actually rapid.

Other factors would also have slowed the pace of change. Societies made up of many small communities tend to display great linguistic diversity. In Aboriginal Australia, a population of several hundred thousand people may have had 200 different languages. Though related to each other, these languages were distinct and only close neighbors could communicate easily with each other. In California as late as 1750, at least 64 and perhaps 80 different languages were spoken, and in Papua New Guinea even today, there are almost 850 living languages.20 Cultural differences would also have limited the exchange of ecological and other forms of information, as would the large distances between neighboring groups in a world in which each group needed a large territory to support itself. All in all, it should be no surprise to find that new technologies and new adaptations evolved slowly in the Paleolithic. And they emerged locally, so the earliest human societies were probably extremely varied: each group made its own adaptive experiments in relative isolation, and opportunities to pool technological discoveries remained limited.





PALEOLITHIC LIFEWAYS




Anyone trying to determine how the earliest humans lived must depend on a lot of guesswork. And studies of modern foraging communities suggest that lifeways varied greatly in their details from group to group. Still, there are some broad generalizations we can make with considerable confidence.21 The small number of fossil remains, combined with what we know from observations of modern foragers, makes it certain that the number of early humans was small and that they lived in small communities. How small we really cannot know. But it seems a reasonable guess that for some time, human populations were similar to those of modern chimps, with perhaps significant fluctuations both up and down.

We can be sure that groups were small because all modern foraging technologies require large areas of land to support small populations. In early Holocene Europe, for example, foraging lifeways could support population densities of up to one person to every 10 square kilometers, while early forms of farming could support between fifty and one hundred people in the same area.22 We have no reason to think that Paleolithic communities were any more efficient in this respect. Modern foragers are mostly nomadic, moving to different parts of their home territory in different parts of the year. Their diet normally depends largely on gathered foods, including plants, nuts, tubers, and small animals of various kinds. In addition, most hunt larger animals and highly value their meat, even though catching it is uncertain; as a result, smaller, more reliable forms of food usually make up the basis of their diet. Living a foraging life requires immense knowledge about available resources, about the migration patterns of birds and animals, and about the life cycles of particular plants, so it would be a mistake to underestimate the ecological skills of such communities.

How well did people live in the Paleolithic? A modern city dweller transported into this world would not find things easy, but the once-popular assumption that the lives of foragers were intrinsically harsh is exaggerated. It is probably equally true that a citizen of Paleolithic Siberia transported suddenly into the twenty-first century would find life hard today, if in different ways. In a deliberately provocative essay published in 1972, the anthropologist Marshall Sahlins describes the world of the Stone Age as “the original affluent society.” He argues that an affluent society is “one in which all the people’s material wants are easily satisfied,” and he suggests that by some standards, Stone Age societies met this criterion better than do modern industrialized societies.23 He points out that affluence can be achieved either by producing more goods to fulfill more desires or by limiting one’s desires to what is available (the “Zen road to affluence”). Using modern anthropological data to gain some insight into the life experience of Stone Age societies, he accepts that levels of material consumption were undoubtedly low among Stone Age peoples. Indeed, nomadism, by its very nature, discourages the accumulation of material goods, for the need to carry what one owns limits any desire to accumulate material possessions. Studies suggest that modern nomadic societies may also deliberately check population growth using many different methods, including prolonged breast-feeding of children (which inhibits ovulation) and more brutal techniques, such as the abandonment of excess children or of older members no longer capable of moving with the rest of the community. In all these ways, foraging communities may have limited their needs.

Nevertheless, Sahlins argues that normal levels of consumption in such communities were more than adequate to supply basic needs. Capable of exploiting an extremely wide range of foodstuffs, foragers in all but the harshest of regions rarely suffered from serious shortages. And nomadism in small groups provided variety and freedom from the diseases characteristic of larger, sedentary communities. Even more strikingly, attempts by anthropologists to assess how much time modern foragers spend “working” for a living suggest that far from toiling desperately just to stay alive, they work less than most wage earners or household workers in modern industrial societies. Studies of traditional communities from Arnhem land showed that “people do not work hard. The average length of time per person per day put into the appropriation and preparation of food was four or five hours. Moreover, they do not work continuously. The subsistence quest was highly intermittent. It would stop for the time being when the people had procured enough for the time being, which left them plenty of time to spare.”24 Here, there was plenty of what we are tempted to call “leisure” time. Researchers studying other modern communities of foragers have come to similar conclusions. And, given that today’s foragers have generally been driven away from regions of greatest abundance, there is little doubt that those of the Upper Paleolithic would, if anything, have spent an even smaller proportion of time working. There have been many attempts to sketch changes in work patterns as societies have increased in size from the Paleolithic to the modern day. In summary, these suggest that daily work time for adult males and females has increased, on average, from ca. 6 hours in foraging societies to ca. 6.75 hours among horticulturalists to ca. 9 hours among intensive farmers, falling to slightly less than 9 hours for modern industrialized urban dwellers. Total time spent on “housekeeping” has increased as dwellings have become more permanent, containing more goods, but the proportion of housekeeping done by men has decreased as societies have become larger. On the other hand, time spent making and repairing household goods has decreased as households have begun to acquire more goods from outside specialists.25

All in all, Sahlins concludes that Stone Age society was a world of abundance, in the sense that most basic desires could be satisfied with a minimum of stress and effort. It may be that Sahlins’s article was a deliberate overstatement, intended to counter a traditional view of human history that could see only progress in the transition from forager to farmer to industrial worker. There is little reason to think that life expectancies in Stone Age societies were much above 30 or 40 years; and undoubtedly many people died in ways that could now have been avoided. But there is no getting around the basic paradox that Sahlins highlighted: the increasing “productivity” of human societies has created societies in which more is desired, and less free time is available to enjoy what is available. Rising productivity levels have supported larger populations, but it would be difficult to prove that they have generated increased levels of human contentment. Humans collectively have got better and better at extracting resources from the environment, but we cannot automatically equate this change with “betterment” or “progress.”

The earliest humans probably lived, like most hominines, in family groups of ten to twenty related individuals who traveled together. The family was the community in which most people lived most of the time. Because (being human) they talked to each other, we can also be pretty sure that they thought of those closest to them as “family” or “kin.” All primates live in groups that we can loosely think of as families. But only with the appearance of symbolic language was it possible to share ideas about family and kin. This means that kinship (whether based on ties of blood or ties of convention such as marriage) became the fundamental organizing principle of human social networks in early human history. In his simple but influential model of social structures, Eric Wolf has suggested that “kin-ordered” societies constitute a major type of human community, one that survives in many different forms even in the modern world.26 But family groups rarely lived in total isolation. Like modern families, each was normally part of a network of related communities that periodically met with each other, particularly when supplies of food were plentiful enough to feed large numbers. At such meetings (known in Australia as corroborees), groups probably swapped information and even individuals with other groups that included at least some close relatives. Within these networks, a sense of kinship could define who you were, who you could trust, and who you had to be wary of.

Modern analogies suggest that the Paleolithic sense of kinship was embedded in a distinctively Paleolithic set of economic relations. We can perhaps understand these relationships by imagining a social equivalent of the law of gravity. Humans are intensely social creatures; thus every individual exerts a gentle gravitational pull on every other individual, which is why humans live in groups. But each group also tugs gently at the ideas, the goods, and the people inside neighboring groups. We have seen that even modern chimps (who are also extremely social) exchange valued goods such as meat as a way of cementing relations within their community. Among humans, exchanges of information, goods, and favors of many kinds provide the social gravity that holds close-knit groups such as families together. These exchanges should be thought of not as trade in a modern sense but rather as a form of gift-giving. In the Christian world, Christmas is a modern survivor of such exchanges, in which the gifts themselves (think of socks, ties, and cheap perfumes) are less important than the social relations they symbolize. In such a context, gifts are exchanged not mainly for economic advantage but primarily to maintain good relations. Anthropologists refer to the principle behind such exchanges as reciprocity.27 Reciprocity depends on building up good relations through gift-giving as a sort of insurance for the future. Robert Wright quotes an account of Eskimo life that makes the point well: “the best place for [an Eskimo] to store his surplus is in someone else’s stomach.”28

The opposite of reciprocity is vengeance. Where reciprocity failed to prevent conflict, individuals or families took vengeance for wrongs done to them. After all, in small, stateless communities, if individuals or families did not exact justice, no one else would do it for them. The anthropologist Richard Lee reports a modern example, which hints at what capital punishment may have meant in the Paleolithic world:



/Twi had killed three other people, when the community, in a rare move of unanimity, ambushed and fatally wounded him in full daylight. As he lay dying, all the men fired at him with poisoned arrows until, in the words of one informant, “he looked like a porcupine.” Then, after he was dead, all the women as well as the men approached his body and stabbed him with spears, symbolically sharing the responsibility for his death.29





Large-scale warfare, like large-scale trade, was probably rare in the Paleolithic era. For the most part, exchanges of gifts (and also of the negative gifts of violence and insult) remained personal and “familiar.” Nevertheless, these exchanges played a fundamental role in survival, creating systems of knowledge, alliance, and mutual assistance that embraced many distinct family groups and covered huge areas. And we can be sure that group violence did occur even in Paleolithic societies, as it does within modern families, as well as among modern nonhuman primates.30

Though we cannot be certain, it is likely that social networks were thought of as extending into the nonhuman world. Symbolic language makes it possible to imagine, and to share what is imagined. Such sharing lies at the base of all forms of religious thought. Modern studies of small-group religions suggest that the earliest human communities thought of the entire cosmos as bound into webs of kinship. Totemic thought—the belief that particular families or lineages are related to particular animal species and may return to life in animal form—reflects a sense of close kinship with the animal world that seems to be pervasive in small communities, even today. The supernatural world may also have been seen as a distinct but accessible realm—almost like a separate tribal territory, with whose occupants one could negotiate, fight, or intermarry. This was a realm into which people could travel, certainly at death and often even in life. And when they did so, rituals and symbols of kinship provided a sort of passport between worlds. Modern shamans plead with, negotiate with, and even “marry” supernatural beings in order to pacify them or secure their favor. Above all, they give gifts of food or sacrificed animals to please or pacify the gods, so that reciprocal gift-giving shapes relations with the world of spirits as well as humans. The relationship between kinship thinking and religion survives even in the great modern religions, which often describe transcendental beings as parents or ancestors, to whom one must give gifts or “sacrifices” as marks of respect. But in relatively egalitarian communities, it seems that the world of the gods, too, was thought of as egalitarian and individualistic. Christopher Chase-Dunn and Thomas D. Hall report that in northern California before European colonization,



there was little hierarchy among the many powers and beings. Many groups believed that Coyote, the trickster, had created the universe. No families or lineages had special relationships with deities or sacred ancestors. Rather, it was the job of each individual to seek out and establish relations with those spiritual forces that were to become his or her special ally. An individual who obtained a great deal of this kind of “power” was more likely to become a shaman, but each person constructed his or her own relationship to the spiritual world. This kind of religious cosmology is quite resistant to claims of seniority or hierarchy.31





In at least one respect, though, it is likely that Paleolithic thinking about the world was very different from the thinking typical of later eras of human history: it was much more specific. People dealt not with “the gods” in general but with this spirit and that magical force, just as their technologies were not generalized but highly specific, concerned with this particular herd of deer, or that particular forest or shoreline. And this characteristic may be why, as far as we can tell, the religions and cosmologies of the Paleolithic world were attached so strongly to particular places.32 Because Paleolithic communities were so small, their thinking about the world lacked the distinctive modern concern with universality and generality. It was particular places that mattered most of all; such places were the source of everything that mattered. Something of this sensibility may be captured in what Hobbles Danaiyarri, from Yarralin in Australia’s Northern Territory, once said to Deborah Bird Rose: “Everything come up out of ground—language, people, emu, kangaroo, grass. That’s Law.”33





“EXTENSIFICATION”: MIGRATIONS OF THE UPPER PALEOLITHIC AND THEIR IMPACTS




The small size of Paleolithic groups and the limited exchanges between them ensured that ecological knowledge accumulated slowly, slowly enough that it is often assumed (wrongly) that there was no technological evolution at all during this period. In fact, though it is not easy for us to see the details, we can be certain that a huge amount of ecological knowledge was accumulating within Paleolithic communities. Indeed, looking back from modern times, it is easier for us than for contemporaries to see that change was occurring, for most of the changes that stand out in retrospect (as opposed to the births, deaths, and other life events that mattered at the time) occurred on scales too large to be noticeable within a single life span.34 Over many thousands of years, the size and diversity of environments occupied by humans within Africa increased. It did so by a process that we can usefully describe using the ugly word extensification, whose complement is the more familiar notion of intensification. Extensification means an increase in the range of humans without any parallel increase in the average size or density of human communities, and consequently with little increase in the complexity of human societies. It involves the gradual movement of small groups into new lands, usually adjacent to and similar to those they have left. Humans moved in this way in part because they had the adaptive flexibility to do so, whereas related species such as chimps lacked the ability to move far beyond the habitats in which they had evolved. As for their motivation to migrate, that may have ranged from social conflicts within the home group to local overpopulation. But it is important to note that extensification leaves the average group size unchanged, even as it may lead to a slow expansion in the range and the total number of modern humans. So, though humans had to constantly make minor adaptations to new habitats, in the course of which they developed the new technologies necessary to live in environments as diverse as tropical forest and arctic tundra, the synergy of collective learning did not increase greatly.

Whatever the cause, and however slow such changes might appear to modern eyes, when repeated many times, over perhaps seven to eight thousand generations and 250,000 years, they eventually led modern humans to settle all continents apart from Antarctica. Beginning ca. 100,000 BP, evidence is found of the presence of modern humans outside of Africa. The first evidence is the appearance of modern human skulls in the Middle East that date to about 100,000 years ago. This means that modern humans lived in the Middle East at the same time as Neanderthals. In this region, at least, members of the two species may even have met each other.35 Like earlier hominines, modern humans would have found it easy to migrate eastward or westward around the Mediterranean or toward Asia, for in southern Eurasia they found environments quite similar to those in Africa.

The first migrations into very different environments were into the continent of Sahul (which included modern Australia and Papua New Guinea), and into the Ice Age steppes and tundra lands of northern Eurasia (see maps 7.1 and 7.2). No earlier hominines had made these migrations; so they provide decisive evidence for the increasing ecological creativity of modern humans. The difficulty of occupying colder northern latitudes is evident in the long time it took modern humans to move from the Middle East to Europe and Inner Eurasia. Modern humans first appear in these regions from ca. 40,000 years ago. Humans were in Ukraine by 40,000 to 30,000 years ago, and had probably settled parts of northern Siberia by 25,000 years ago. Eventually, communities living in eastern Siberia crossed into the Americas—perhaps using boats, or perhaps crossing the land bridge across Beringia that was exposed during the colder parts of the last ice age. We know that humans had entered the Americas by about 13,000 years ago, but there are hints that they may have arrived earlier, possibly as early as 30,000 years ago.

Meanwhile, some humans had made the first significant sea crossing from what is today Indonesia into Sahul. As late as the 1960s, there was no firm evidence of settlement in Australia before 10,000 years ago. But since then, dates for the settlement of Sahul by modern humans have been pushed back in time. Humans had certainly arrived by 40,000 years ago, and they may have arrived earlier. Recent evidence, examined with the new dating technique of thermoluminescence, suggests a date of almost 60,000 years for the occupation of the Malakunanja rock shelter in Arnhem land in Northern Australia, while a skeleton found at Lake Mungo in New South Wales in 1974 has recently been dated at 56,000 to 68,000 BP.36 These are significant dates, because no earlier hominines had managed to settle Sahul. And the reason is clear. Even during the last ice age, when sea levels were lower than today, the journey to Sahul required a sea crossing of at least 65 kilometers. At other times, the distance was at least 100 kilometers. Any humans traveling to Sahul from Timor or the Sula group of islands had to be superb sailors. And they had to be careful planners, for populations that drifted to Sahul by chance would not have been large enough to form permanent colonies. So, settling Sahul required technologies that we do not find in any previous hominine species (see map 7.2). Careful analysis of genetic variations in modern populations confirms the story of migration that is apparent in the fossil record. They show that East Asian and Australian populations diverged more than 50,000 years ago, and Amerindian populations diverged from North Asian populations 15,000 to 35,000 years ago.37





Map 7.1. Extent of glaciation during the ice ages. Data from Neil Roberts, The Holocene: An Environmental History, 2nd ed. (Oxford: Blackwell, 1998), p. 89.





Map 7.2. Migrations of Homo sapiens from 100,000 BP.





As humans moved into these new environments, they had to develop new technologies. Improved control of fire may have been one of the most important of all technological developments in the later Paleolithic. We have seen that some communities of H. ergaster/erectus may have used fire, but only in limited ways. Modern humans put fire to many more productive uses. It provided warmth and some protection against predators. It was also used for cooking, a development that made it possible to process and use foods that might otherwise have been unusable: heating softened the fibers in meats and destroyed the toxins that had evolved as a form of protection in many plant species, from tubers to legumes.38 Fire could also be used to shape entire landscapes, and as a supplement to hunting and gathering. In a famous article, the Australian archaeologist Rhys Jones referred to such techniques as “fire stick farming.”39 Fire stick “farmers” deliberately set fire to bushland in regular cycles. In part, their aim was to prevent buildups of combustible material that could lead to hotter and more dangerous fires. But by clearing away underbrush, fire stick farming also encouraged the growth of new plants that, in turn, attracted browsers that could be hunted. Recent research suggests that such techniques may have been used as early as 45,000 years ago.40 But, at least in temperate zones, they have been used more or less continuously ever since, with a profound effect on entire biota. As Stephen Pyne writes:



hardly any plant community in the temperate zone has escaped fire’s selective action, and, thanks to the radiation of Homo sapiens throughout the world, fire has been introduced to nearly every landscape on earth. Many biotas have consequently so adapted themselves to fire that, as with biotas frequented by floods and hurricanes, adaptation has become symbiosis. Such ecosystems do not merely tolerate fire, but often encourage it and even require it. In many environments fire is the most effective form of decomposition, the dominant selective force for determining the relative distribution of certain species, and the means for effective nutrient recycling and even the recycling of whole communities.41





In some form, the practice can be found in many different parts of the world at the end of the Paleolithic era, and more recently.42 Captain Cook saw the smoke of bush fires while sailing off the Australian coast in the eighteenth century, and Magellan saw huge plumes of smoke off Tierra del Fuego. Modern anthropological research has also revealed a long history of fire use in North America.43 According to I. G. Simmons,



the Beaver Indians of northern Alberta had a sophisticated and delicately tuned approach to creating fire. Certain patches of vegetation were burned deliberately in order to maximize their value as resources. Openings or clearings (“yards”) were created within a forest area and maintained by burning; grass fringes of streams, wetlands, trails and ridges (“corridors”) were similarly created and maintained, for they were both areas where hunted species of animal would either collect or traverse, or both. Fires were also set along traplines, around lakes and ponds, and within large areas of dead fallen trees which otherwise had no resource value; indeed, they were a danger since if ignited in summer they might start off a crown fire, whereas the Indian groups controlled time and place so as to produce only surface fires. So the yards and corridors may well have existed alongside a natural fire-produced mosaic, or could have used natural patterning as a starting point and maintained a version of it.44





So pervasive is the use of fire that the Dutch sociologist Johan Goudsblom has argued that it constitutes the first great technological transition in human history.45

In colder climates, improved hunting techniques were crucial, for while accessible plant foods were scarcer than farther south, there were huge herds of herbivores to be hunted on the Ice Age steppes of Russia, Siberia, and North America. The evidence for new forms of technological creativity is particularly abundant from eastern Europe. In this region, Upper Paleolithic innovations may have included some of the earliest forms of weaving and pottery, technologies that were once thought to have appeared first in the Neolithic era. Sites from the Moravian lowlands, dated to between 28,000 and 24,000 years ago, suggest the use of fired clay and also of weaving, probably to make nets and baskets, as well as simple forms of clothing.46 There is also evidence in eastern Europe in the Upper Paleolithic for improvements in clothing, particularly in northern environments. At Sungir, near Vladimir in Russia, there is a burial dating to ca. 23,000 BP; it contains the remains of a boy and a girl, who wore clothing covered with beads. The position of the beads suggests that the clothes, made from hides and furs, were carefully tailored and well fitting. The girl’s grave is the more elaborate. It contained more than 5,000 beads, many ivory lances, and other carved ivory ornaments. The boy’s grave also contained many beads, as well as a belt made from 250 carved fox teeth, a bracelet, a pendant, several spears, and a mammoth figure carved from ivory Many Upper Paleolithic sites also contain bone needles.47

Dwellings became more specialized. There is particularly striking evidence of systematic and well-planned building from what is today Ukraine and southwest Russia.48 Perhaps most astonishing of all, communities in some regions exploited local resources so efficiently that they became less nomadic. The clearest evidence for the existence of Upper Paleolithic “villages” also comes from Ukraine, where Olga Soffer has studied almost thirty Upper Paleolithic sites. Many have mammoth bones and pits for the storage of frozen meat. Linked to these are other, less permanent sites—on high ground, away from the river valleys—which may have been temporary summer hunting camps. The earliest mammoth bone dwellings date from ca. 20,000 BP, but similar dwellings are present at many sites in the Dnieper basin, usually near river valleys. At Mezhirich, on the river Dnieper, there are large concentrations of mammoth bones, along with carefully prepared hearths and many bone or ivory ornaments. Mammoth bones provided a frame for dwellings partly dug into the ground, and covered over with skins. There were about five dwellings, each about 80 square meters in area and housing up to ten people. The builders used mammoth bones not just for scaffolding but also as “tent pegs,” in preference to wood, which rots more easily. They forced them deep into the ground and cut sockets into which they inserted wooden poles. They also used mammoth bones as fuel, after splintering them.49 These settlements were probably winter camps for groups of perhaps thirty people, who may have occupied them for as long as nine months each year. The relative permanence of these settlements is reflected in the care with which they were built. At the Kostenki 21 site, there were several dwellings along 200 meters of the Don river shore, set 10 to 15 meters apart. One dwelling, near marshland, had an area paved with limestone slabs to avoid the damp. There are also objects that seem to have ritual importance, such as the two musk ox skulls found at Kostenki. Perhaps these were the site of annual gatherings or ritual activities that affirmed the unity of related groups.50 The inhabitants of these Ice Age villages lived off frozen stocks of meat, kept in storage pits and thawed out by fire. The meat, most of which came from gregarious herbivores such as mammoth or bison, was hunted in summer and autumn, when the animals were at their fittest. Each year, some of the inhabitants moved out to temporary summer camps for the hunting season. On returning, they stored meat in pits whose depth suggests they were dug from the top layer of permafrost as it thawed during the brief summers.51

The skills needed to survive in such environments were social as well as technological. In harsh environments, knowledge is as crucial as tools; modern anthropological studies suggest that knowledge was highly valued, and carefully codified and stored in stories, rituals, songs, paintings, and dances. In the Upper Paleolithic there are many hints that information and prestige goods of various kinds were being exchanged—sometimes over huge areas. This does not mean that such exchanges were regular, but it does mean that information could spread extensively, though slowly and fitfully. The astonishing Venus figurines, which appear from the Pyrenees to the river Don at the coldest period of the last ice age, about 20,000 years ago, are a spectacular example of such diffusion. Even more astonishing are the similarities between the cave paintings of southwestern Europe and those of western Mongolia toward the end of the Upper Paleolithic.52 In Sahul, too, there is evidence that goods and ideas could be exchanged over vast areas. The Wilgie Mia ochre mine in Western Australia has been excavated for thousands of years, using technologies including wooden scaffolding, heavy stones to bash the rocks, and fire-hardened wedges to extract the ochre embedded within the rocks. The mine’s red ochre, which may have represented the blood of a Dreamtime being, was traded from Western Australia right across the continent to distant Queensland.53

Technologies that gave early humans access to more and more diverse environments, enabling them to settle on all the world’s major landmasses, imply an increase in the total number of humans. But estimating how human populations grew in the Paleolithic is extremely tricky. Most calculations depend on little more than careful guesswork. And there is a danger, which should be admitted at the outset, that in any deductions from such figures we will merely rediscover the assumptions behind the original guesses. Nevertheless, if such estimates are accurate, even within a wide margin of error, they suggest some clear and important conclusions. Though early human populations were undoubtedly small and probably fluctuated significantly, we have seen that the range of humans expanded markedly within Africa over 150,000 years and more. This expansion in range suggests that the total number of early humans also increased. As noted in chapter 6, genetic evidence hints that the number of modern humans may have fallen dangerously low (perhaps to 10,000 adults) about 100,000 years ago, at the beginning of the last ice age.54 However, the migration of some modern humans out of Africa—first into the Middle East and then, beginning ca. 50,000 years ago, into central and northern parts of the Eurasian land-mass, as well as to East Asia and Australia—must imply a significant increase in human numbers after that date. The harsh conditions of the later stages of the last ice age may have slowed growth, but the spread of humans into entirely new environments such as Siberia and the Americas presumably had the opposite effect, at least on a global scale. One indirect sign of population growth is the increasing number of settlement sites from the Upper Paleolithic: in the lands from north of the Black Sea to the northern ice sheets, only six Neanderthal sites have been found, but more than 500 sites date from the period after 50,000 years ago.55 The Italian demographer Massimo Livi-Bacci proposes a global figure of “several hundred” thousand for Upper Paleolithic populations of ca. 30,000 years ago, and a figure of ca. 6 million at the end of the last ice age, almost 12,000 years ago (see tables 6.2 and 6.3).56

If we take these three figures—10,000 at the beginning of the last ice age, a guess of ca. 500,000 early in the Upper Paleolithic, and another guess of 6 million at the end of the last ice age, 10,000 years ago—we can calculate some approximate growth rates for early human populations. Taken at face value, these figures imply that human populations multiplied by a factor of ca. 1.006 every century from 100,000 to 30,000 BP, a rate that yields a doubling time of ca. 12,500 years. In the period from 30,000 to 10,000 BP, world populations grew at a factor of ca. 1.013 every century, yielding a doubling time of ca. 5,600 years.

These growth rates are rapid by comparison with those of any other large mammal. Yet they are slow by the standards of later human history. Table 6.3 shows that on these figures, the average doubling time for populations in the agrarian era fell to about one-sixth of what it was in the late Paleolithic era. In the modern era, the average doubling time has fallen again, to about one-eighth of what it was in the agrarian era. One way of getting a general feeling for the difference between these eras is by estimating average population densities. The total land surface of the earth (including Antarctica) is ca. 148 million square kilometers. Dividing world populations at different eras into this figure, we get a notional average population density of 1 person for every 25 square kilometers in 10,000 BP. In 5000 BP, the same average area would have contained ca. 8 people; in 2000 BP, ca. 42 people; in 1800 CE, ca. 160 people; and today, ca. 1,013 people. This is just one way of saying that since the end of the Paleolithic era, world populations have multiplied a thousand times, from 6 million to about 6 billion. As this chapter has shown, this astonishing change began deep in the Paleolithic era, with the first migrations into new terrain within Africa.





THE HUMAN IMPACT ON THE BIOSPHERE




Though they may seem crude to modern humans, the technological skills that made this expansion possible imply a marked increase in human ecological control. That increase was enough to have a significant impact on Paleolithic environments. Fire stick farming offers the most spectacular example, for it seems that the regular firing of landscapes over many thousands of years could transform large areas, sometimes in fundamental ways.57 In Australia, fire-loving species such as eucalypti multiplied under a regime of fire stick farming while other species declined; thus the eucalyptus-dominated landscapes that European migrants took for the “natural” landscape of Australia were, in fact, as much a human artifact as the landscaped gardens of eighteenth-century Britain.

Another important way in which Paleolithic communities began to shape their surroundings was by driving other species to extinction. Improved hunting techniques and the increasing use of fire may both have played a role here, as did the spread of humans into new environments. Particularly threatened were many large species, or megafauna: large mammals, reptiles, and birds that reproduced slowly and were therefore more vulnerable to sudden population declines. Mammoths, woolly rhinoceros, and giant Irish elk vanished in northern and inner Eurasia; horses, elephants, giant armadillos, and sloths vanished in North America.58 In Australia many genera of large marsupials vanished, including the Diprotodon, a wombat-like creature about 2 meters high (see figure 7.3). And they seem to have vanished within 10,000 years of the first arrival of humans.59 As Darwin’s collaborator, Alfred Wallace, noted as early as 1876, the extinctions occurred with varying degrees of intensity in much of the world, from the Pacific to Eurasia to the Americas: “We live in a zoologically impoverished world, from which all the hugest, and fiercest, and strangest forms have recently disappeared; and it is, no doubt, a much better world for us now that they have gone. Yet it is surely a marvelous fact, and one that has hardly been sufficiently dwelt upon, this sudden dying out of so many large Mammalia, not in one place only but over half the land surface of the globe.”60

Scientists have long debated the relative importance in these extinctions of climatic change and human overhunting. Both may have played a role, but as we begin to date the extinctions more precisely, the evidence is mounting that the main extinctions, certainly in newly colonized regions such as Siberia, Australia, and the Americas, coincide with the arrival of humans.61 This is where the extinctions were most severe. Australia and the Americas may have lost 70 to 80 percent of all mammal species over 44 kilograms in weight; in Europe, about 40 percent of megafauna disappeared, and in Africa only ca. 14 percent.62 In recent times, too, species were particularly vulnerable in environments such as the Pacific islands, whose animals had no previous experience of dealing with humans. The absence of any sign of similar rates of extinction in previous periods of rapid climatic change during the Pleistocene also supports the claim that human activity is implicated. Whatever the cause, the removal of most large mammal species from Australia and the Americas was to prove momentous. By eliminating several species that might eventually have been domesticable, it may have slowed or prevented the emergence of agriculture in these huge regions, as well as depriving them of a major potential energy source.63





Figure 7.3. Extinct (and dwarfed) Australian megafauna: shadow drawings of Australia’s lost and dwarfed fauna. The human hunter at left gives some idea of their size. From Tim Flannery, The Future Eaters: An Ecological History of the Australasian Lands and People (Chatswood, N.S.W.: Reed, 1995), p. 119; courtesy of Peter Murray.



There is a sad and striking end to the story of Paleolithic extinctions. Those species driven to extinction by the spread of modern humans probably included the last remaining hominines who were not members of our species. Neanderthals, as we have seen, had brains as large as those of modern humans, and they were creative enough to settle in cold regions of modern Russia and Europe that no earlier hominines had occupied. But they apparently lacked the technological creativity of modern humans, presumably because they lacked a developed symbolic language. In the Middle East, modern humans were present at the same time as Neanderthals; moreover, in this region modern humans seem to have used tools similar to those of their Neanderthal neighbors. But the two species used similar tools in different ways. Studies of the bones of prey species left by modern humans show that most animals were taken either in summer or winter, while those from Neanderthal sites were taken throughout the year. In other words, modern humans were probably moving around more, and taking prey more selectively, while Neanderthals were occupying the same site year-round. These subtle differences may point to more profound differences between the two groups. The greater mobility of modern humans suggests that different groups had more contact with each other, and may have shared information more widely, while Neanderthal groups and individuals remained more isolated from each other. Among modern foraging communities, particularly in colder regions (similar, perhaps, to those of the Middle East during the last ice age), information sharing between different groups can be vital to survival. At the same time, groups that are more self-sufficient and less mobile may be more vulnerable to sudden ecological crises. Such groups, with their less efficient hunting methods, may also have to expend more physical energy in order to survive. That need may explain why Neanderthals seem to have been so stocky; their hunting relied more on individual strength than on collective cunning.64

Over time, these differences told, as modern humans spread more widely and eventually migrated into regions occupied by Neanderthals. One such region may have been the south of France, which probably had the densest populations of any region of Upper Paleolithic Europe late in the last ice age (which may be why it also contains 80 percent of Europe’s cave art).65 In France, there is evidence that Neanderthal communities survived during most of the last ice age and may have tried to borrow some of the new technologies of their neighbors. But they had little success with it. The last Neanderthals perished somewhere in southwest Europe, 25,000–30,000 years ago. It is just possible that a similar story was played out at about the same time at the eastern end of the Eurasian landmass as well, as evidence has emerged that other hominine populations may have survived there as late as Neanderthals, vanishing perhaps 50,000 or even 27,000 years ago.66

Even in the Paleolithic, the ecological virtuosity of modern humans had both destructive and creative sides. The migrations of Paleolithic humans, their cave art, and their technological skills rightly win our admiration; but the elimination of so many other large animals, including the only surviving species of hominines, is a powerful reminder of a more deadly side to human history.





SUMMARY




Recent research suggests that modern humans, equipped with a symbolic language and the capacity for collective learning, appeared in Africa about 250,000 years ago. Gradually, community by community, humans evolved new technologies and learned to live in new environments. Beginning ca. 100,000 years ago, humans began to migrate out of Africa and into lands no earlier hominines had settled, lands whose occupation required entirely new ecological skills. The continent of Sahul was occupied between 60,000 and 40,000 years ago; Ice Age Russia and Siberia were occupied from ca. 30,000 years ago onward; and the Americas were certainly occupied by migrants from Siberia by 13,000 years ago, and perhaps much earlier. As humans spread, they began, for the first time, to have a significant impact on the biosphere, transforming landscapes with fire and hunting a large number of Pleistocene megafauna to extinction. By the end of the last ice age, ca. 10,000 years ago, humans occupied all habitable parts of the world except the many islands of the Pacific. They had also driven the only other surviving hominines to extinction.





FURTHER READING




The early history of our own species is complex territory, and riddled with controversy. There are several good general surveys, including Peter Bogucki, The Origins of Human Society (1999); Göran Burenhult, ed., The Illustrated History of Humankind (5 vols., 1993–94); Roger Lewin, Human Evolution (4th ed., 1999); Ian Tattersall, Becoming Human (1998); Richard Klein, The Human Career (1999); Luigi Luca and Francesco Cavalli-Sforza, The Great Human Diasporas (1995); Chris Stringer and Robin McKie, African Exodus (1996); and Robert Wenke, Patterns in Prehistory (3rd ed., 1990). This chapter relies heavily on a superb recent article by Sally McBrearty and Alison Brooks, “The Revolution That Wasn’t” (2000), but it is too early to know if this account will achieve general recognition. The early history of language is equally controversial. Aspects of the current debates on the subject are covered in books by Terrence Deacon (The Symbolic Species [1997]), Steven Mithen (The Prehistory of the Mind [1996]), Henry Plotkin (Evolution in Mind [1997]), John Maynard Smith and Eörs Szathmáry (The Origins of Life [1999]), and Steven Pinker (The Language Instinct [1994]). Clive Gamble’s Timewalkers (1995) is one of the best recent surveys of Paleolithic history, with a strong focus on changing social relations and networks. Tim Flannery’s The Future Eaters (1995) is a superb, if controversial, book on early ecological impacts of humans in Sahul; his more recent work, The Eternal Frontier (2001), discusses the ecological history of North America. The work of Olga Soffer (see the articles listed in the bibliography) is fundamental for understanding the settlement of Ice Age Russia. The Cambridge Encyclopedia of Human Evolution (1992), edited by Steven Jones et al., is also useful for many details in this chapter.