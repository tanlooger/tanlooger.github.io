---
---
---
title: CHAPTER 10: GETTING THE LEAD OUT
---




IN THE LATE 1940s, a graduate student at the University of Chicago named Clair Patterson (who was, first name notwithstanding, an Iowa farm boy by origin) was using a new method of lead isotope measurement to try to get a definitive age for the Earth at last. Unfortunately all his samples came up contaminatedusually wildly so. Most contained something like two hundred times the levels of lead that would normally be expected to occur. Many years would pass before Patterson realized that the reason for this lay with a regrettable Ohio inventor named Thomas Midgley, Jr.

Midgley was an engineer by training, and the world would no doubt have been a safer place if he had stayed so. Instead, he developed an interest in the industrial applications of chemistry. In 1921, while working for the General Motors Research Corporation in Dayton, Ohio, he investigated a compound called tetraethyl lead (also known, confusingly, as lead tetraethyl), and discovered that it significantly reduced the juddering condition known as engine knock.

Even though lead was widely known to be dangerous, by the early years of the twentieth century it could be found in all manner of consumer products. Food came in cans sealed with lead solder. Water was often stored in lead-lined tanks. It was sprayed onto fruit as a pesticide in the form of lead arsenate. It even came as part of the packaging of toothpaste tubes. Hardly a product existed that didnt bring a little lead into consumers lives. However, nothing gave it a greater and more lasting intimacy than its addition to gasoline.

Lead is a neurotoxin. Get too much of it and you can irreparably damage the brain and central nervous system. Among the many symptoms associated with overexposure are blindness, insomnia, kidney failure, hearing loss, cancer, palsies, and convulsions. In its most acute form it produces abrupt and terrifying hallucinations, disturbing to victims and onlookers alike, which generally then give way to coma and death. You really dont want to get too much lead into your system.

On the other hand, lead was easy to extract and work, and almost embarrassingly profitable to produce industriallyand tetraethyl lead did indubitably stop engines from knocking. So in 1923 three of Americas largest corporations, General Motors, Du Pont, and Standard Oil of New Jersey, formed a joint enterprise called the Ethyl Gasoline Corporation (later shortened to simply Ethyl Corporation) with a view to making as much tetraethyl lead as the world was willing to buy, and that proved to be a very great deal. They called their additive ethyl because it sounded friendlier and less toxic than lead and introduced it for public consumption (in more ways than most people realized) on February 1, 1923.

Almost at once production workers began to exhibit the staggered gait and confused faculties that mark the recently poisoned. Also almost at once, the Ethyl Corporation embarked on a policy of calm but unyielding denial that would serve it well for decades. As Sharon Bertsch McGrayne notes in her absorbing history of industrial chemistry,Prometheans in the Lab , when employees at one plant developed irreversible delusions, a spokesman blandly informed reporters: These men probably went insane because they worked too hard. Altogether at least fifteen workers died in the early days of production of leaded gasoline, and untold numbers of others became ill, often violently so; the exact numbers are unknown because the company nearly always managed to hush up news of embarrassing leakages, spills, and poisonings. At times, however, suppressing the news became impossible, most notably in 1924 when in a matter of days five production workers died and thirty-five more were turned into permanent staggering wrecks at a single ill-ventilated facility.

As rumors circulated about the dangers of the new product, ethyls ebullient inventor, Thomas Midgley, decided to hold a demonstration for reporters to allay their concerns. As he chatted away about the companys commitment to safety, he poured tetraethyl lead over his hands, then held a beaker of it to his nose for sixty seconds, claiming all the while that he could repeat the procedure daily without harm. In fact, Midgley knew only too well the perils of lead poisoning: he had himself been made seriously ill from overexposure a few months earlier and now, except when reassuring journalists, never went near the stuff if he could help it.

Buoyed by the success of leaded gasoline, Midgley now turned to another technological problem of the age. Refrigerators in the 1920s were often appallingly risky because they used dangerous gases that sometimes leaked. One leak from a refrigerator at a hospital in Cleveland, Ohio, in 1929 killed more than a hundred people. Midgley set out to create a gas that was stable, nonflammable, noncorrosive, and safe to breathe. With an instinct for the regrettable that was almost uncanny, he invented chlorofluorocarbons, or CFCs.

Seldom has an industrial product been more swiftly or unfortunately embraced. CFCs went into production in the early 1930s and found a thousand applications in everything from car air conditioners to deodorant sprays before it was noticed, half a century later, that they were devouring the ozone in the stratosphere. As you will be aware, this was not a good thing.

Ozone is a form of oxygen in which each molecule bears three atoms of oxygen instead of two. It is a bit of a chemical oddity in that at ground level it is a pollutant, while way up in the stratosphere it is beneficial, since it soaks up dangerous ultraviolet radiation. Beneficial ozone is not terribly abundant, however. If it were distributed evenly throughout the stratosphere, it would form a layer just one eighth of an inch or so thick. That is why it is so easily disturbed, and why such disturbances dont take long to become critical.

Chlorofluorocarbons are also not very abundantthey constitute only about one part per billion of the atmosphere as a wholebut they are extravagantly destructive. One pound of CFCs can capture and annihilate seventy thousand pounds of atmospheric ozone. CFCs also hang around for a long timeabout a century on averagewreaking havoc all the while. They are also great heat sponges. A single CFC molecule is about ten thousand times more efficient at exacerbating greenhouse effects than a molecule of carbon dioxideand carbon dioxide is of course no slouch itself as a greenhouse gas. In short, chlorofluorocarbons may ultimately prove to be just about the worst invention of the twentieth century.

Midgley never knew this because he died long before anyone realized how destructive CFCs were. His death was itself memorably unusual. After becoming crippled with polio, Midgley invented a contraption involving a series of motorized pulleys that automatically raised or turned him in bed. In 1944, he became entangled in the cords as the machine went into action and was strangled.

If you were interested in finding out the ages of things, the University of Chicago in the 1940s was the place to be. Willard Libby was in the process of inventing radiocarbon dating, allowing scientists to get an accurate reading of the age of bones and other organic remains, something they had never been able to do before. Up to this time, the oldest reliable dates went back no further than the First Dynasty in Egypt from about 3000B.C.No one could confidently say, for instance, when the last ice sheets had retreated or at what time in the past the Cro-Magnon people had decorated the caves of Lascaux in France.

Libbys idea was so useful that he would be awarded a Nobel Prize for it in 1960. It was based on the realization that all living things have within them an isotope of carbon called carbon-14, which begins to decay at a measurable rate the instant they die. Carbon-14 has a half-lifethat is, the time it takes for half of any sample to disappear[24]of about 5,600 years, so by working out how much a given sample of carbon had decayed, Libby could get a good fix on the age of an objectthough only up to a point. After eight half-lives, only 1/256 of the original radioactive carbon remains, which is too little to make a reliable measurement, so radiocarbon dating works only for objects up to forty thousand or so years old.

Curiously, just as the technique was becoming widespread, certain flaws within it became apparent. To begin with, it was discovered that one of the basic components of Libbys formula, known as the decay constant, was off by about 3 percent. By this time, however, thousands of measurements had been taken throughout the world. Rather than restate every one, scientists decided to keep the inaccurate constant. Thus, Tim Flannery notes, every raw radiocarbon date you read today is given as too young by around 3 percent. The problems didnt quite stop there. It was also quickly discovered that carbon-14 samples can be easily contaminated with carbon from other sourcesa tiny scrap of vegetable matter, for instance, that has been collected with the sample and not noticed. For younger samplesthose under twenty thousand years or soslight contamination does not always matter so much, but for older samples it can be a serious problem because so few remaining atoms are being counted. In the first instance, to borrow from Flannery, it is like miscounting by a dollar when counting to a thousand; in the second it is more like miscounting by a dollar when you have only two dollars to count.

Libbys method was also based on the assumption that the amount of carbon-14 in the atmosphere, and the rate at which it has been absorbed by living things, has been consistent throughout history. In fact it hasnt been. We now know that the volume of atmospheric carbon-14 varies depending on how well or not Earths magnetism is deflecting cosmic rays, and that that can vary significantly over time. This means that some carbon-14 dates are more dubious than others. This is particularly so with dates just around the time that people first came to the Americas, which is one of the reasons the matter is so perennially in dispute.

Finally, and perhaps a little unexpectedly, readings can be thrown out by seemingly unrelated external factorssuch as the diets of those whose bones are being tested. One recent case involved the long-running debate over whether syphilis originated in the New World or the Old. Archeologists in Hull, in the north of England, found that monks in a monastery graveyard had suffered from syphilis, but the initial conclusion that the monks had done so before Columbuss voyage was cast into doubt by the realization that they had eaten a lot of fish, which could make their bones appear to be older than in fact they were. The monks may well have had syphilis, but how it got to them, and when, remain tantalizingly unresolved.

Because of the accumulated shortcomings of carbon-14, scientists devised other methods of dating ancient materials, among them thermoluminesence, which measures electrons trapped in clays, and electron spin resonance, which involves bombarding a sample with electromagnetic waves and measuring the vibrations of the electrons. But even the best of these could not date anything older than about 200,000 years, and they couldnt date inorganic materials like rocks at all, which is of course what you need if you wish to determine the age of your planet.

The problems of dating rocks were such that at one point almost everyone in the world had given up on them. Had it not been for a determined English professor named Arthur Holmes, the quest might well have fallen into abeyance altogether.

Holmes was heroic as much for the obstacles he overcame as for the results he achieved. By the 1920s, when Holmes was in the prime of his career, geology had slipped out of fashionphysics was the new excitement of the ageand had become severely underfunded, particularly in Britain, its spiritual birthplace. At Durham University, Holmes was for many years the entire geology department. Often he had to borrow or patch together equipment in order to pursue his radiometric dating of rocks. At one point, his calculations were effectively held up for a year while he waited for the university to provide him with a simple adding machine. Occasionally, he had to drop out of academic life altogether to earn enough to support his familyfor a time he ran a curio shop in Newcastle upon Tyneand sometimes he could not even afford the £5 annual membership fee for the Geological Society.

The technique Holmes used in his work was theoretically straightforward and arose directly from the process, first observed by Ernest Rutherford in 1904, in which some atoms decay from one element into another at a rate predictable enough that you can use them as clocks. If you know how long it takes for potassium-40 to become argon-40, and you measure the amounts of each in a sample, you can work out how old a material is. Holmess contribution was to measure the decay rate of uranium into lead to calculate the age of rocks, and thushe hopedof the Earth.

But there were many technical difficulties to overcome. Holmes also neededor at least would very much have appreciatedsophisticated gadgetry of a sort that could make very fine measurements from tiny samples, and as we have seen it was all he could do to get a simple adding machine. So it was quite an achievement when in 1946 he was able to announce with some confidence that the Earth was at least three billion years old and possibly rather more. Unfortunately, he now met yet another formidable impediment to acceptance: the conservativeness of his fellow scientists. Although happy to praise his methodology, many maintained that he had found not the age of the Earth but merely the age of the materials from which the Earth had been formed.

It was just at this time that Harrison Brown of the University of Chicago developed a new method for counting lead isotopes in igneous rocks (which is to say those that were created through heating, as opposed to the laying down of sediments). Realizing that the work would be exceedingly tedious, he assigned it to young Clair Patterson as his dissertation project. Famously he promised Patterson that determining the age of the Earth with his new method would be duck soup. In fact, it would take years.

Patterson began work on the project in 1948. Compared with Thomas Midgleys colorful contributions to the march of progress, Pattersons discovery of the age of the Earth feels more than a touch anticlimactic. For seven years, first at the University of Chicago and then at the California Institute of Technology (where he moved in 1952), he worked in a sterile lab, making very precise measurements of the lead/uranium ratios in carefully selected samples of old rock.

The problem with measuring the age of the Earth was that you needed rocks that were extremely ancient, containing lead- and uranium-bearing crystals that were about as old as the planet itselfanything much younger would obviously give you misleadingly youthful datesbut really ancient rocks are only rarely found on Earth. In the late 1940s no one altogether understood why this should be. Indeed, and rather extraordinarily, we would be well into the space age before anyone could plausibly account for where all the Earths old rocks went. (The answer was plate tectonics, which we shall of course get to.) Patterson, meantime, was left to try to make sense of things with very limited materials. Eventually, and ingeniously, it occurred to him that he could circumvent the rock shortage by using rocks from beyond Earth. He turned to meteorites.

The assumption he maderather a large one, but correct as it turned outwas that many meteorites are essentially leftover building materials from the early days of the solar system, and thus have managed to preserve a more or less pristine interior chemistry. Measure the age of these wandering rocks and you would have the age also (near enough) of the Earth.

As always, however, nothing was quite as straightforward as such a breezy description makes it sound. Meteorites are not abundant and meteoritic samples not especially easy to get hold of. Moreover, Browns measurement technique proved finicky in the extreme and needed much refinement. Above all, there was the problem that Pattersons samples were continuously and unaccountably contaminated with large doses of atmospheric lead whenever they were exposed to air. It was this that eventually led him to create a sterile laboratorythe worlds first, according to at least one account.

It took Patterson seven years of patient work just to assemble suitable samples for final testing. In the spring of 1953 he traveled to the Argonne National Laboratory in Illinois, where he was granted time on a late-model mass spectrograph, a machine capable of detecting and measuring the minute quantities of uranium and lead locked up in ancient crystals. When at last he had his results, Patterson was so excited that he drove straight to his boyhood home in Iowa and had his mother check him into a hospital because he thought he was having a heart attack.

Soon afterward, at a meeting in Wisconsin, Patterson announced a definitive age for the Earth of 4,550 million years (plus or minus 70 million years)a figure that stands unchanged 50 years later, as McGrayne admiringly notes. After two hundred years of trying, the Earth finally had an age.

His main work done, Patterson now turned his attention to the nagging question of all that lead in the atmosphere. He was astounded to find that what little was known about the effects of lead on humans was almost invariably wrong or misleadingand not surprisingly, he discovered, since for forty years every study of leads effects had been funded exclusively by manufacturers of lead additives.

In one such study, a doctor who had no specialized training in chemical pathology undertook a five-year program in which volunteers were asked to breathe in or swallow lead in elevated quantities. Then their urine and feces were tested. Unfortunately, as the doctor appears not to have known, lead is not excreted as a waste product. Rather, it accumulates in the bones and bloodthats what makes it so dangerousand neither bone nor blood was tested. In consequence, lead was given a clean bill of health.

Patterson quickly established that we had a lot of lead in the atmospherestill do, in fact, since lead never goes awayand that about 90 percent of it appeared to come from automobile exhaust pipes, but he couldnt prove it. What he needed was a way to compare lead levels in the atmosphere now with the levels that existed before 1923, when tetraethyl lead was introduced. It occurred to him that ice cores could provide the answer.

It was known that snowfall in places like Greenland accumulates into discrete annual layers (because seasonal temperature differences produce slight changes in coloration from winter to summer). By counting back through these layers and measuring the amount of lead in each, he could work out global lead concentrations at any time for hundreds, or even thousands, of years. The notion became the foundation of ice core studies, on which much modern climatological work is based.

What Patterson found was that before 1923 there was almost no lead in the atmosphere, and that since that time its level had climbed steadily and dangerously. He now made it his lifes quest to get lead taken out of gasoline. To that end, he became a constant and often vocal critic of the lead industry and its interests.

It would prove to be a hellish campaign. Ethyl was a powerful global corporation with many friends in high places. (Among its directors have been Supreme Court Justice Lewis Powell and Gilbert Grosvenor of the National Geographic Society.) Patterson suddenly found research funding withdrawn or difficult to acquire. The American Petroleum Institute canceled a research contract with him, as did the United States Public Health Service, a supposedly neutral government institution.

As Patterson increasingly became a liability to his institution, the school trustees were repeatedly pressed by lead industry officials to shut him up or let him go. According to Jamie Lincoln Kitman, writing inThe Nation in 2000, Ethyl executives allegedly offered to endow a chair at Caltech if Patterson was sent packing. Absurdly, he was excluded from a 1971 National Research Council panel appointed to investigate the dangers of atmospheric lead poisoning even though he was by now unquestionably the leading expert on atmospheric lead.

To his great credit, Patterson never wavered or buckled. Eventually his efforts led to the introduction of the Clean Air Act of 1970 and finally to the removal from sale of all leaded gasoline in the United States in 1986. Almost immediately lead levels in the blood of Americans fell by 80 percent. But because lead is forever, those of us alive today have about 625 times more lead in our blood than people did a century ago. The amount of lead in the atmosphere also continues to grow, quite legally, by about a hundred thousand metric tons a year, mostly from mining, smelting, and industrial activities. The United States also banned lead in indoor paint, forty-four years after most of Europe, as McGrayne notes. Remarkably, considering its startling toxicity, lead solder was not removed from American food containers until 1993.

As for the Ethyl Corporation, its still going strong, though GM, Standard Oil, and Du Pont no longer have stakes in the company. (They sold out to a company called Albemarle Paper in 1962.) According to McGrayne, as late as February 2001 Ethyl continued to contend that research has failed to show that leaded gasoline poses a threat to human health or the environment. On its website, a history of the company makes no mention of leador indeed of Thomas Midgleybut simply refers to the original product as containing a certain combination of chemicals.

Ethyl no longer makes leaded gasoline, although, according to its 2001 company accounts, tetraethyl lead (or TEL as it calls it) still accounted for $25.1 million in sales in 2000 (out of overall sales of $795 million), up from $24.1 million in 1999, but down from $117 million in 1998. In its report the company stated its determination to maximize the cash generated by TEL as its usage continues to phase down around the world. Ethyl markets TEL through an agreement with Associated Octel of England.

As for the other scourge left to us by Thomas Midgley, chlorofluorocarbons, they were banned in 1974 in the United States, but they are tenacious little devils and any that you loosed into the atmosphere before then (in your deodorants or hair sprays, for instance) will almost certainly be around and devouring ozone long after you have shuffled off. Worse, we are still introducing huge amounts of CFCs into the atmosphere every year. According to Wayne Biddle, 60 million pounds of the stuff, worth $1.5 billion, still finds its way onto the market every year. So who is making it? We arethat is to say, many of our large corporations are still making it at their plants overseas. It will not be banned in Third World countries until 2010.

Clair Patterson died in 1995. He didnt win a Nobel Prize for his work. Geologists never do. Nor, more puzzlingly, did he gain any fame or even much attention from half a century of consistent and increasingly selfless achievement. A good case could be made that he was the most influential geologist of the twentieth century. Yet who has ever heard of Clair Patterson? Most geology textbooks dont mention him. Two recent popular books on the history of the dating of Earth actually manage to misspell his name. In early 2001, a reviewer of one of these books in the journalNature made the additional, rather astounding error of thinking Patterson was a woman.

At all events, thanks to the work of Clair Patterson by 1953 the Earth at last had an age everyone could agree on. The only problem now was it was older than the universe that contained it.





A Short History of Nearly Everything