---
---
---
title: 1 The Last Fortress
---






I hadn’t known I would see the future when I agreed to speak at the 2018 Summit of the Wharton Neuroscience Initiative at the University of Pennsylvania. But as soon as Josh Duyan, the chief strategy officer of a company called CTRL-labs, began his presentation, the magnitude of the change and the urgency of the choices we are facing became blindingly clear.

Holding up his hands, Duyan lamented the fact that the extraordinary input capabilities of our brains were tethered to such “limited and clumsy output devices.” He noted the step backward we’ve taken when it comes to typing on our phones, moving from ten fingers to two thumbs. Just imagine how much more efficient we’d be, he said, if we could type with our brains instead or better yet, “operate octopus-like tentacles.”

Until that day, I had puzzled over how (and even whether) consumer neurotechnology could go mainstream. The then-existing applications of neurotech that enabled us to play video games, meditate, or improve our focus with our minds seemed like niche applications that were unlikely to motivate people to go about their everyday lives wearing a silly-looking headband.

But the wristband Duyan was describing seemed altogether different. Our brains, he told us, are constantly transmitting signals to our peripheral nervous system—the parts of the nervous system outside of the brain and the spinal cord. CTRL-labs’ wristband detects these signals using electromyography (EMG).1 When I move my hand, for example, the region in my brain called the motor cortex sends an electrical signal to my spinal cord, which distributes a set of signals to the relevant muscles to tell them to move. Where my low motor neurons innervate my muscles, a cascade of activity creates a current (measured in milliamperes) and potential difference or voltage (V, measured in millivolts) that can be detected by the electrodes in the wristband.

With its compact and easy-to-wear form, easy integration into existing wearables—like the smart watches it resembles—and application as an interface to other technologies like virtual reality or swiping a smartphone, this device was different in kind from anything I’d seen. It could offer a significantly better user performance for the tasks currently done by peripheral devices like keyboards and mice.

If people are willing to give up reams of personal data to keep in touch with their friends on Facebook, it seemed likely they would be willing to trade their brain privacy to swipe a screen or type with their minds.





The Last Fortress


The things we think, feel, and mull over in our minds help us define who we are to ourselves and to others. What we choose to share about those things—and perhaps more important, what we choose not to share—is fundamental to the intimacy we create with other people.

Until that day in 2018, I believed that our brains were the one place of solace to which we could safely and privately retreat. Your personal diary was always at risk of discovery. If you wrote it on paper, someone could find it and read it; if you typed it on your computer, someone or something could be tracking your keystrokes. But your brain was different. You could think that your friend’s new couch is hideous without hurting her feelings. You could think your boss was a clown while nodding affirmatively at his latest pronouncement. You could let your thoughts wander while listening to a boring speaker, fantasizing about your latest romantic interest. Or imagine new ideas or ways of doing things, without having to worry what others would think if your innovations turned out to be duds. You could work through your sexual orientation and decide when and if you would be ready to share that with others. Or you could dare to dream about overthrowing your tyrannical government.

We may soon lose that last realm of privacy. As noted, new technologies collect our brain data to help us become faster, more efficient, safer, healthier, less stressed, and even more spiritual. Just as we exchanged access to our web search history for free and powerful internet browsers, we will have reasons to want to share the brain data these devices collect. To be clear, the data itself is not the same thing as our thoughts and feelings themselves. But powerful machine learning algorithms are getting better and better at translating brain activity into what we are feeling, seeing, imagining, or thinking.

Once we become aware that others can access what we are thinking, feeling, or imagining, we may attempt to censor even our thoughts, lest we be ridiculed or ostracized for having ideas that go against the grain. Worse still, if governments gain the power to track the contents of our brains, they can arrest us and punish us for thought crimes.

I am not alone in my concerns; other scholars are starting to sound the alarm too. The neuroscientist Rafael Yuste has advocated for what he calls neurorights, because, he says, our “brain data may be one of the few remaining bulwarks against fully compromising privacy in modern life.”2 “While the body can easily be subject to domination and control by others,” the Swiss bioethicist Marcello Ienca warned, “our minds, along with our thoughts, beliefs and convictions, are to a large extent beyond external constraint. Yet with advances in neural engineering, brain imaging, and pervasive neurotechnology, the mind might no longer be such an unassailable fortress.”3 Dr. Wrye Sententia and the legal theorist Richard Boire have long worried about these issues, having founded the nonprofit Center for Cognitive Liberty and Ethics more than a decade ago.

By now, most people realize that “free” digital services come at the expense of an individual’s personal data. While Google originally sought to “bring order to the web” and provide “high quality search results,”4 it now commands 92 percent of the search engine market in the United States—all the while taking the data we enter into its search engines, web browser, and assets like YouTube and Gmail to create detailed profiles of us that they use to draw conclusions about what different people of different demographics (but increasingly each of us individually) want to see or buy.5

Tech companies’ business models rest on their ability to sell their understanding of us to others. Google does so through its “real-time bidding” process, which provides advertisers with opportunities to acquire uniquely targeted advertising real estate. Meta does much the same thing, harvesting data on its billions of users and creating psychological profiles of them that advertisers can use to microtarget their pitches.6 Shoshana Zuboff coined the term “surveillance capitalism” to describe this now ubiquitous phenomenon, characterizing “data about the behaviors of bodies, minds, and things” as “surveillance assets” that can be used for the purpose of “knowing, controlling, and modifying behavior to produce new varieties of commodification, monetization and control.”7

It’s not just tech giants that are commodifying our data, and it’s not just advertisers that are interested in it. Consumer data has also enabled a revolution in our understanding of health and disease. The personal genomics company 23andMe, for example, made headlines in 2018 when it announced that it had secured a $300 million deal to share its consumers’ genetic data with GlaxoSmithKline.8 It had already entered into data sharing agreements with other major pharmaceutical companies, including Genentech and Pfizer.

Through its business model, default settings, and privacy policies, 23andMe has included 80 percent of its 10.7 million customers in a database that associates millions of raw genome sequences with consumer demographics and other information, enabling large-scale analyses of genetic diseases and their indicators.9 That was 23andMe’s intention all along, as board member Patrick Chung explained in 2013: “The long game here is not to make money selling [saliva] kits [to collect and report on consumer DNA], although the kits are essential to get the base level data. Once you have the data, [23andMe] does become the Google of personalized health care.”10

All of which explains why, when I returned home from the Wharton summit, I dived into learning everything I could about CTRL-labs, its products, and the direction it might lead us.

I watched presentations by Thomas Reardon, cofounder of CTRL-labs, describing a future in which our interactions with technology are driven by neural interface. Some of which already exist. Google’s Dinosaur Game is a feature of its Google Chrome web browser that makes losing internet connectivity a little bit less frustrating. When Google Chrome goes offline and you angrily pound on your space bar, a pixelated Tyrannosaurus rex appears. You can use your arrow keys to make the dinosaur run across the side-scrolling landscape and jump over obstacles to earn points. Earn a hundred points, and you’ll be rewarded with a screech.

The next time the T. rex appears, you could use CTRL-labs’ Bluetooth-connected wristband to make the dinosaur jump just by willing it to do so. By maintaining the same mental focus as you work the arrows, the device uses powerful algorithms to translate the electrical activity and brain signals being sent to your hands into signals the computer can understand as commands. “Here’s the cool thing,” Reardon described. “I don’t have to tell you to stop [moving your hand]. What you start to realize is the dinosaur is going to jump whether you push the button or not.”11 Which is cool, I thought. But should we really be plugging our brains into Google?

The more I learned, the more certain I became that CTRL-labs would soon be acquired by a major technology firm. It seemed like a natural fit for Apple, as the company could integrate the EMG sensors into its already-popular Apple Watch. In time, the interfaces would allow users to track their sleep, control smart devices in their environment, send text messages just by thinking about it, and even detect signs that they are becoming dangerously drowsy while driving. To my surprise, Facebook’s umbrella company, Meta, acquired CTRL-labs instead, in September 2019, paying somewhere between $500 million and $1 billion—one of its most expensive recent acquisitions.12

Meta’s head of augmented reality (AR) and virtual reality (VR), Andrew “Boz” Bosworth, announced the acquisition on his personal Facebook page. Bosworth explained how the wristbands would become the “universal controller for all your interactions with technology.”13 So far, Meta has showcased typing and swiping with AR and VR as its likely first application, but as Meta founder and CEO Mark Zuckerberg summed it up, “In some ways, the holy grail of all this is a neural interface.”14

It’s easy to see how using neurotechnology as an interface to other technology could fundamentally change our lives. With advances in predictive algorithms, the wristband could anticipate whole words to type from single letters. Reardon calls this “word forming,” where “you’re not typing. You’re kind of forming words in real time and they kind of spill out of your hand. It’s giving you … choices between words and you quickly learn how to get to the word you want to form.” He added, “There would be no difference between how you produce oral speech and how you produce this controlled text flow.”15

Meta has not achieved word formation at anything close to the rate of speech yet. In demos prior its acquisition, CTRL-labs was only able to achieve forty words per minute (about the same rate as an average typist but significantly slower than our rate of speech, which is about 140 to 160 words per minute). But that was already double the rate achieved by other researchers, and they have undoubtedly made progress in the meantime.16 With the backing of a company like Meta, real-time neural word decoding is on the horizon.

As for my initial bet that CTRL-lab’s EMG sensors would be integrated into the Apple Watch? I was wrong about the acquiring company, but not about its intentions. Meta plans to launch its own smart watch soon. Zuckerberg posted a photo of EssilorLuxottica’s chairman, Leonardo Del Vecchio, donning the wristband as part of a joint venture with the smart glasses company. “Leonardo is using a prototype of our neural interface EMG wristband that will eventually let you control your glasses and other devices,” Zuckerberg explained.17 While Meta’s first smart watch release may not yet have the EMG sensors, the tech giant promises that future releases will have that integration.18





Big Tech Going All In on Brain Decoding


Meta may be leading the big tech pack, but scores of other companies are also in the race to develop neural interfaces. Until now, most have focused on much narrower applications. InteraXon’s EEG headset, which I was using to mitigate my migraines, helps consumers meditate more effectively through audible neurofeedback, like the bird’s chirping in response to my brain wave activity.

Myontec, Athos, Delsys, and Noraxon offer athletes and sports therapists EMG-generated insights into what’s happening with their muscles during training and competitions, such as the rate of force development (a measure of explosive strength), improvements in coordination through training, and symmetry and asymmetry in muscle activation. Control Bionics sells NeuroNode, a wearable EMG device for patients with degenerative neurological disorders like ALS/MND. It enables them to control a computer, tablet, or motor device via the bioelectrical signals that are sent to muscles to trigger movements, even if those movements aren’t visible. Kernel offers Flow—a functional near-infrared spectroscopy (fNIRS) device—that looks like a high-tech bike helmet and that measures changes in blood oxygenation levels in the brain to understand and improve its functioning.

But Meta’s investment heralds a new frontier for consumer neurotechnology, in which mainstream technology companies use neurotechnology as the new—and potentially primary—way we will interface with all their platforms.

Apple appears poised to make a similar bet, as it has hinted that it will integrate health sensors like EEG into its AirPods, much as it integrated ECG sensors into the Apple Watch.19 Other neurotech companies are charting the way, making them likely targets for acquisition. Emotiv has launched MN8, earbuds with two-channel integrated EEG sensors.20 NextSense, backed by Alphabet’s moonshot division and spun out as an independent company, believes it has the winning recipe for a brain-health monitoring platform with its EEG-earbuds, and hope to build a “mass-market brain monitor.”21 Apple may be the company’s key to doing so. In the spirit of Steve Jobs’s Digital Hub, Apple executive Kevin Lynch has extolled the value of multiple devices working together.22 EEG may be the next device in its wheelhouse.

Snap (the company behind Snapchat) has acquired Paris-based neurotech company NextMind, known for its EEG-based brain wave controller. Snap plans to intergrate the technology into their augmented reality platform, to “monitor neural activity to understand your intent when interacting with a computing interface, allowing you to push virtual button simply by focusing on it,” the company explained in their blog post announcing the acquisition.23

Microsoft has obtained a patent on an EEG device that allows users to navigate web browsers and apps with their brains and will reward people with cryptocurrency for doing so.24 Neurable promises “the mind unlocked” with its “smart headphones for smarter focus.” And then there are Elon Musk’s Neuralink, Thomas Oxley’s Synchron, and Marcus Gerhardt’s Blackrock Neurotech (companies we’ll look at more closely in chapter 9), which are working on implantable neurotechnology that will be implanted inside our brains. These devices will be far more powerful than any existing wearable neurotechnology—powerful enough to achieve real-time thought and imagery decoding, which is way beyond the capabilities of existing consumer-grade EEG and EMG devices.

But whether worn on our scalps or wrists, or deeply embedded in our brains, all these devices share one striking commonality. Each records our raw neural activity—which can be saved, aggregated, and mined for much more than what consumers are using it for. The black box of our brains has been opened. Mark Zuckerberg is right. Neural interface is the “holy grail.”

Of data tracking by corporations.





“Raw” Brain Data Is Uniquely Sensitive


Suppose you keep a written diary and wish to share a passage from it with a friend. You hand it to them and ask them to read the highlighted passage. Your friend does so and hands it back. Now imagine instead that your friend also made a copy of your diary, which they keep in a file on their desk so they can return to it anytime they want to learn something new about you, whether you intended to share it or not.

Raw brain data captured by EEG, EMG, and other neurotechnology are similar. EEG, for example, records raw brain data—delta (slow waves), theta (medium), alpha (higher), beta (higher still), and gamma (the highest, at 30–80 Hz)—as well as the electrical activity of nearby muscles, electrode motion interference, and ambient noise.

This “raw” data is then fed through software that filters out artifacts and extraneous information, analyzes the brain waves, and picks out the relevant information to return to you. If brain activity is recorded and stored, that same raw brain data can be returned to time and again and mined to learn all kinds of additional insights about you—such as whether you are at risk of stroke or Alzheimer’s or ADHD. All without your knowledge.

While you can still choose to use or not use most consumer neurotechnology, once you do use it, you may be revealing far more than you intended: Blinking, the beating of our hearts, sweating—these are all automatic functions that neither require nor follow our conscious wills. More complex automatic brain functions include our visceral or emotional reactions to external events. A scary movie, a passionate kiss, or a painful burn will all evoke automatic reactions that occur outside our cognitive or “rational” thought processing, but that nonetheless leave traces in the brain.25

Even emotional states and biases can be decoded. When someone says something that is hurtful, you might choose to conceal your feelings. But your brain still registers them. You might be feeling bored and lonely in your relationship but aren’t ready to share that with your spouse. But if your spouse had access to your raw brain data and the tools to interpret it, your brain could give you away.26 You might work hard to combat your implicit biases while you are at work, but your subconscious still registers them.27 If you were wearing a consumer headset at the time, your biases could be decoded and made public.

Hackers could even install brain spyware into the apps and devices you are using. A research team led by UC Berkeley computer science professor Dawn Song tried this on gamers who were using neural interface to control a video game. As they played, the researchers inserted subliminal images into the game and probed the players’ unconscious brains for reaction to stimuli—like postal addresses, bank details, or human faces. Unbeknownst to the gamers, the researchers were able to steal information from their brains by measuring their unconscious brain responses that signaled recognition to stimuli, including a PIN code for one gamer’s credit card and their home address.28 Neural data could also be intercepted as it is sent to a paired cell phone if it isn’t well secured.29

“It’s happening somewhat faster than we thought,” says Howard Chizeck, a professor of electrical engineering at the University of Washington. Chizeck expects that millions of people will soon be playing online games while wearing brain–computer interface devices. The operator of the game could play Twenty Questions, and measure the automatic brain reactions to what the gamer sees. “I could flash pictures of [gay and straight] couples and see which ones you react to. And going through a logic tree, I could extract your sexual orientation,” Chizeck says. “I could show political candidates and begin to understand your political orientation, and then sell that to pollsters.” This kind of probing could be accomplished through spyware by a malicious actor but could just as easily be built into popular games and technologies, allowing the manufacturers to surreptitiously collect even more data about us.30 A recent report claims the Chinese government is already using cutting-edge AI and neurotechnology to analyze facial expressions and brain waves to see if a person is attentive to “thought and political education.”31

While other personal tracking data has proved strongly predictive of certain things—our purchasing behavior, for example—brain data can reveal our deepest-held feelings or biases, ones we ourselves may not yet have acknowledged. It can even be used to predict how agreeable or neurotic we are by looking at our alpha, beta, and theta bands!32 In the words of the philosopher Sarah Goering and neuroscientist Rafael Yuste, raw brain wave data “could provide access to highly intimate information that is proximal to one’s identity,” such as our political orientation, sexual orientation, or our tolerance or strategy for dealing with risk.33

Many people believe they are shielded from targeted misuse if their information is stripped of identifying information. But our brain patterns may be even more unique than our fingerprints.34





Will People Unwittingly Share Their Brain Data?


Will people willingly cede their neural data to corporations? If the world’s largest furniture retailer’s recent marketing campaign is any indication, the answer is a resounding yes.

Since 2015, IKEA has commissioned contemporary artists to create limited-edition hand-woven rugs, figurines, wall art, and other household objects. Dubbed the IKEA Art Event Collection, the program is intended, in IKEA’s words, “to democratize art and make it accessible and affordable for everyone.”35 IKEA was discouraged by the number of buyers who purchased the pieces in bulk so they could resell them through online-auction sites at steeply inflated prices. So, in 2019, it tried something different.

Between May 7 and May 11, 109 rugs were put on display in its Anderlecht, Belgium, store, designed by the likes of SupaKitch, the French street artist; Chiaozza, American play-and-craft sculptors Adam Frezza and Terri Chiao; Noah, a Brooklyn-based multidisciplinary artist; the UK fashion designer Craig Green; Seulgi Lee, a Paris-based Korean contemporary-folklore artist; Virgil Abloh, artistic director of Louis Vuitton’s ready-to-wear menswear line; the Japanese-born, LA-based artist Misaki Kawai; and world-renowned Polish pop-culture artist Filip Pagowski. Prospective buyers were told to don an EEG headset so the IKEA (He)art Scanner, a device that was devised in partnership with the advertising and marketing giant Ogilvy Brussels—whose clients coincidentally include Meta—could find out whether they really loved one of the pieces.

“When people looked at each carpet,” a representative from Ogilvy explained, “our specially designed algorithm captured and analyzed brain and body reaction data in real time. This data was then promptly translated into an uplift score, which was projected next to the rug they just saw for the first time. If the uplift was sufficient, they could buy the rug. If not, they moved on to try the next one.”36 More disturbing than the blatant intrusion into customers’ mental privacy is how blandly unconcerned IKEA customers were about it. Ogilvy claims that not a single prospective customer objected to using the EEG device and that “everyone had a great time.”

Opening our brains to the outside world, making the information contained therein a target for corporations, governments, and society to use puts all our freedoms at risk. While you may well think, But, aha! I will just avoid using neurotechnology, that might not be possible if it’s the gateway to goods and services that you already enjoy. And, as we’ll explore in the chapters ahead, neurotech may become a requirement in modern workplaces—no wristband, no job.

Though IKEA’s campaign is almost certainly a well-intended and harmless gimmick, to me it heralds a chilling future in which society will increasingly encroach upon our brains—to discover anything and everything therein.





Brain Waves as the Newest Status Update


There are, of course, good reasons for us to want to share our raw brain activity data. Along with typing better and interacting with our machines with less friction, it may give us advance warnings for diseases that affect the brain. For that to happen, large sets of brain data must be made available to scientists for analysis. Which means assurances must be put in place so we can share our data without fear that it will be misused.

A few years ago, Kevin Schoeninger and Stephen Skelton learned about EEG studies of monks who meditate and wondered whether consumer neurotechnology could enhance their own practice. Using Muse, InteraXon’s EEG device, they took thousands of recordings of their own brains while meditating.37 Both saw consistent changes in their brain wave activity.

When they launched their ten-week course on meditation, they invited their students to use a Muse headband. But none of their students were able to replicate the changes in brain wave activity that Kevin and Stephen had seen. So, they started experimenting with other devices, among them Flowtime, which records heart rate and breathing alongside EEG. More important, Flowtime allows students to see their raw brain wave activity in real time, and not just the interpretation of that activity that Muse provides, and even the subtle changes in brain waves that a novice could achieve in a short time were visible.

While several students switched to Flowtime, others found a workaround by uploading the Muse data on their brain waves to software from a company called Mind-Monitor.com, which allowed them to view it directly. Kevin told me that both he and his students regularly share their brain wave activity with device manufacturers, third-party software providers, and one another in chat rooms. In fact, thousands of people share and compare their brain wave activity after meditating via social media. On Kevin’s prompting, I joined a Facebook group called HeartMind Alchemy Lab, where I was immediately met with screenshot after screenshot of members’ EEG activity, which were being commented on and evaluated by other members of the group.38

My curiosity piqued, I asked Kevin if he believed that his brain wave activity was any more sensitive than any of the other information he could be sharing on social media. “Um, I haven’t really considered that,” he replied. I probed a little further and asked if he would have any concerns if the companies who have access to his raw brain wave activity mined it to learn more. “No,” he said. “As long as they don’t have my social security number and my financial information, I’m not worried. I’m not doing anything that I care if anybody knows.” In fact, “everything I’m doing I’m promoting to others!”

Kevin’s views align with research results from surveys conducted by my lab at Duke—the Science Law & Policy Lab. Building on a 2014 Pew Research Study on privacy,39 in 2018 we asked 1,450 US participants to rate the perceived sensitivity of different kinds of information about themselves, ranging from their social security number, health and medication, the contents of their phone conversations, personal relationship histories, and the kinds of information that consumer neurotechnology can reveal—how well they concentrate while working, their emotional states, details about their brain waves, sleep patterns, and alertness throughout the day, the general state of their brain health, and the thoughts in their minds. Just like Kevin, our survey respondents rated their social security number as their single most sensitive piece of personal information. Of the top five items that participants ranked as “very sensitive,” only one of our brain variables made the list: the thoughts in their minds. And even those were rated as considerably less sensitive than their social security numbers!





Brain Data Is Being Commodified Too


Whether they are used for meditation, playing games, navigating AR/VR, detecting early seizure onset, tracking tumor development, or detecting the signs of Alzheimer’s disease, consumer neurotechnology products all work with apps on a computer or a mobile device. The devices collect raw brain wave and electromyographical signals and filter them through the proprietary algorithms. While the data may have been recorded locally, it ends up on servers operated by companies. And those companies use the data for much more than most of us suspect.

The Flowtime device that Kevin and his students were using? It’s manufactured by Hangzhou Enter Electronic Technology (Entertech), a Chinese-based company that has a suite of consumer and enterprise neurotechnology devices with applications for education, psychological health, VR, and the military.40 Entertech has sold tens of thousands of helmets fitted with EEG sensors to the State Grid Corporation of China—a Chinese state-owned electric utility corporation—so it can measure its workers’ fatigue and other brain wave activities in real time on the job. Entertech has accumulated millions of raw EEG data recordings from individuals engaged in various sorts of activities, from mind-controlled video game racing to working and sleeping.41

Entertech collects much more than raw brain wave data from its users. It records their personal information when they create their accounts—such as their birthdate, gender, height, weight, and in some cases, mobile telephone numbers. When location services are enabled on the app, Entertech collects GPS signals, device sensors, WiFi access points, and cell tower IDs. It tracks information about other devices, computers, and services a person may be using, including their IP addresses, browser types, languages, operating systems, referring web page, other pages visited, and cookies they may have installed. If a user connects their services to Meta or Google, Entertech will collect their email addresses, and those of their friends. And it associates all this data with a user’s raw brain wave activity.

If you read Entertech’s privacy policy, you’ll see that the company plainly discloses its intention to use all this information and to share aggregated or de-identified data, including brain wave activity and its interpretation of that data, with third-party partners.42 Given all of that, it’s not surprising that in November 2018, SingularityNET announced that it had entered a partnership with Entertech to analyze data gathered from Entertech’s EEG measurement products using its AI platform. While it’s unclear what they ultimately hope to learn from the data, they have publicly stated that they aim to “allow people to regulate their states of mind better so as to guide their daily lives and their meditative experiences,” and to “help enterprise employees better monitor and self-regulate their states of consciousness on the job to improve both their job performance and their satisfaction.”43 As we’ll see in the chapters that follow, employers are already making use of their insights. And so is the Chinese military.

Other consumer neurotech companies are similarly open about their commodification of the raw brain data they collect. Developed in 2011 by students at the MIT Media Lab, Multimer’s MindRider EEG bike helmet was originally designed to read a bicyclists’ brain waves so it could alert motorists and the cyclists to their mental states with colored lights—such as whether they were focused or drowsy, or nervous because you were driving too close.44 The company’s founder, Arlene Ducao, then went on to create Multimer Data, which received its first round of seed funding in 2016, to mine the data MindRider collects to create maps of cities with location-specific insights gathered from users’ brains—like the ideal site for a retail establishment, or where to place a billboard so that it has the least visual distractions surrounding it.45

Some companies have progressive privacy policies. Mind-monitor.com promises not to “collect your personal brainwave data,” and says that the “brainwaves recorded by Mind Monitor are stored only on your personal device and [we] have no access to them.”46 But its approach is a rare exception. Most corporations claim unfettered rights to our raw brain-activity data, and to record, store, and mine it.

As for Meta? Commoditizing consumer data is its business model. While it emphasizes that CTRL-labs’ EMG device collects information locally, at muscle junctures rather than the brain, EMG data is no less sensitive than raw brain data.

Whether you’re washing your hands, developing an imperceptible tremor as you progress into the earliest stages of a neurodegenerative disease, are forming words you decide not to type, or are engaged in intimate activity using your hands in your bedroom, your EMG wristband can recognize it. Which means Meta can too.

That’s why it’s high time that consumers wake up and start to learn about the unique sensitivity of their brain-activity data—and why our mental privacy is worth defending.





Brain Data Varies in Sensitivity, So Governing It Should Too


In the summer of 2020, Chicago police investigated a homicide recorded on a home security camera. The recording shows two men fighting in front of a Rogers Park apartment building. One punches the other in the head. The punched man staggers, then pulls a gun and shoots his assailant. The gunman’s face is never visible, but the video captures his voice.

Based on an anonymous tip, the police identify a suspect, but even the voiceprint can’t connect him to the crime.47 Could neurotechnology help?

I started to grapple with the possibility of our brains being used as evidence against us in a criminal trial in a pair of law review articles.48 I laid out a spectrum of neurological evidence that could be used—from personally identifying to automatic, memorialized, or silently uttered information gleaned from our brains. In doing so, I realized that not all brain evidence is equally sensitive, and perhaps not all of it should be off-limits.

We might begin by obtaining physically identifying evidence from the suspect. The police knew that the gunman suffered a blow to the head, so brain activity could reveal head trauma and how recently it had occurred, even when a visual inspection of his head might not. Much like a blood sample, a saliva swab of DNA, or a dental exam, brain activity of this sort can provide identifying evidence about a suspect’s physical state without revealing information about his mental processes.

But the police would need more than identifying information to prove the gunman was involved. They might, for example, try to measure his visceral or emotional reactions to photographs of the victim. This kind of automatically generated unconscious brain information would be more sensitive than traces of a trauma, but still less revealing than the most intimate information we might obtain.

Our brains, after all, preserve not just our present thoughts and mental experiences but also our memories of everyday encounters. These include the people we have met: the timbre of their voices, visual imagery we encountered, and episodic memories of our experiences. The police could play the tape of the victim’s voice and see whether the suspect’s brain reveals recognition.

The most intimate information we might decode from the suspect’s brain, which would most directly rob us of our control over what we share with others, would be the silent utterances, thoughts, visual images, or statements present in his mind. The respondents to my labs’ survey of a nationally representative sample of US residents ranked the thoughts and images in their mind as highly sensitive information, implicitly recognizing the difference in sensitivity between silent utterances and memories in our minds versus the automatic functional information and identifying characteristics of the brain that they ranked as less sensitive.

As we endeavor to define the contours of mental privacy included within the right to cognitive liberty, we should recognize that not all neural data is equally sensitive. While the right to mental privacy should protect the entire spectrum of neural data—identifying, automatic, memorialized, and silent utterances—the individual interest in mental privacy should be the most unyielding when silently uttered or memorialized information is sought. When automatic brain functioning or identifying information is sought instead, mental privacy may at times yield to societal interests.

As vigilant as we should be when it comes to mental privacy, there are reasons we shouldn’t make our brains completely off-limits, and instead balance the interests at stake. We don’t want to unnecessarily constrain neuroscience research by prohibiting the use of any neural data merely to ensure the most sensitive brain data is protected.





There Is Value in Sharing


Balancing our individual interests against society’s will help us define our rights. We’ll explore this process in the context of the modern workplace, education, and other settings in the chapters to come. But individually and collectively, we stand to benefit a lot from what we can learn from our brain wave activity. Large data sets of brain wave activity could enable the promises of precision medicine—from discovering the causes of neurological disease to individualized treatments for afflictions of mind, helping us to eliminate some of the leading causes of human suffering today.

Data sets like those collected by Entertech are unique in that they capture neural data of individuals “in the wild” rather than in more limited and controlled research settings. While such data has limitations in quality because it isn’t well controlled (environment and other factors aren’t as consistent as they would be in a laboratory), and people moving about their everyday lives with neural interface devices may give rise to many recorded artifacts from movement, this kind of data nevertheless offers unprecedented opportunities for understanding brain functioning across individuals and in the real world. And that’s precisely why we need large data sets—so we can understand how noise, device variability, and user differences impact the accuracy of the measurements, and improve the devices and the algorithms that they use.

There are very few big data sets that researchers can use to study the neural activity of healthy individuals in their everyday lives. There have been some large-scale efforts to promote brain data sharing through organizations like the International Neuroinformatic Coordinating Facility in Stockholm and the Neuroimaging Informatics Tools and Resources Collaboratory in the United States.49 But when it comes to raw EEG, EMG, and fNIRS data, “the vast majority of human neuroscientific studies use a very small number of participants employed in very specific tasks,” as Dr. Jonathan Touryan, an army scientist for the US Army Combat Capabilities Development Command’s Army Research Laboratory, puts it. Touryan’s laboratory has teamed up with the University of Texas at San Antonio and Intheon Labs to develop a first-of-its-kind mega-analysis of EEG data.50

EEG is decoded using powerful machine learning approaches, including what’s known as neural networks and support vector machines.51 Widespread use of consumer neurotechnology will allow for the creation of bigger and better data sets that can train more advanced machine learning algorithms. With larger and more diverse data sets, scientists can identify universal features that link neural activity to different brain states and tasks. This isn’t easy to do in EEG and EMG, given all the different devices that are in use—with different configurations, numbers, and locations of electrodes—the different tasks people are performing while wearing them, and the different ways those tasks are annotated by the software powering them.52 But with mega-analysis, it may be possible. This would allow for improvements in the accuracy and predictive quality of consumer neurotechnology, bringing significant benefit to the individuals who use it.

But we can realize these benefits only if our brain data isn’t co-opted by a few big tech giants like Meta, which may choose to keep it proprietary, for its own purposes. To prevent that data grab from happening—and to protect individuals from the misuse of their personal information—we need to establish new norms, put into place clear rules and regulations to protect our mental privacy, and see that they are strictly enforced.

Most people will be reticent to share their neural data unless they are assured that it won’t be used against them. As we explore the growing use of neurotechnology in employment, education, and the government in the chapters ahead, we’ll look at the additional context-specific protections required to enable people to share their neural data without facing discriminatory consequences.

Sharing our neural data creates unprecedented possibilities to solve some of the most challenging health issues we face. But it also introduces unprecedented risks to our right to our own vulnerability, which is one of the many reasons we need to explicitly define the right to mental privacy. In doing so, we can specify precisely who can track our raw brain activity, when they can do so, and for what purposes.





The Right to Mental Privacy


It isn’t too late to secure individuals a right to mental privacy. By recognizing a new right to cognitive liberty and defining its contours, we can enjoy the benefits of neurotechnology while preserving a space for mental reprieve.

Securing mental privacy to individuals will require us to update and broaden our contemporary understanding of the bundle of rights included within cognitive liberty in international human rights law—including mental privacy, freedom of thought, and extending the collective right of self-determination to individuals. In later chapters, we’ll explore self-determination and the absolute right to freedom of thought (which overlaps with mental privacy to protect any unauthorized access to the silent utterances and memories in our brains). Mental privacy protects brain data, too, but less stringently than the right to freedom of thought recognized in international law, since it is a relative right, meaning that it will yield to societal interests when warranted.

The United Nations (UN) has been credited for launching the international human rights movement, galvanized by the atrocities committed during World War II. Although the normative foundation for human rights began well before the founding of the UN in 1945, the adoption of the Universal Declaration of Human Rights (UDHR) in 1948 was a crucial milestone. The declaration outlines thirty rights and freedoms necessary to secure human dignity to every person and forms the basis for modern international human rights law.

The UDHR established important global norms about human rights and created moral obligations for governments, corporations, and individuals to follow and international condemnation when they fail to do so.53 The UDHR is also the source of international treaties—including the International Covenant on Civil and Political Rights (ICCPR)—that define freedom of thought and other human rights with greater specificity. These treaties create international courts and monitoring bodies to review whether governments are respecting these rights. The UDHR and human rights treaties are also incorporated into national laws around the world to help make the rights enforceable.54

The Swiss bioethicist Marcello Ienca and the law professor Roberto Adorno have called on the UN to recognize a new human right to mental privacy “to protect any bit or set of brain information about an individual recorded by a neurodevice and shared across the digital ecosystem.” They see this as a relative right that “can be limited by certain circumstances, provided that some restrictions are a necessary and proportionate way of achieving a legitimate purpose.”55 A group of ethicists and legal scholars called the Morningside Group have similarly called for a new right to mental privacy, as an absolute right, where “any interference with it by States without consent should be considered de facto cruel, inhuman, or degrading treatment.”56

A new right to mental privacy may not be necessary, however. Both privacy and freedom of thought are already protected by the UDHR. Article 12 of the UDHR states that “No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation.” And Article 18 affirms that “Everyone has the right to freedom of thought…” While mental privacy is not explicitly mentioned in these articles, it is widely recorgnized that the interpretation of human rights law evolves as we discover and deliberate about novel challenges to human dignity.57 As changes in society and technology reveal gaps in our interpretation of existing human rights, we can and should update application of human rights in to these new contexts.

Standard setting in international human rights law is an important first step to establishing social norms and moral obligations for governments, corporations, and individuals to follow. Norms have a powerful effect in their own right—actors who violate human rights norms often face international scrutiny and accountability for doing so. But merely updating those rights to make explicit that they include mental privacy will not transform behavior. For standards to become effective, there must also be pathways for enforcement within countries.58

In other words, how the right to mental privacy is implemented will become just as important as its recognition. Implementation will require corporations, governments, academics, and the public to join in deliberating and defining the requirements for the responsible use of neurotechnology and neural data in society.

For example, when Joseph Cannataci, the special rapporteur on the Right to Privacy, presented the final report of his six-year mandate to the UN Human Rights Council in 2021, he advocated for implementing norms for data management to protect society as the use of artificial intelligence proliferates. He also urged minimizing data use to the original purpose for which it was collected, securing people against the use of data to discriminate against people, and improving data accuracy.59

Cannataci’s report and proposals are a good starting point, but they don’t go far enough. This book offers a comprehensive blueprint to recognize a new international human right to cognitive liberty and the right to mental privacy it includes. The chapters ahead discuss how the specific contours of mental privacy will apply in different contexts, and the need to adopt data management practices within countries for the protection of neural data. At a minimum, we should require that corporations be transparent about the neural data they are collecting. Apple has paved the way by developing “data nutrition labels” for the apps in its store, arming users with knowledge of not just how an app is used but how it’s using them.60 We need similar nutrition labels on neurotechnology devices and their related apps so we can know what kinds of neural data are being collected, and how that data is being used. Recognizing a human right to mental privacy in international law will create the powerful norms to incentivize more corporate actions like this one. But greater transparency is not enough.

We must also require through laws, regulations, and norm enforcement that corporations limit further processing of raw neural data to prevent them from mining it for “sensitive” data—including memorialized and uttered information, and even automatic functions of the brain that are more closely aligned with our sense of self. Any access to brain data will violate mental privacy, so it should occur only when consumers have explicitly opted in to that information being processed and only if there is a compelling justification for its use. While opt-in creates much greater friction for companies, it is not only justified but also essential to secure our mental privacy.

Corporations must also create individual user-based controls on the devices and applications themselves. These controls should give users full transparency about what is being collected, stored, and shared, and allow them to switch the devices on and off so they can control what data is being collected and when. EMG watches and EEG earbuds, for example, should have Power-On and Power-Off switches that will allow users to wear them continuously without having to worry about what neural activity is being collected. Similarly, apps should give users the ability to store their raw neural data for processing locally, and to have that data overwritten continuously rather than stored indefinitely on the companies’ servers for further processing by themselves and their partners.

Individuals should also be empowered to share their raw neural data in a de-identified and aggregated form. This will require society to implement standards against the discriminatory use of this data, and to ensure that individuals have rights of redress if that data is misused.

Developing a human right to mental privacy and implementing it across the world will require us to define the kinds of analyses of raw neural data that are permissible through a continuing process of democratic deliberation, a truly inclusive one, that allows for town hall–style forums to discuss issues like data mining. This process will increase public awareness and create a platform for sharing ideas and values. And it will empower participants to work in partnership with decision-makers (including corporations) to implement specific policies that will secure mental privacy to individuals in the domains they care about the most.

Mental privacy is a critical aspect of cognitive liberty. But like all privacy interests, it is not absolute. People can and should have the right to give others access to their brain activity. There will be times when we will want to do so, to promote research or in exchanges for goods and services. And there will be times that society will demand tracking of our brain activity when the lives of others are at risk.