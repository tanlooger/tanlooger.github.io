---
---
---
title: 4 Know Thyself
---






Many scouts believed that Justin Skyler Fields would be a top five pick in the 2021 NFL draft and very likely the first quarterback selected. Fields was an all-star at Ohio State University, a finalist for the Heisman Trophy, and had been named the 2019 Big Ten Offensive Player of the Year. But just eight days before the draft, Justin threw a monkey wrench into the league’s evaluation of him when he went public with his epilepsy diagnosis.1

When Justin was in high school, he woke up disoriented and confused in an ambulance, later learning that he’d had his first epileptic seizure. Fortunately, and like most people with epilepsy, he responded well to medication. In fact, he never missed a college game. “I mean, it’s pretty simple for me to manage it,” he explained. “I just have to take three to four pills a night, every night. It’s nothing crazy. It’s a thing that’s been there for the past seven or eight years, so I’m used to it.”2

With neurotechnology, Justin could soon learn about an impending seizure before it happens through real-time alerts on his smartphone or just-in-time calls from his doctor’s office. He could track his brain activity over time and share it with team scouts to show that his condition is well-managed. But whether individuals should have direct access to their own brain data or rely on their doctors instead to learn about their brain functioning is a question we will soon have to address as neurotechnology becomes more widespread. After Justin’s announcement, he fell to the eleventh pick overall and the fourth quarterback off the board, selected by the Chicago Bears. The Bears’ general manager, Ryan Pace, said that his team’s medical evaluators had gotten in touch with their peers at Ohio State and decided that they were “completely fine with it,” having “dealt with something similar in the past with different players over the years.”3 But other teams without similar experience with epilepsy may have been more hesitant to draft him.

It’s not surprising that a star football player would suffer from epilepsy. The most common serious brain disorder, epilepsy affects about fifty million people worldwide.4 Epileptic seizures are associated with short bursts of abnormal electrical signals in the brain. A so-called grand mal seizure renders a person unconscious and sends them into convulsions, but not all seizures are that dramatic; some can cause a brief feeling of disorientation, while in other cases a person may appear normal and awake but is actually in a trance. Any kind of seizure could be extremely dangerous for someone driving a car—or playing pro football.5

That’s the kind of informational uncertainty that the teams likely grappled with after Justin’s bombshell announcement. His draft stock likely fell because not all seizures are as well controlled as his. It was no secret to the league that in 2007, the Baltimore Ravens’ standout safety player Samari Rolle had suffered three major seizures that kept him out of six games.6 An adjustment to Rolle’s medication allowed him to play again but losing a star quarterback for several games could be devastating for a team’s playoff potential.7

About a third of adults and about 20 to 25 percent of children with epilepsy do not respond well to conventional anti-seizure medications. If two different medications are tried and both fail to keep them seizure-free, their epilepsy is classified as drug-resistant. There aren’t a lot of great options for people in that category. They could undergo surgery to remove the part of the brain that is causing the seizures, be put on a special diet, take immunotherapy drugs with serious side effects, or rely on benzodiazepines that become increasingly less effective as their brains develop a tolerance to them.8

Consumer neurotechnology may change that picture. EEG has been effectively used to diagnose and manage patients with epilepsy, and machine learning algorithms can use brain data to discern neural signatures of imminent seizures.9 Researchers from Ben-Gurion University of the Negev in Israel have already developed a wearable EEG device called the Epiness, which they claim can predict seizures up to an hour before they occur and send a warning to a smartphone.10 Clinical trials are underway.

Fitted with a device like Epiness, a person with epilepsy could drive to work confident that they won’t have an accident on the way. A sufferer of drug-resistant epilepsy could take benzodiazepines on an as-needed basis to mitigate an impending seizure, preventing the risk of tolerance that develops with continual use.

The potential of these devices is astonishing but having direct access to our brain data is threatening to a deeply entrenched system in which information about our bodies is filtered to us through intermediaries. Regulators, physicians, and even most bioethicists(!) still believe that consumers lack sufficient medical literacy to correctly interpret data about their own brains and bodies and are thus prone to making dangerous choices. The solution, they say, is to restrict how much information consumer devices and apps can display.

Regulators worldwide will classify Epiness and devices like it as medical devices, subject to stringent premarket and post-market controls. That means extensive premarket testing on the safety and efficacy of the product before it can be marketed, and the requirement that only experts can have direct access to its software. As consumers, we will of course want reassurance that Epiness and similar devices are accurate. But if they do what their makers claim, should an expert be the one to decide whether and how we can use them?

Many doctors, regulators, and cultures believe that information about our health and medical treatments should be filtered and contextualized, and that to do otherwise causes unnecessary and avoidable harm, while others believe that the patient’s right to informed consent requires sharing with them more fully. It’s time we critically examine these claims. Can having access to accurate information about our brains actually harm us?





From Meditation to Alzheimer’s Disease


Dr. Richard Davidson, a neuroscientist at the University of Wisconsin–Madison, had been meditating for more than forty years when he redirected his research to the effects of meditation on the brain. This shift in his work occurred at a meeting with the Dalai Lama, who wondered at the focus of his research: “You’ve been using the tools of modern neuroscience to study depression, and anxiety, and fear. Why can’t you use these same tools to study kindness and compassion?”11 This question would lead him on a journey to find out just how much we can learn from EEG.

Davidson and his colleagues began by flying Yongey Mingyur Rinpoche, a Tibetan monk and a master of the practice of mindfulness, from Kathmandu, Nepal, to Madison. Using a sophisticated clinical-grade EEG setup, the researchers monitored Rinpoche’s brain wave activity as he alternated between meditations on compassion and thirty-second rests.

As Mingyur began his first meditation, the researchers saw a huge burst of electrical activity on the screen.12 At first, they assumed that Mingyur must have moved, because movement creates artifacts that often confound accurate EEG detection.13 But they soon discovered that this burst of electrical activity occurred every time Mingyur meditated on compassion. When they repeated the experiments on twenty-one other Buddhist monks, they discovered the same thing. Davidson and his colleagues proved not only that EEG can detect the neural changes that occur during meditation, but that meditation has powerful and lasting effects on the brain.14

Other researchers have conducted similar experiments since. In one study, researchers recorded the brain waves of Buddhist monks in the northeastern region of Thailand via a Muse EEG headband. The monks answered questions, read books, and meditated, and based on their brain wave activity alone, the researchers could tell which activity they were engaged in.15

As related in the story that opens this book, today, companies like InteraXon are marketing EEG headsets to allow consumers to “see” if their subjective experience of meditation aligns with the brain wave data their devices decipher. While some studies give reasons for caution about the accuracy of the data the devices report, others have shown that people enjoy significant increases in mindfulness after using them, leading to reductions in stress, heart rate, and an improved sense of well-being.16 That monitoring could also reveal more devastating information, for example, that a brain tumor has started to grow.

Diffuse gliomas are the most common and aggressive form of brain cancer. There are very few effective treatments; the relative five-year survival rate for diffuse midline glioma is less than 50 percent. While many factors affect a person’s prognosis, early detection, especially in younger patients, is critical, because it allows for intervention before it spreads. But traditional screening strategies generally fail at early detection. Case reports and serial brain scans show that small brain lesions can evolve into established disease in as few as sixty-eight days.17

Herein lies the promise and peril of portable EEG devices for individuals. About 68 to 85 percent of brain tumor patients have abnormal EEG profiles when they are first diagnosed. We aren’t there yet, and we have a way to go to even learn if this is possible, but someday, regular monitoring with consumer devices may allow us to catch glioblastomas in their earliest and most treatable stages.18 They might be able to catch a host of other diseases, as well. The South Korean biomedical startup iMediSync is already marketing an EEG device that can detect the mild cognitive impairment that heralds early Alzheimer’s dementia with 90 percent accuracy as well as other neuropsychiatric disorders, such as Parkinson’s disease, traumatic brain injury, PTSD, ADHD, depression, and more.19 The data the device captures is sent directly to physicians and specialists, who then inform the patients of the results. Having completed its clinical trials in South Korea, iMediSync has received regulatory approval from the Ministry of Food and Drug Safety (MFDS).20

But is it in fact better to route all “bad news” through an expert like a physician? Would we be better off routing it through someone trained in empathy and compassion? Or letting patients decide whether they want to learn about it themselves first, through a connected app and smart device? And do some patients fare better if they don’t know about the diseases that may be lurking in their brains?





A Good Lie


When I left North Carolina for college, I doubted I would ever move back to the South. But when I was getting ready to attend graduate school, my mother was diagnosed with myelofibrosis, a very rare and progressive form of bone-marrow cancer, and I knew that I had to be close to her. Which is why I received my JD, MA, and PhD from Duke.

Since then, I have thrown myself into learning as much about my mother’s illness as I can. I’ve read everything I can find about the disease, its available treatments, and my mother’s prognosis. This is how I navigate uncertainty in life—by arming myself with knowledge. With this knowledge, I have been able to advocate for my mother, joining her for many of her medical appointments and even reviewing her regular blood test results.

My father, a retired physician, has also been deeply involved in her care, and it is through our mutual work together that I have learned something the medical journals and textbooks leave out: how to tell a good lie.

When my mother’s blood cell counts started to plummet, a sign of disease progression, I started to explain this to my mother, drawing a sharp look from my father. He took me aside and admonished me that a physician learns when to share, but also what not to share with their patients. What good would it do to tell her the implications of those results? “Medicine is about much more than treating a patient’s condition,” he said. “It is about learning to shield a patient from information that won’t serve them well.”

I was taken aback. But when I researched the question, I learned that his view is not uncommon. Like many physicians of his generation and cultural heritage, my father believes that it is not only unnecessary to share some information with patients, but that doing so may harm them.

Eric Topol, the founder and director of the Scripps Research Translational Institute, and author of best-selling books on the future of medicine, writes elegantly about the history of this perspective in his 2015 book The Patient Will See You Now, tracing the idea that physicians should patronize their patients and decide whether and how much information to disclose to them from 2600 B.C. to the present. Topol frames our present-day informational asymmetry as one in which “Doctors have all the data, information, and knowledge. Patients can remain passive or ignorant of their medical information, or, if they choose to be active, they typically have to call repeatedly or beg to get their data.”21 The reason for this, he explains, is because their doctors are “trying to ‘protect’ the patient by not disclosing adverse, anxiety-provoking information.”22

Underlying this approach is the assumption that only an experienced physician, with years of training and unique knowledge of the patient’s medical history, has the context and knowledge to know what information to share and when. And that they are in the best position to balance when and whether the benefits of a device or treatment outweigh its risks.23

This approach presumes that some kinds of information—like information about your brain functioning—are so arcane that the average layman can only react emotionally to it; that they lack the objectivity and hence the autonomy that they need to confront it directly and responsibly. Having access to truthful information about your brain functioning may cause you unnecessary anxiety or psychological distress, the theory goes, or cause you to pursue harmful medical treatments to mitigate risks that you don’t understand.24 Or you might waste precious medical resources by seeking further evaluations that you don’t need.

As a legal ethicist, I struggle factually and philosophically with these claims. First, the empirical research shows people adapt quickly to information about themselves. Worse still, the information denied is critical to making choices about how they will live their lives. And yet, as a daughter who deeply loves her mother, I can’t help but notice that my mother doesn’t ask very many questions about her prognosis. She seems to actually do better when she believes that her disease is under control. In a way, she is asserting a right of self-determination; for her, it may be better not to know, or she may not want to know. But does her subjective experience bear on whether you or your mother should have the right to informational self-determination?

Consider whether you would want to know if you were about to have an epileptic seizure. If so, would you prefer to hear about it through a phone call from your physician, or a real-time alert on your smartphone, particularly if the availability of the latter allowed you to drive or play sports or do other things that are part of a full life? How about whether you are meditating properly? Doctor or cell phone app? What about if a brain tumor has taken root or if you are suffering from the earliest stages of Alzheimer’s disease? Does your preference for what you learn and how you learn it vary by the kind of information that is in play?

Consumer neurotechnology will force you to grapple with these choices. Regular monitoring of your brain wave activity could allow you to “see” that you have achieved a meditative state, but also that you may be showing signs of early Alzheimer’s disease. Does the right to informational self-determination include the right to decide how you learn about either one?





To Know or Not to Know


Questions about whether and when to share “bad” news remind me of a recent film, The Farewell, which follows a Chinese family who decide not to tell their grandmother Nai Nai that she is dying of lung cancer.25 Using the pretense of a cousin’s wedding, the family gathers in China to say their final goodbyes.

The main character, Billi, struggles to reconcile her family’s deception of Nai Nai with her own personal guilt in doing so, underscoring the narrative tension the film draws between contrasting moral frameworks—Western individualism and Eastern collectivism.

To Western audiences more accustomed to the idea that individual autonomy reigns supreme, the deception of Nai Nai might seem incontrovertibly wrong. But that deception is both familiar and demanded by many Eastern cultures. In countries like Iran, China, Singapore, Japan, and Lebanon, and even in immigrant subpopulations within the United States like my parents’, the practice is common. Many Iranian and Chinese families object to sharing a “bad” diagnosis or prognosis with a patient, and some experts recommend that the wishes of the family be respected. The deception is both well meaning and consistent with an ethical norm of non-maleficence—avoiding unnecessary harms to individuals. These cultures believe that individuals may die faster when they know they have cancer, because of the fear it instills.26

A recent survey of Chinese doctors found that 98 percent of them would tell family members about a cancer diagnosis before telling the patient, and 82 percent would follow the family’s wishes as to whether the patient should be told.27 While the Western approach to disclosure of information is now different, it hasn’t been that way for very long. A 1961 study in Chicago surveyed doctors on this same question. Ninety percent said they wouldn’t inform a cancer patient of their diagnosis, and that they would deliberately mislead them to protect them.28

Recent studies have shown that knowing about a terminal diagnosis doesn’t shorten a person’s lifespan—and that knowing better enables a person to make treatment decisions, end-of-life care choices, and personal choices about their remaining life and estate.29 In fact, a recent meta-analysis of twenty-three different studies on the issue—which reviewed 11,740 records—found no evidence that uninformed cancer patients had a better quality of life or fewer disease-related symptoms than informed cancer patients. In fact, informed patients showed greater vitality than uninformed ones.30

The same finding—that people do better when they know the truth about their condition—has emerged in research studies again and again, even when the information is as dire as having a high risk of developing Alzheimer’s disease. In the case of 23andMe discussed in chapter 1, the FDA expressed concern that people would react poorly to learning exactly this kind of information. The empirical evidence shows otherwise. For most people, knowing about their risk of Alzheimer’s disease (based on apolipoprotein E genotyping) does not increase their risk for psychological harm. Often it leads them to have more positive feelings about the risk assessment experience. Of course, most people are distressed in the short term. But this temporary distress doesn’t lead to an increase in anxiety or depression overall. And the same people who proactively sought out their risk of developing Alzheimer’s disease are more likely to use that knowledge to adopt healthier lifestyle habits and engage in better long-term planning.31

The weight of this empirical research, together with shifting norms, has created a growing trend of more honest communication with patients.32 Informed consent laws in the United States, Europe, and China, as well as international human rights laws, require full and complete disclosure of information to patients about their conditions. These laws have evolved over the past fifty years from a focus on what a reasonable physician would find necessary to disclose to a patient to what a reasonable patient would want to know, based on their right of self-determination.33 This legal trend follows a demographic one in which younger people expect—nay, demand—access to information.34 Generation X and millennials are far less likely than the generations before them to trust health-care providers, and far more likely to trust self-directed information.

While we can’t yet be sure how people will react to what they learn about themselves from consumer neurotechnology, we can see that “good lies,” even about serious diseases, are hardly a better alternative. Consumers are more likely to benefit from informational self-determination over their own brain data. Of course, not everyone will choose to know. My mother says she would not. But a right to informational self-determination gives a person that choice.





Headwinds Against Change


Less than a decade ago, it would have been unthinkable for a person with a healthy brain to track their own brain activity. If you were curious about your brain, you would have to see a specialist, who would order imaging only if he or she believed it was medically warranted. Further, the tests would provide only a snapshot of the brain at a given moment.

But thanks to the proliferation of new technology, a cultural shift is underway; consumer-based self-quantification is fundamentally changing how we learn about our biological selves. If you walk into your local pharmacy, you can buy home testing kits and devices that track your heart rate, sleep patterns, blood sugar, moods, and even your attention span.35

With every step forward, there are myriad attempts to turn back the clock, reinserting “experts” between consumers and their data.

If you suspect you have a fever, more than likely you’ll just grab a thermometer from your medicine cabinet and take your temperature. But in 1867, when England’s Sir Thomas Allbutt invented the first practical clinical thermometer, only a physician could read the results.36 Fifty years ago, regulators and physicians believed that laywomen lacked the competence to self-administer pregnancy tests and react to them appropriately.37 British and Canadian regulators didn’t approve do-it-yourself home pregnancy tests until 1971,38 while the United States’ FDA held out until 1976.39

Rapid HIV home tests followed this now familiar arc. The OraQuick In-Home HIV Test allows individuals to test themselves or a partner (or would-be partner!) for HIV and have an answer in about twenty minutes. But during the HIV/AIDS crisis in the late 1980s, regulators worldwide took a hard stance against HIV home testing, dogmatic in their view that consumers needed those results to be contextualized by a physician lest they react in “hysterical or irrational ways, such as committing suicide.”40 The FDA waited twenty-five years before it approved the first home test kit for HIV in 2012,41 and it wasn’t until 2015 that many other countries followed suit, including England, Scotland, Wales,42 and China.43

In each of these cases, consumers ultimately received unfettered access to the new technologies—a reason for optimism when it comes to neurotechnology with appropriate safeguards for our mental privacy. But when we look at the recent debates around direct-to-consumer (DTC) genetic testing technology, the future looks much less rosy.





One Small Step Forward, One Giant Leap Back


In 2006, after attending a scientific conference, the US biologist and entrepreneur Linda Avey took an audacious gamble. Now that it was possible to extract DNA from saliva as well as blood, she could massively scale the amount of genetic material available to scientists, allowing researchers to learn much more about genetic contributions to human traits and diseases, while at the same time empowering people to learn more about their own genomes. All she had to do was invite people to spit into a plastic tube.44

Linda pitched the idea to her former boss at Perlegen Sciences, Paul Cusenza, who was just as excited as she was. They worked furiously over the following months to develop a business idea and started shopping it to potential investors. When they pitched it to entrepreneur Anne Wojcicki, she became the third cofounder of the company that became 23andMe. When the marketing team asked the founders for one word that described their product, Linda replied, “Audacious!”45 23andMe boldly challenged the expert intermediary system by marketing its product directly to consumers. At first, it offered limited genetic sequencing paired with regularly updated reports about variations a person had at points along their genome, and what those variations meant. They started by reporting on fourteen different conditions and soon expanded to 254 disease and health predispositions.46

As an early adopter of the 23andMe service, I enjoyed a much richer ecosystem of informational offerings than is now available, including my unsurprising risk of rheumatoid arthritis (my father has it), scoliosis, and Hodgkin’s lymphoma, and my decreased risk of Alzheimer’s disease, stomach cancer, and celiac disease. I received my carrier risk status for more than fifty different diseases, my likely responses to twenty-five different drugs, and information about nearly sixty different genetic correlations to traits such as my likelihood of avoiding repeat errors and degree of caffeine consumption. 23andMe also specified their level of confidence in my report, quantified my personal risk level relative to the population, and provided me with a genotype summary and all the relevant citations to learn more. The confidence level on many of those early reports was low and the research quite thin, so I didn’t know what to make of that data. I was bummed to learn I had a higher risk of rheumatoid arthritis, but I haven’t lost any sleep over the information, and so far, haven’t developed it. Equipped with the data and the research that informed their insights, I could decide what I thought about the information they reported.

But there was a storm coming. Linda left the company in 2009. Shortly afterward, motivated by the sector’s increasingly health-related claims, the FDA started to increase its oversight of DTC genomic testing companies,47 warning that it could classify their services as medical devices subject to stringent premarket approval requirements that would be nearly impossible for most companies to meet.48

Between 2010 and 2012, the FDA ramped up the pressure. 23andMe worked with the agency to classify its product as a lower-risk medical device, which would have allowed it an easier pathway to regulatory approval.49 Then its general counsel left, and the lone remaining cofounder, Anne Wojcicki, inexplicably stopped responding to the FDA.50 At the same time, the company launched a major national TV campaign, touting the purported health benefits of the product—including your risk of celiac disease—and encouraging consumers to use the service to discover their risks.51

We will never know what might have been if 23andMe had not ghosted the FDA. What we do know is that on November 22, 2013, 23andMe received what is known as a “warning letter” (a regulatory enforcement action against the company) that described its personal genome service (PGS) as an unapproved Class III device—meaning that it needed to go through stringent premarket safety and efficacy testing—and ordered the company to “immediately discontinue marketing” it.52 Despite its multiyear engagement with 23andMe, the FDA said that it lacked “any assurance that the firm analytically or clinically validated the PGS for its intended uses.”53

An earlier paragraph in that now infamous letter suggests a different reason for the FDA’s action, which was concern that the average customer could overreact to some of the information they received. “For instance,” the letter stated, “if the BRCA-related risk assessment for breast or ovarian cancer (like the one [Angelina] Jolie received from her physician) reports a false positive, it could lead a patient to undergo prophylactic surgery, chemoprevention, intensive screening, or other morbidity inducing actions.”54 In other words, the FDA worried that “patients relying on such tests may begin to self-manage their treatments,” which raises “serious concerns … if test results are not adequately understood by patients or if incorrect test results are reported.”55

My Duke colleague Misha Angrist, a bioethicist, called out the FDA for being “borderline absurd” in assuming that a woman would get a double mastectomy based on a cheaply available consumer test without following up with a physician—or that a surgeon would operate solely on that basis.56 But the now all-too-familiar concerns about consumer health literacy had hijacked the dialogue.

23andMe tried to hold out, but on December 5, Wojcicki announced that the company would stop offering the service.57 When she later clarified that the company would still offer existing customers their raw DNA data,58 I rushed to download it, along with an archive of my old reports. 23andMe has since become a poster child for regulatory compliance. Whereas before, you could have access to hundreds of reports, now only a few dozen are made available to consumers.59

Other countries have enacted onerous laws to restrict consumer access to genetic testing in the name of consumer safety, adding layers upon layers of restrictions, including mandatory supervision of genetic testing and limitations on the way it is performed. France, Hungary, and Germany have mandated that it can be performed for health-care purposes only, with a medical prescription and undertaken by an authorized laboratory. Other countries impose mandatory genetic counseling before consumers can receive their reports. South Korea strictly limits certain kinds of genetic tests, such as those that predict physical characteristics or personality traits.60

Most doctors and geneticists support these restrictions, believing that the average person lacks “health literacy,” which undermines their autonomy and renders any right of access to their own data meaningless.61 In my view, the FDA’s actions against 23andMe were a tragic defeat for patient empowerment and a threat to freedom of speech by restricting our free access to information.62 Alongside other academics, like Gary Marchant, the Regents Professor of Law at Arizona State University, I have argued that consumers have a right to the information contained in their own genes and that the FDA’s approach was appallingly paternalistic. The legal ethicist Barbara Evans emphasizes that the moral principles upon which bioethics rests define “autonomy as the capacity to make one’s own decision, regardless of whether society would see that as a good decision. Autonomy is about the right to be wrong.”63

In other words, while there is an important information-forcing role for regulators to play to ensure consumers receive robust safety and efficacy data about drugs and devices, it should do so to inform their choices, not prevent them from making one.

Now a growing chorus of experts are worried that consumers will misuse or misinterpret their brain data.

Anna Wexler, University of Pennsylvania professor of Medical Ethics and Health Policy, advocates for increased regulatory oversight because consumers are “left to themselves to evaluate the veracity of neuroscientific claims—a task in which very few are well trained.”64 Scholar and communications expert Caitlin Shure wrote to the FDA to call for “proactive safeguards against unsubstantiated claims by neurotechnology manufacturers” to protect “vulnerable consumers.”65 Shure argues that our “understanding of brain electrophysiology is extremely limited,” and that consumers lack the “base of intuition with which to discriminate the value and validity of neurogadgets.”66

As Jack Nicholson famously cried out in the movie A Few Good Men, this growing chorus of experts believe that when it comes to your own brain data, “You can’t handle the truth.” The FDA’s regulatory approach suggests they may well agree, foreshadowing a straitened future for consumer neurotechnology applications.

The FDA has already rated EEG,67 EMG,68 and fNIRS products as Class II medical devices, meaning that it considers them to pose moderate to high risks to consumers, and thus are subject to specific regulatory controls rather than being treated as “generally safe” and okay to market just by notifying the FDA. While this is still less onerous than Class III devices, in that it does not require premarket safety and efficacy studies, manufacturers must file premarket submissions to the FDA to show that their devices are safe and effective or substantially equivalent to other already-approved products. Until the FDA gives a manufacturer its okay, the manufacturer cannot market or sell a new Class II device.

There is an exception in the rules that manufacturers have tried to exploit to bypass these premarket regulatory controls. The FDA does not regulate “general wellness” products designed to promote “a healthy lifestyle.”69 If a product is intended for “relaxation or stress management,” “mental acuity,” “concentration,” or enhancing “learning capacity,”70 it can be sold directly to consumers without premarket controls or prescriptions. But if and only if the product is intended solely for general wellness use.71

As a result, the vast majority of manufacturers limit their marketing claims to those “wellness” categories. They market products that allow you to wriggle EEG-powered cat ears with your emotions,72 improve your golf game through neurofeedback,73 play video games by thinking about moving around the screen,74 hone your focus by mentally opening and closing a flower,75 monitor your sleep activity,76 meditate with biofeedback,77 or navigate AR/VR78 hands-free. But don’t let this fool you into believing that your access to neurotechnology isn’t at risk. Despite calls for reassurance from the industry, the FDA has not promised that consumer neurotechnology qualifies for this loophole. In its “general wellness” guidance document, the FDA doesn’t mention a single neurotechnology device as “low-risk,” and it has ignored explicit requests to do so.

And we shouldn’t ignore the more worrisome message that is lurking within existing regulatory guidance. Consumer neurotechnology holds much greater promise than playing games with our minds; those devices can potentially revolutionize how we diagnose and treat neurological conditions. But when Dr. Daniel Johnston, formerly of the Pentagon’s Executive Medicine Office of the Surgeon General and now the cofounder of BrainSpan, an integrated nutritional and functional brain health assessment system, asked the FDA to include terms like “cognitive/brain performance” (in addition to concentration), or “tracking brain performance” (when it mentioned “mental acuity”), or language like “detect cognitive decline or change as a result of aging or injury” within its wellness guidance documents, the agency declined to do so.79

When 23andMe starting advertising its product as a tool to learn about our disease risks, regulators worldwide shut it down. As consumer neurotech manufacturers begin to do the same, will they face the same fate?





Using Health Literacy as a Sword


The common thread in all these debates is whether you have a right to unmediated information about yourself.

I believe traditionalists are on the wrong side of history. The view that consumers are too health illiterate to justify self-access to their brains belittles the average person and denies them the opportunity to become more educated.

To teach a person to read, you wouldn’t take away their books; you would teach them how to read them. Requiring an expert to filter all our data is like allowing only experts to read books to the illiterate out loud. Moreover, a troubling aspect of health literacy claims is the historical backdrop of using literacy as a tool to disenfranchise people. Echoes of those efforts reverberate through calls to deny individuals tracking of their own brain activity. The wrong side of history, indeed.

Using consumer neurotechnology, we may learn hopeful and fascinating and even potentially upsetting facts about ourselves. But that is no reason to suppress or limit access to that information if it is trustworthy. None of this is to deny that those experts and regulators have an important role to play when it comes to neurotechnology—we need their carrots and sticks to incentivize manufacturers to conduct robust safety and efficacy testing of their products and disclose those results to consumers. That empowers consumers, rather than disenfranchises them.





Your Right to Know Thyself


The philosopher Aristotle believed the self is in the heart, while René Descartes argued the self is embodied in a soul separate from the body.80 But when the Yale psychologist Christina Starmans and her colleagues showed children and adults pictures of flies circling around their bodies and asked them to point to the pictures where the flies were closest to their “selves,”81 the participants invariably pointed to the ones where the flies were closest to their eyes. Other research shows that this holds regardless of age or culture.

Our strong intuition that our sense of self is inextricably bound up with our brains makes self-access to our brain activity uniquely important to our self-determination. When Neo wanted to understand his fate in the epic science fiction movie trilogy The Matrix, he went to the Oracle to find out. The Oracle helped him learn that he had to discover the truth himself, by looking inward. While who we are is importantly different from the data produced by our brains and nervous systems, access to that essential information about ourselves is central to the self-reflection and self-knowledge we need to develop our own personalities—a right that has been explicitly recognized by the European Court of Human Rights (ECtHR) as part of the privacy protected in Article 8 of the European Convention on Human Rights in the case of Satakunnan Markkinapörssi Oy and Satamedia Oy v. Finland, where the court explained how “The protection of personal data is of fundamental importance to a person’s enjoyment of his or her right to respect for private and family life,” and found that “Article 8 of the Convention thus provides for the right to a form of informational self-determination, allowing individuals to rely on their right to privacy as regards data which, albeit neutral, are collected, processed and disseminated collectively…”82 That approach should inform an updated understanding of the Universal Declaration of Human Rights to include a broad international right to informational self-determination.

The German Federal Constitutional Court first defined the right to informational self-determination in a 1983 opinion that described “the authority of the individual to decide himself, on the basis of the idea of self-determination, when and within what limits information about his private life should be communicated to others.”83 A proactive aspect of informational self-determination is our right to access and record our own personal information. “This right,” explains the Argentinian philosopher Gabriel Stilman, “includes the power to record or collect data” that we want to document about ourselves, including, for example, our own “identifiable brain activity.”84

While the right to information has been most fully realized as a right to information held by governments, the special rapporteur for Freedom of Expression and Opinion has called for an expanded understanding of Article 19 of the Universal Declaration of Human Rights, the right to “freedom of opinion and expression” and to “receive and impart information and ideas through any media and regardless of frontiers” to include the right of information “held by public bodies in the broadest possible terms.”85 People should “be able access adequate, accessible and necessary information as soon as it is known.”

I believe the right to obtain and record your own brain activity should be included within our understanding of the right to privacy and right to freedom of opinion and expression in international human rights law, as an essential precondition to cognitive liberty. General Comment 34, last updated in September 2011, recognizes the right of every individual “to ascertain in an intelligible form, whether, and if so, what personal data is stored in automatic data files, and for what purpose.”86 International human rights law should be interpreted to include access to brain data collected automatically by neural devices should we wish to receive that information. Without the right to informational self-determination, other important rights, like mental privacy, and freedom of thought and speech, are put at risk. How can we protect freedom of speech, if we cannot record, show, and then share information and circumstances from our own brains?87

Of course, our right to informational self-determination is more meaningful if the enforcement of that right includes specific provisions to ensure the information in question is accurate. And manufacturers are perversely incentivized to be the first to market—even if that means their claims are inaccurate or misleading. As a society, we should combat misleading claims by companies about their products and penalize companies that consistently produce inaccurate data.88 To do so is to mandate a corporate duty of discovery and effective disclosure: to honestly label their products, do their part in educating consumers, and only sell products that have been vetted by objective parties to ensure that they do what they say they do. But there are far better ways of doing this than denying consumers access to these technologies. Regulatory and market mechanisms can nudge companies toward greater transparency and self-reporting. Fines and enforcement actions can be taken against companies that make false or misleading claims. And responsible companies will help to educate consumers about what their results could mean.

Linda Avey described to me her pride in the fact that 23andMe offered videos and other educational materials to consumers that could help them contextualize difficult news. Technology enables manufacturers to deliver similarly useful information, like peer-reviewed journal articles, in a just-in-time manner. To do so effectively, they must communicate using clear and effective language, delivered through media most likely to help consumers understand the information being provided to them.

Third-party solutions from other industries can serve as models. While the dietary supplement industry is largely unregulated, organizations like ConsumerLab.com have emerged to deliver high-quality information to consumers about different supplements and their effects, and conduct independent testing to empower consumers’ decision-making.

Regulators have an important function to play by putting effective incentives for informational disclosure into place for drug and device manufacturers. Often market mechanisms fail to nudge manufacturers to be fully transparent about the limitation of their products. Just as nutrition labels are standardized to help consumers compare the nutritional contents of different foods, consistent labeling requirements by neurotech manufacturers would enable consumers to compare products against each other. Such labels would make details such as how many electrodes are used, what is being recorded, the error rate in detecting brain wave activity, what brain wave “bands” are being recorded, where the electrodes are placed, how accurate claims are about the associations between brain waves being detected and the function being measured. (If a device claims to detect seizures, for example, how often is a positive reading correct? How often is a seizure missed? How often is the prediction wrong? What is the data that supports those claims?)

Consumer protection agencies should continue to take action against manufacturers that make false and misleading claims, while implementing a series of carrots and sticks that would require manufacturers to avoid making false and misleading claims and to disclose information that would guide consumers in effective ways. With a recognized right to cognitive liberty, we can blaze a better path forward.

But self-determination is about more than our right to access information regarding our own brains. To define the contours of self-determination over our brains and our mental experiences, we need to decide if we can not only track our own brains but also hack them. And what rights, if any, have we against being hacked by others?