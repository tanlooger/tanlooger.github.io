---
---
---
title: 6 Braking the Brain
---






Beatriz Arguedas, a subway conductor in Boston, Massachusetts, was driving her normal route on the Red Line on September 30, 2005, when a man jumped off a platform as her train pulled into a station. “We sort of made eye contact,” she recalls. “I felt the thud from him hitting the train and then I heard a cracking sound underneath the train. And then, of course, my head starts thumping.”1

Traumatized, Beatriz sought help in the ER, where she encountered Dr. Roger Pitman, a psychiatrist at Harvard Medical School who studies post-traumatic stress disorder (PTSD). Neither Pitman nor anyone else could predict whether Beatriz would go on to develop PTSD. But if she did, even years later the smallest trigger—a sound, a smell—could bring back the full horror of what she had just experienced. Faced with that uncertainty, Pitman suggested she enroll in an experimental study for off-label use of propranolol (a drug that is usually prescribed for high blood pressure). If she took it immediately, he told her, her brain might not store all the emotional memories of what she had witnessed.2

The hippocampus functions like the RAM (random-access memory) of a computer. It processes and temporarily stores new memories for a short time before they are transferred to the cortex, where consolidation for long-term storage occurs. When traumatic events occur, our bodies release stress hormones like adrenaline, which enhance memory consolidation. This is why people remember traumatic experiences so vividly, despite being unable to remember what they had for breakfast the day before. This trait likely evolved over many generations; the advantage it confers is that those strong memories will help us to avoid similar situations in the future. But the disadvantage can be crippling anxiety, insomnia, irritability, and self-destructive behavior. Dr. Pitman’s trials were designed to find out if propranolol could disrupt Beatriz’s memory files before they were consolidated, reducing the likelihood that she would develop PTSD.

Surely, if the drug is safe, there is no moral or other reason Beatriz should not have the option of reducing her future suffering. But should the choice be hers alone? What if there are social costs to her doing so, such as making her an ineffective witness in a police investigation? Or if the side effects change her personality in ways that make her a danger to others? Does it matter whether the drug diminishes or enhances her brain functioning if it helps her cope later on?

Some of these questions were lurking in the recesses of my own mind when I took propranolol for the same reason. Our second child, Callista, had been hospitalized after contracting respiratory syncytial virus. As the days she spent in the this is the pediatric cardiac intensive care unit (PCICU) stretched into weeks, it became harder and harder for me to process the unfolding trauma. After a particularly harrowing night in the PCICU, I asked my neurologist to prescribe propranolol in the hope that it might help me endure, and perhaps lessen the likelihood of my developing PTSD, and he did. Our daughter did not recover; she died on Mother’s Day in 2017.

While the early reports about propranolol were encouraging, it didn’t work for me; I did go on to develop PTSD, suffering from vivid and visceral flashbacks of images from our first visit to the emergency room to Callista’s gut-wrenching cries in the weeks that followed. I’m not alone in my failed response to propranolol. A recent meta-analysis suggests that it has no beneficial effects on PTSD.3 Samuel Schacher, Emeritus Professor of Neuroscience at Columbia University, explains that “a given memory is very sparsely encoded. What that means is that our cerebral cortex, where most of these memories are stored, has about fifteen billion nerve cells, and a particular memory may involve a change of only a couple hundred of them. Finding those few hundred cells is very, very complicated.”4

Neurotechnology may offer a better hope. In one technique, known as decoded neurofeedback (DecNef), an individual lies inside a functional magnetic resonance image scanner while recalling a traumatic memory so that machine learning algorithms can map the precise areas of the brain that it activates.5 Once the neural mapping is complete, the person “erases” those memories through implicit neural feedback.6 Just as I learned to make the birds chirp when I meditated with neurofeedback, people learn to achieve the target brain state by a process of trial and error—thinking of a scary movie, singing a song in their head, or doing a math calculation. Pretty quickly, they successfully induce it without being consciously aware of the trauma that it is associated with, and receive a reward, like a visual representation of a rising thermometer or hearing the sound of birds chirping. Through this process of repeated implicit memory reactivation paired with rewards, the individual retrains their brain.7 Instead of revving the brain, the idea here is to brake it. I haven’t tried it yet; should it become more widely available, I will.





We Allow Brain Braking All the Time


People brake their brains in all kinds of ways. Alcohol’s brain-braking effects are well documented. It interferes with activities in the frontal lobes, limbic system, and cerebellum, leading individuals to feel disinhibited and act impulsively.8 Many people who get drunk do so for those very reasons.9 A college student at a party gets drunk purportedly to relax and have fun, a lawyer has a nightcap after a long day at the office, and my husband, Thede, and I join our monthly “wine club” as an excuse to connect with our friends in a social setting.

Many societies across the world permit the consumption of alcohol, with some important limitations. So long as you are of age, not driving, and not making a public scene, “being drunk” is entirely permissible in many cultures—perhaps even encouraged.

Of course, the United States and many other countries have tried to prevent individuals from consuming alcohol, and some societies and cultures still do so today. In 1919, the US Congress passed the Eighteenth Amendment, outlawing the manufacture, transportation, and sale of alcohol. In a piece written in support of Prohibition at the time, the Reverend Floyd W. Tomkins described “the menace of intoxicating drink to the peace and safety of a community.”10 The behaviors of drunken individuals have not changed since those days, but societies’ willingness to accept such behaviors has; many states are now legalizing the recreational consumption of marijuana as well.

Does cognitive liberty include the freedom to choose to diminish one’s brain and mental experiences, just as it includes the right to enhance them? The battle over motorcycle helmets sheds a stark light on the question.

An iconic fashion model who has graced the cover of Vogue more than twenty-five times, Lauren Hutton was the original supermodel. She is also an insatiable thrill-seeker who has wrestled alligators and rides motorcycles. In October of 2000, the then fifty-six-year-old Hutton participated in a hundred-mile celebrity motorcycle ride commemorating the opening of the Guggenheim Hermitage Museum and its inaugural exhibition The Art of the Motorcycle. Two and a half hours into it, she pulled over for a break as the heavy wind was making her eyes tear involuntarily. The English actor Jeremy Irons offered her his extra helmet, which had a visor, and insisted that she wear it. Hutton reluctantly put it on. That choice would save her life.11

Three minutes later, Hutton slid into a curve at a speed of more than ninety miles per hour. After skidding a hundred feet, she was launched into the air. A friend recalled the almost cartoonish scene they witnessed as they rounded the same curve: Hutton hanging twenty feet in the air, her legs sprawled out against the crystal clear desert sky. She came down hard on a hill of rocks breaking both her arms and legs, crushing three of her ribs, and puncturing a lung, and skidded another 170 feet facedown on her visor. She was in a coma for two and a half weeks. Though her initial prognosis was poor, less than a year later, she climbed aboard another bike to film a Tropicana commercial.12 “I wouldn’t have prefrontal lobes if I didn’t have a helmet with a visor on,” she later reflected.13

Each year, more than sixty-nine million people suffer from traumatic brain injuries (TBIs), the leading cause of death in motorcycle crashes.14 TBI occurs after a sudden trauma, often a blow or jolt to the head, which causes damage to the brain. They range from a mild concussion, which may cause temporary confusion or headache, to severe ones leading to coma or death.15 Simply by wearing a helmet, Hutton reduced her risk of death by 42 percent and her risk of a TBI by 69 percent.16 But despite the overwhelming evidence that helmets save lives, only twenty US states, the District of Columbia, and Puerto Rico require all motorcyclists to wear them. Another twenty-seven states have helmet laws that apply only to minors, and three states (Colorado, Illinois, and Iowa) have no helmet laws at all.17 The United States is an outlier in this regard. Forty-nine countries have comprehensive helmet laws in place, and most have at least some laws mandating helmet usage.18 A little over half a century ago, forty-seven states, DC, and Puerto Rico all had mandatory motorcycle-helmet laws too.

Why has the United States moved away from mandating motorcycle helmets when the rest of the world has gone in the other direction? According to public health scholars Marian Jones and Ronald Bayer, the answer dates to the 1940s, when the market for motorcycles in the United States was shaped by returning veterans who had learned to ride military-issue Harley-Davidsons while overseas. Few of them wore helmets or even goggles, and only three states had helmet laws. In 1966, as public health rationales started to gain more support as a legitimate countervailing interest to individual liberties, the US Congress passed the National Highway Safety Act, which included a novel provision targeting motorcyclists—funding for future highway safety programs was made contingent on states passing helmet laws. In less than ten years, nearly every state had complied. California was the sole holdout, because it had a strong anti-helmet lobby.19 The police power of states, the lobby successfully argued, did not extend to risks individuals choose to take.

The Supreme Court disagreed; in Simon v. Sargent (1972), it upheld a lower court decision that maintained that helmet laws were indeed in the public interest and not just paternalistic interference with the individual rights of motorcyclists.20 Lower courts had been in near-uniform agreement that the laws were in the public interest. In one of those cases, a plaintiff had invoked John Stuart Mill to argue that the choice to wear a helmet was entirely personal. The court found otherwise, because “society picks the person up off the highway; delivers him to a municipal hospital and municipal doctors; provides him with unemployment compensation if, after recovery, he cannot replace his lost job, and, if the injury causes permanent disability, may assume the responsibility for his and his family’s continued subsistence. We do not understand a state of mind that permits plaintiff to think that only he himself is concerned.”21

Unconvinced, in 1975, motorcycle clubs, associations, and gangs descended on Washington, DC, for a mass protest, and the House Committee on Public Works and Transportation agreed to reconsider its stance. In May 1976, President Gerald Ford signed a bill that eliminated the helmet provision of the NHSA. During the four years that followed, twenty-eight states repealed their mandatory helmet laws, and deaths from motorcycle accidents increased by 20 percent. Some public health experts advocated for reinstatement of the laws, while others took the side of the motorcycle lobbies. Society permits individuals to take on all kinds of risks, they said, including participating in dangerous sports like rock climbing, where helmets are not required. That is the view that still prevails in the United States, thanks to what Jones and Bayer call the primacy of individual self-determination that Americans place over laws that protect “people from self-imposed injuries and avoidable harm”—an attitude that likely informed Americans’ negative responses to mandatory mask and vaccine laws during the COVID-19 pandemic.22

Which side of the debate has it right? Does self-determination include the unfettered right to damage or destroy one’s own brain? Do the costs of social services give society a say in choices that burden those systems? Or are we asking the wrong questions?





The Principle of Self-Determination


Self-determination is best understood as the right to be free from governmental interference when making decisions that only affect oneself. Often described as individual autonomy, the principle underlies many fundamental rights recognized across European, British, and US laws. But it is as misunderstood in the popular mind as it is fiercely protected.

The philosophical roots of self-determination trace back to the ethical theories of Immanuel Kant and John Stuart Mill, who described it as securing to individuals the capacity for free choice. But neither Kant nor Mill ever argued that the right to choose is limitless. Both recognized that it is circumscribed by the liberty interests of others.23

For Kant, who believed that rational agency and the capacity for self-determination set humans apart from other species, respect for individual autonomy was tantamount to being human. Kant and Mill both described the right to individual autonomy as a negative freedom (a freedom from external restraint) and, more broadly, the positive freedom to be in control of one’s own life.24 Mill extended the idea of autonomy to a broad conception of political liberty, as well.25 Individuals, he wrote, are “the proper guardians of their own interests” and governments owe them nothing but the duty to prevent others from interfering with their liberty.26

Medical decisions are an example of purely self-regarding choices. Take my choice to use propranolol when our daughter Callista was hospitalized. Noninterference would mean that I should neither be forced to take the drug nor prevented from doing so, so long as it doesn’t make me unable to perform my duties to others—most important, my duty as a medical decision-maker for our child.

Setting aside special cases like sports and mind games, cognitive enhancement is also a purely self-regarding choice that is fundamental to human flourishing. Government interference with it thus runs afoul of self-determination as a necessary element of cognitive liberty. The societal costs would have to be exceptionally high to justify interference with the choice to self-enhance. But does the same apply to the right to cognitive diminishment?

While international human rights law does not answer the question for us explicitly, a careful reading of the Universal Declaration of Human Rights (UDHR) suggests that an individual right to self-determination is a necessary precondition for all the individual rights it enumerates, including the right to be equal in dignity (Article 1 of the UDHR); to be free from discrimination (Article 7); to privacy (Article 12); freedom of expression (Article 19); and the right to one’s own personality (Article 22, which secures to an “individual economic, social and cultural rights indispensable for his dignity and the free development of his personality”).27 In Jehovah’s Witnesses of Moscow v. Russian Federation, the European Court of Human Rights (ECtHR) recognized the same, opining that “The very essence of the Convention is respect for human dignity and human freedom and the notions of self-determination and personal autonomy are important principles underlying the interpretation of its guarantees.”28

The right to be free from discrimination protects against societal distinctions between the rights protected by the UDHR, which makes problematic societal distinctions between the choice to enhance or to diminish. Freedom against arbitrary interference with privacy has been interpreted by the European Convention on Human Rights to include the right to personal identity and a space to be in control of one’s own faculties,29 just as freedom of expression includes the freedom to develop one’s own voice and identity. The right to privacy in Article 12 of the UDHR shields individuals against interference with their purely personal choices, without regard to whether we socially judge those choices as life-enhancing or diminishing. Just as the ECtHR recognized the individual right to self-determination, so too should international law be updated to explicitly recognize a right to self-determination as underlying the human rights guarantees of the UDHR.

These rights are circumscribed by Article 29, which incorporates a necessity and proportionality test on government restrictions on individual human rights: “In the exercise of his rights and freedoms, everyone shall be subject only to such limitations as are determined by law solely for the purpose of securing due recognition and respect for the rights and freedoms of others and of meeting the just requirements of morality, public order and the general welfare in a democratic society.” Which means that just as Kant and Mill recognized a limit on free choice by individuals, so, too, does international human rights law. Self-regarding choices are those that do not affect the interests of anyone other than oneself.30 But when a “person is led to violate a distinct and assignable obligation to any other person or persons, the case is taken out of the self-regarding class” and can be limited by law and public opinion.31

Each person in society exists relative to others, within a broader community and society of individuals who are also entitled to liberty.32 Kant argued that self-determination also creates an obligation to not only avoid “intentionally withdrawing anything from the happiness of others, but also to try to further the ends of others.”33

Mill believed that our obligations to others could limit the extent to which we could exercise our self-determination, explaining that “the mischief which a person does to himself, may seriously affect, both through their sympathies and their interests, those nearly connected with him, and in a minor degree, society at large. When, by conduct of this sort, a person is led to violate a distinct and assignable obligation to any other person or persons, the case is taken out of the self-regarding class, and becomes amenable to moral disapprobation in the proper sense of the term.”34 If our individual use of neurotechnology leaves us unable to care for a dependent child, for example, we are no longer acting in a self-regarding way, and society could rightly regulate our actions.

Self-determination over one’s brains and mental experiences builds upon these views. It requires that an individual be free from governmental interference unless their choices directly affect the interests of others. This means that purely self-regarding choices over our brains—like tracking our own brain data and ordinary cases of cognitive enhancement or diminishment—should not be subject to societal interference. When our choices do interfere with the interests of others, any government interference with our choices must adhere to the principles of legality, necessity and proportionality.

It may very well turn out that diminishing our brains incurs greater societal costs than enhancing them does—by rendering us less able to fulfill our duties to others, by requiring others to scrape us off the pavement when we’re injured or killed, and to care for us if we’re unable to care for ourselves. And that is the right question to answer when we ask whether we have a right to diminish our own brains—whether the costs to society of our doing so are sufficiently high to justify societal intervention. The same argument is used to justify laws against drug abuse.





Breaking the Brain


The not-infrequent throbbing in my head began when I was ten or perhaps even younger. My well-intentioned parents attributed it to inflamed sinuses and treated me with eucalyptus steam baths and hugs. It wasn’t until after college—when, by a stroke of good luck, my work at a strategy consulting firm brought me into contact with an insightful primary care physician whose office was in the same building—that I learned the truth. When I told him about my throbbing “sinus pain,” he kindly explained that “those are migraine headaches.” And the migraine medication he prescribed completely changed my life for the better. Along with it, he prescribed an opiate for acute migraine pain. When I handed my prescriptions to a pharmacist later that same day, I remember being perplexed by the suspicious look she gave me. It gave me the distinct feeling that I was doing something wrong.

Since then, the world has been turned upside down by opioid addiction. Starting in the early 2000s, overdoses and deaths from prescription opioids began to spike. This was partly attributable to OxyContin—a controlled-release version of the potent opioid drug oxycodone that I had been prescribed. Recreational use of OxyContin grew from four hundred thousand doses in 1999 to over 2.8 million by 2003.35 By October 2017, the secretary of Health and Human Services had declared a public health emergency.36

Combatting the opioid epidemic has proved exceptionally difficult. More than five hundred thousand deaths a year arise from drug misuse, and more than 70 percent of those are related to opioids.37 Over $740 billion is lost annually in reduced workplace productivity, health-care expenses, and crime-related costs.38 Despite a multidecade, multibillion-dollar effort to combat the crisis, prescription painkillers remain easy to find, easy to use, and relatively cheap to buy on the black market. New drugs have been developed to help addicts kick their habits, and more addiction treatment centers have opened worldwide.39 But the battle against opioid addiction is at a stalemate at best. More policing, arrests, and jail time are unlikely to reverse these tragic trends,40 as these drugs take hold of the brain and don’t let it go. The dopamine rush that they cause is so pleasurable that people will go to any length to experience and reexperience it.

Does the epidemic justify banning the sale of opioid drugs? Does the high risk of addiction justify limiting access to them, even for people who suffer from chronic pain? In 2016, the CDC issued guidelines that encouraged doctors to prescribe them less—which many doctors and insurance companies interpreted as guidance to avoid their use altogether. But not all patients who are prescribed opioids for chronic pain misuse them. In fact, only 8 to 12 percent of them become addicted. The result has been the systemic undertreatment of pain, with consequences that are every bit as devastating as addiction.41 While I have been among the fortunate few whose chronic migraine pain is adequately treated, I am regularly made to jump through hoops to fill my prescriptions.

We may all agree that governments ought to try to mitigate the high societal costs of opioid addiction. But does doing so unduly interfere with the self-determination of individuals who have a self-regarding interest in controlling their own pain, brains, and mental experiences? How do we decide when the external costs to others sufficiently outweigh their own interests? How do we decide if, when, and how to intervene?

Almost every known addictive substance causes dopamine to be released in the nucleus accumbens in the brain.42 Burning away a small section of this so-called pleasure center can stop the craving that defines addiction.43 In 2003, Chinese scientists reported that they had successfully resectioned the nucleus accumbens of eleven patients. Within a year, another thousand underwent the surgery.44 But international controversy followed as devastating side effects surfaced, including profound personality changes and memory loss. The Chinese Ministry of Health quickly responded by putting a moratorium on the procedure.45

Fortunately, the neurotechnological development known as deep brain stimulation (DBS) offers a more targeted intervention for willing participants. Gerod Buckhalter was one of the first to undergo this procedure. Once described as “Mr. Everything” by his local newspaper—he’d been a high school basketball and football star—Buckhalter fell far from grace at age fifteen, when he suffered a shoulder injury and was prescribed a six-week regimen of opioids to treat the pain. That became his gateway to heroin and other drugs. When offered the opportunity to undergo experimental DBS surgery at age thirty, he jumped at the chance.46 A neurosurgeon drilled a hole in his skull and inserted an electrical probe in his brain. Then he was shown images that induced his craving for drugs while the probe delivered electrical signals that effectively reprogrammed the neural circuits underlying his reward pathways.47

Compared to ablative brain surgery, DBS holds greater appeal for the individual and society because it has fewer side effects, greater precision, and is reversible.48 Other techniques using neurotechnology involve the use of transcranial magnetic stimulation from a device held outside the head that targets ultrasound in high- or low-frequency waves to reach structures deep inside the brain.49

The nucleus accumbens is just one of the parts of the brain that these techniques target, and addiction is just one of many behaviors that they treat. Much as the nucleus accumbens triggers pleasure in response to opioids, the amygdala triggers fear.50 An amygdalotomy—the surgical removal of the amygdala—could, in theory, prevent an individual from having panic attacks. But ablating the amygdala can also cause profound personality changes, blunting emotions to such an extent that the person becomes a psychopath. Nearly all serial killers show psychopathic tendencies, such as a lack of remorse and apparent disregard for the loss of human life.51 Some of their behavior can be explained by the lack of a fear response in their brains.52

Does the risk of side effects to the individual and to society outweigh the potential benefits of these interventions? Do the external costs of the behaviors they are meant to change justify those risks? “External costs” means that a person doesn’t bear all the costs of their choices. Take cigarette smoking. An individual smoker may believe that cigarette smoking is purely self-regarding behavior. But studies estimate its external costs at $323 billion every year. That said, less than $5 billion is imposed outside smokers’ own families, or less than 2 percent.53 Do any external costs justify limiting or banning smoking?

Estimates of the external costs of risky behavior depend on the social context in which an individual makes their choices, including the availability of health insurance and health-care structures, medical practices and technologies, and scientific knowledge.54 Alongside those empirical factors are many subjective ones that our biases can lead us to over- or understate.55 The Reverend Floyd W. Tomkins’s assessment of the “menace to the peace and safety of a community” that alcohol poses might not be the same as our own.

Certain choices may be more likely to have external costs than others. The choice to diminish our brains with medications that block pain receptors while releasing a rush of dopamine has high external costs because it renders any given individual more likely to become addicted, limiting the scope of their future autonomous choices. My choice to erase my memory of an event I have witnessed might also have more easily measurable societal costs than my choice to enhance my memory.

But Mill cautioned us against adding too many remote externalities into our calculations. While we are all interrelated, is it fair to characterize all these costs as direct costs that limit the liberty interests of any others? The mere fact that a negative externality exists does not always justify interfering with people’s freedom of choice. Moreover, governmental interference with individual choices is unlikely to affect all people equally. A simple cost–benefit analysis can elide issues of equity.56 The racial disparities in drug arrests have been well documented. Restrictive regulations on pain medications, for example, are more likely to harm poor people than rich, who can shop for more sympathetic doctors.

What all of this means is that when an individual chooses to brake their brain for a self-regarding reason—such as to break an addiction—we ought to tread lightly before we intervene. And we ought to clearly articulate ex ante what societal harms we will consider when making the decision about when to do so.





The Right to Self-Determination over Our Brains and Mental Experiences


Some brain hacking impose costs on others that, depending upon their nature and degree, may justify some interventions. When we drink alcohol, we have slower reaction times, so society prohibits us from driving a motor vehicle. But most Western societies do not outright prohibit individuals from drinking. This may be because the impacts of drinking on most individuals—and society—are temporary and transitory, rather than permanent and unavoidable. We may weigh other interventions that have more permanent effects differently. But as for the broader question, whether individuals have the right to enhance or diminish their own brains, the answer is that they do—so long as the externalities of their decision does not unduly interfere with the liberty interests of others.

Like other rights in the bundle of sticks that make up cognitive liberty, the right to self-determination over our brains and mental experiences requires us to explicitly update our understanding of international human rights law. Self-determination is not an absolute right—rather, it is a liberty interest that is limited only by the liberty interests of others. This understanding undergirds our right to be equal in dignity, free from discrimination, to have our privacy respected, and be able to express ourselves freely.

As Mill argued, “With regard to the merely contingent, or, as it may be called, constructive injury which a person causes to society, by conduct which neither violates any specific duty to the public, nor occasions perceptible hurt to any assignable individuals except himself; the inconvenience is one which society can afford to bear, for the sake of the greater good of human freedom.”57 Put simply, contingent injuries don’t rise to satisfy the necessity test for government interference. When society interferes with purely personal conduct, it invariably does so wrongly and in the wrong place. We can and should avoid acting as the moral police by focusing our inquiries strictly on whether our choices harm other people. If they do not, we ought not interfere with them, as they are purely self-regarding ones.58

This does not leave society powerless to influence our choices over our brains and mental experiences. We should provide people with information about the risks and benefits of certain brain interventions, offer incentives for choices that align with the common good, and even consider the necessity of restricting certain behaviors—perhaps driving a motorcycle without a helmet—to limit the financial burden that an individual’s choices can impose on society.

But here, too, we should tread lightly. John Stuart Mill cautioned that while the state has an interest in discouraging injurious conduct like drunkenness, to “tax stimulants for the sole purpose of making them more difficult to be obtained, is a measure differing only in degree from their entire prohibition; and would be justifiable only if that were justifiable … [citizens’] choice[s] of pleasures, and their mode of expending their income, after satisfying their legal and moral obligations to the State and to the individuals, are their own concern, and must rest with their own judgement.”59

There is an inextricable link between self-determination and individuality. “It is not by wearing down into uniformity all that is individual in themselves, but by cultivating it and calling it forth, within the limits imposed by the rights and interests of others, that human beings become a noble and beautiful object of contemplation,” wrote Mill.60 We should not discourage individual choices simply because we wish for greater uniformity, or have particular views about how individuals ought to exercise their right to cognitive liberty. Respecting people’s right to self-determination—including their right to enhance or diminish their brains—will further enable human flourishing. But that flourishing will be threatened if we don’t better define what others can be allowed to do to our brains.