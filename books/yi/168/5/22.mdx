---
---
---
title: 22 心灵、脑与程序
---




约翰·塞尔

（1980）



近期，人们在计算机模拟人类认知能力方面付出了努力，我们该如何看待其心理学及哲学意义？要回答这个问题，我发现如下区分很有用处：我称一些情况为“强”AI，区别于“弱”AI或说“小心谨慎的”AI。根据弱AI，在心灵研究领域，计算机最重要的价值是为我们提供了强有力的工具。例如，它使我们能以更严格更精确的方式形成并检验假设。但根据强AI，计算机就不单单是心灵研究的一种工具了；毋宁说，因为带有正确程序的计算机完全称得上是在进行理解，也有其他一些认知状态，这种意义上，适当编程的计算机就是一个心灵。在强AI中，由于被编程的计算机具有认知状态，那么程序便不再只是检验心理解释的工具，它们本身也就成了解释。

我不反对弱AI的主张，至少就本文而言。我这里的讨论将直指我定义的那种强AI的主张，特别是这种论调：适当编程的计算机确实具有认知状态，于是这些程序能解释人类的认知。此后再提及AI时，我所说的都是这两条论断表述的强版本。

我将考虑罗杰·尚克和他耶鲁同事们的工作（Schank ＆ Abelson，1977），因为和其他类似主张相比，我更熟悉这个；也因为它为我想考察的那类工作提供了一个非常清晰的例子。不过下文并不局限于尚克程序的细节，同样的论证均亦适用于维诺格拉德的SHRDLU（Winograd，1973）、魏岑鲍姆的ELIZA（Weizenbaum，1965）以及任何对人类心理现象的图灵机模拟（见《延伸阅读》中塞尔的参考文献）。

抛开各类细节，简言之，可以这样描述尚克程序：这个程序的目标是要模拟人类理解故事的能力。人类理解故事的能力有这样的特点，即人类能够回答有关故事的问题，即便相关信息从未在故事里明说。例如，假设你听到了下面这个故事：“一个人走进一家餐厅点了一个汉堡。汉堡上来的时候已经烤焦了，于是这个人生气地冲出餐厅，没有付汉堡钱也没有留小费。”现在，如果问你：“这个人吃没吃那个汉堡？”你大概会说：“他没吃。”同样地，如果你听到的是下面这个故事：“一个人走进一家餐厅点了一个汉堡。汉堡上来的时候他非常满意。他离开餐厅付账前，还给了女侍者一大笔小费。”那这么问你：“这个人有没有吃汉堡？”你可能就会说：“他吃了。”据说，尚克的机器就能以这种方式在有关餐厅的问题上给出类似的回答。要做到这一点，它们要对人类关于餐厅所具有的那类信息有一种“表征”，从而给定上述故事，它们就能做出上述回答。给机器讲故事并提问时，机器打印出来的答案会正是我们预想给人类讲类似的故事时，人类会给出的。强AI的死忠信徒声称，机器在这一系列的问答中绝非只是模拟了人类的能力，而且：（1）机器完全称得上理解了故事并回答了问题，并且（2）机器及其程序的所作所为也确实解释了人类理解故事及回答问题的能力。

在我看来，尚克的工作完全不支持这两条论断，下面我尝试揭示这一点。当然，我不是说尚克本人站在了这些论断的一边。

检验任何关于心灵的理论，一种方式就是去自问，如果我的心灵真的按照理论所说的那些普适于一切心灵的原则去运作的话，会是怎样的。就让我们用下面这个思想实验来检验一下尚克程序吧。试想我被锁在一个房间里，有人给了我一大堆中文文稿。再设想我对汉语，无论是书面语还是口语，都一窍不通（也确实如此），我甚至难保能把中文与比如日文或者毫无意义的曲里拐弯儿区分开来。对我来说，汉字就是一大堆毫无意义的曲里拐弯儿。现在进一步设想，在这第一批中文文稿之后，有人又给了我第二批中文手稿，与之一起的还有一套将第二批与第一批关联起来的规则。规则是英文的，因而我就像任何一个英语母语者一样理解它们。这些规则能让我把一组形式符号与另一组形式符号关联起来，而“形式”在这里意味着，我单凭这些符号的形状就完全能够辨别它们。再设想又有人给了我第三批中文字符，也随带着一些英文的指令，能让我把第三批的元素与前两批关联起来，还指示我如何返回特定形状的中文字符以回应第三批中的特定形状。给我所有这些字符的人称第一批为“脚本”，第二批为“故事”，第三批为“问题”（我不知道这些）。另外，他们把我回应第三批的返回字符叫作“问题的答案”，他们给我的英文规则叫作“程序”。现在让故事稍微复杂一点，设想这些人又给了我一些我能看懂的英文故事，然后就这些故事用英语问我问题，我也用英语回答他们。再设想，过了一会儿，我已能娴熟地依照指令操作这些中文字符，程序员也已经能娴熟地编写程序，结果是，从外部视角，即从锁着我的房间外的人的视角来看，我的回答与汉语母语者毫无二致。单从我的答案，谁也看不出我不懂一点汉语。让我们再假设，我对英语问题的回答与其他的英语母语者也是别无二致——当然了，这毫无疑问，很简单，因为我自己就是个英语母语者。从外部视角，即从阅读我的“回答”的人的视角来看，我对中文问题的回答和对英文问题的回答一样好。但中文的情况有所不同，我是通过不加理解地操作形式符号炮制出了答案。对中国人而言，我的行为简直就像一台计算机，是对形式上规定好的元素执行计算操作。从中国人的意图来看，我不过是充当了一个计算机程序的实例。

至此，强AI所做的论断是，被编程的计算机理解这些故事，并且这个程序某种意义上解释了人类的理解。不过现在，我们可以根据我们的思想实验来检验一下这些论断。

1.第一个论断对我而言，从我对中文故事一个字也不懂的情况来看已经非常明显了。我的输入输出与汉语母语者别无二致，我也可以具有任何你喜欢的形式程序，但我仍然什么也不理解。出于同样的理由，尚克的计算机也不理解任何故事，无论是中文的、英文的还是其他什么的。因为在上述中文情境中计算机就是我，那么在计算机不是我的情境中，既然我什么也不理解，计算机也不比我懂得更多。

2.第二个论断说程序能解释人类的理解，而我们可以看到计算机及其程序并未给出理解的充分条件，因为计算机和程序只是在运转，这之中并没有理解。不过它是否为理解提供了一个必要条件或者重要贡献呢？强AI的支持者给出的一个论断是，当我理解一个英文故事时，我的所作所为与我对中文字符的操作完全相同，或许大致算得上相同；在英文的情况中我有所理解，在中文的情况中我没有，而两种情况的区别只不过是，英文情况中要操作的形式符号更多。我尚未充分表明这个论断是错的，但在这里的例子中它着实显得不可信。这种论断若有可信性，则来自这样的假定，即我们能构造出某种程序让它与母语人士具有相同的输入和输出；此外我们还得假定在某种层面上也可以把说话者描述为一个程序实例。基于这两个假定，我们设想，即便尚克程序不是关乎理解的全部真相，也可能是关乎其中的一部分。虽说，我可以认为这在经验上是可能的，但迄今还没找到任何理由相信这是真的，因为我们的例子虽不能充分表明，但也暗示了，计算机程序与我对故事的理解毫不相干。在中文的情况中，我具有的一切都可以被人工智能以程序的方式赋予，同时我什么也不理解；而在英文的情况中，我理解一切，并且至今全无理由去假设我的理解与计算机程序，即对纯形式上规定的元素进行的计算操作，有什么关系。只要程序是以“对纯形式上定义的元素进行的计算操作”来定义的，这个例子就能暗示，这些计算操作本身与理解没有任何值得关注的联系。它们无疑不是充分条件，也没有理由被设想为必要条件，或是对理解做出了什么重要贡献。请注意，这个论证的效力并不在于不同的机器有相同的输入输出时，运行所依据的形式原则不尽相同，这完全不是重点所在。毋宁说，无论你把什么纯形式的原则赋予计算机，它们对理解而言都不充分，因为一个人类可以毫无理解地遵循这些形式原则。没有理由设想这种原则是必要的甚至有贡献的，因为毫无理由设想我在理解英语时是在操作什么形式程序。

那么，我在英文语句的情况中，有什么是我在中文语句的情况中没有的？显而易见的答案是，我知道前者的意思，而对后者的意思一无所知。但这又是由于什么？以及，无论由于什么，我们为什么不能把这个因素赋予一台机器？我到后面会再回到这个问题上来，不过我想先接着这个例子往下讲。

我曾在一些场合把这个例子展示给一些位人工智能工作者，有趣的是，他们似乎对怎么才算恰当回应了这个例子莫衷一是。我得到的回应多得惊人，接下来我将考虑其中最常见的几种（标以来源地点）。

不过首先我想屏蔽几种对“理解”的常见误解：这些讨论中相当一些是在“理解”一词上大做文章。我的批评者指出，有许多种不同程度的“理解”；它不是一个简单的二元谓词；“理解”还有不同的种类，不同的层面；甚至连排中律也往往不能直接用于“甲理解乙”这种形式的陈述，许多情况下，甲是否理解了乙不是一个简单的事实，而是需要判定的，等等。对所有这些观点，我想说：没错，当然如此。但它们和我们这里的问题毫无关系。总有一些非常清晰的情况，哪些是“理解”完全适用的，哪些又完全不适用；这里的论证只需要这两类情况。[1]我理解英文故事，在稍低的程度上也理解法文故事，在更低的程度上理解德文故事，中文的则完全不理解。另一方面，我的汽车，我的加法计算器，则什么也不理解，对“理解”一事全不在行。我们时常通过比喻、类比将“理解”及其他认知谓词用在汽车、加法器及其他人造物品之上，但这种用法什么也证明不了。我们说，“门知道什么时候要开，因为它有光电管”，“加法器知道如何（理解如何、能够）做加减法，而非除法”，还有“恒温器会感知温度的变化”。我们这样用词，理由十分有趣，它关乎的事实是，我们会在人造物品中延伸自己的意向性[2]：我们的工具是我们目的的延伸，因而我们会很自然地把意向性的比喻用法施加给它们。但这些例子在哲学上全无用处。自动门从它的光电管中“理解指令”的含义，完全不是我理解英语的那种含义。如果说尚克的编程计算机理解故事的含义是门的“理解”的那种比喻义，而不是我理解英语的那种含义，那这个问题就不值得讨论。不过纽厄尔和西蒙（Newell ＆Simon，1963）写道，他们为计算机主张的那种认知，与为人类主张的完全相同。我喜欢这一论断的直率，这也是我将会考虑的那类论断。我将论证，在严格的意义上，被编程的计算机所理解的也正是汽车和加法器所理解的，亦即，根本什么也不理解。计算机的理解才不是部分的、不完整的（就像我对德语的理解），而是零。





现在是那些回应：

1.系统回应（伯克利）。“虽然被锁在房间里的个体的人确实不理解故事，但事实上他只不过是整个系统的一部分，而系统确实是理解故事的。这个人面前有所有规则的明细，他有大量的草稿纸和铅笔来做演算，还有若干组中文字符的‘数据库’。这样，理解并不是归给单单这个个体，而是归给包含他在内的整个系统。”

我对这种系统理论的回应十分简单：让这个个体把系统的所有元素都内化。他记住了规则明细，也记住了中文字符的数据库，并在头脑中完成所有的计算。这样，个体就吸纳了整个系统，系统不再有任何内容未包含在他之内。我们甚至可以去掉房间而假设他在室外工作。但还是一样，他一点也不理解中文，更不用说系统了，因为系统具有的内容无一不是他也有的。如果他不理解，那么系统就更不可能理解，因为系统就是他的一部分。

其实我给了系统理论这样的回答，自己甚至都有点尴尬，因为在我看来，这个理论从一开始就极不合理。它的想法是，一个人不理解中文，而这个人和一堆纸张的合取竟然能理解中文。我很难想象一个人若非陷入了某种思想观念，怎么会觉得这个想法合理。但我觉得忠于强AI思想观念的人，最终会倾向于说出极为类似的话。因此让我们进一步来探讨一下。根据这种观点的一个版本，系统内化案例中的人并不像汉语母语者那样理解中文（比如因为他并不知道故事涉及餐厅、汉堡等等），但“此人作为一个形式符号操作系统”确实理解中文。这个人的中文形式符号操作子系统，不应与他的英文子系统相混淆。

这个人身上确实有两个子系统，一个理解英文，另一个理解中文，而且这两个系统“并行不悖”。不过我要回复，它们不仅是并行不悖，而且还毫不相似。理解英文的子系统（假设我们暂且允许自己使用“子系统”这样的术语来谈论）知道故事是关于餐厅和吃汉堡的，也知道自己正被问及关于餐厅的问题，并且自己正根据故事的内容进行推理而尽量回答这些问题等等。但中文系统对此却一无所知。相较于英文子系统知道“汉堡”指称汉堡，中文子系统只知道“一堆曲里拐弯儿”后面跟着“另一堆曲里拐弯儿”。这人所知道的只是从一端传入各种形式符号，再根据英文写就的规则一操作，就会从另一端出来另一些符号。最初例子的全部意义就在于说明，这种符号操作本身在任何真正的意义上对于理解中文来说都不可能是充分的，因为这人可以“曲里接拐弯儿”地写下去，但不理解任何中文。而且设定在人内部的各子系统也无法回应那个论证，因为子系统并不比开始的那人好到哪里去，它们所具有的，与一个讲英语的人（或子系统）所具有的，之间仍然全无相似之处。其实，在所述情况中，中文子系统只是英文子系统的一部分，是一个根据英文规则进行无意义符号操作的部分。

首先，让我们问问自己，会是什么驱使系统做出应答，也就是说，要有什么独立的依据，我们才能说一个行动主体内部一定具有一个确实理解中文故事的子系统？就我目前的全部所知，唯一的依据只是，在例子中，我和汉语母语者有一样的输入输出，还有一个从输入到输出的程序。但这些例子的全部要点只在于竭力表明，在我理解英文故事的意义上，对这样的理解而言，这依据不可能是充分的，因为一个人，以及组成这个人的诸系统的集合，可以具有输入、输出及程序的正确组合，却在我理解英文这个相对严格的意义上，还是什么都不理解。说在我身上一定有一个理解中文的子系统，唯一动机就是我有一个程序，并且我能通过图灵测试，能糊弄过汉语母语者。然而争论的要点之一正在于图灵测试的适当性。例子表明，可以有两个“系统”，两者都能通过图灵测试，但只有一个能理解；说“既然两者都能通过图灵测试，那它们一定都能理解”构不成对这一论点的反驳，因为这一论断没能回应“我理解英文的系统大大超出我单纯处理中文的系统”这个论点。简言之，系统回应只是在“乞题”，不加论证地坚持认定系统必定理解中文。

不只如此，系统回应似乎还会导致其他意义上的荒谬结论。如果单凭我具有特定种类的输入输出和一个居间的程序，就得出我一定有认知能力的结论，那好像所有非认知的子系统都会变得有认知了。例如，在某个层面上，我的胃可以被描述为是在进行信息处理，它也可以是各种计算机程序的实例，但我认为我们不会想说胃有什么理解（见Pylyshyn，1980）。但如果接受了系统回应，那么既然说一个子系统理解中文，和说一个胃有理解，二者的动机原则上无从区分，也就很难看到如何才能避免说胃、心、肝等都是有理解的子系统。顺便一说，说中文系统以信息为输入输出，而胃以食物及其产物为输入输出，也没有回答到点子上，因为从行动主体的视角，即我的视角看来，无论是食物还是中文都不包含信息：中文只是一大堆毫无意义的曲里拐弯儿而已。在中文的情况中，信息只在程序员、翻译员的眼中存在，如果他们想把我消化器官的输入输出当作信息，也没有什么障碍。

最后这点与几个独立的强AI问题有关，值得暂时岔开正题解释一下。如果强AI要成为心理学的一个分支，那么它必须能区分哪些是真正的心理系统，哪些不是。它还必须能区分心理和非心理系统的工作原理，不然就不能解释心理有什么特别“心理”之处。心理／非心理的区分不能只存在于旁观者的眼中，而要内在于系统，否则决定权就在任意的旁观者，只要他愿意，就可以把人视作非心理的，而把例如飓风视作心理的。但在AI文献中，这个区分常常以各种方式被抹消了，长此以往，会给“AI是一项认知探索”的宣言带来灾难性的后果。比如麦卡锡就写道：“像恒温器那样简单的机器也可以说是有信念的，并且拥有信念似乎是多数能够解决问题的机器的特征。”（McCarthy，1979）任何认为强AI有望成为一个心灵理论的人都应当深思这个评论的隐含之意。它要求我们把如下情况当作强AI的新发现接受下来：墙上那个我们用来调节温度的大金属块也拥有信念，且与我们、我们的配偶和孩子拥有的是相同意义的信念；此外，房间里的“多数”机器——电话、录音机、加法器、电灯开关等等——也是在这个严格的意义上拥有信念。反驳麦卡锡的观点不是这篇文章的目的，因而我只提出如下主张而不加论证。心灵探究的起步事实是：人类拥有信念，而恒温器、电话、加法器没有。如果你遇到了个理论否定这一点，那你已经对这一理论提出了一个反例，所以这个理论是错的。有人有这种印象，AI界写出这些东西的人之所以认为自己能够蒙混过关，是因为他们从未严肃对待它们，也不指望其他人会。我提议，至少严肃对待它们一小会儿。花一分钟努力想想，需要什么才能确证墙上那个大金属块拥有真正的信念，即那些具有匹配方向、命题内容和满足条件的信念，那些或强或弱的信念，那些紧张、焦虑或是安宁的信念，那些武断、理性或是迷信的信念，那些盲目的信任或者犹疑的酌定……任何一种信念。恒温器不在候选之列，胃、肝、加法器、电话也都不在。但既然我们在严肃对待这个想法，就要注意到，其正确性对于强AI要成为一门心灵科学的主张来说至关重要。而现在到处都是心灵。我们想知道的是，是什么把心灵与恒温器、肝脏区别了开来。而假如麦卡锡是正确的，那强AI就无望告诉我们这一点。

2.机器人回应（耶鲁）。“设想我们写了一个不同于尚克的程序。设想我们将一个计算机置入一个机器人，这个计算机不但能接收形式符号作为输入，发送形式符号作为输出，还能切实地操纵机器人，使机器人做事非常类似于感知、行走、来回移动、钉钉子、吃喝——你喜欢的任何事情。然后比如说这个机器人会身附一个电视摄像机让它能‘看’，还有胳膊和腿让它能‘动’，而全部这些都由其计算机‘脑’控制。不同于尚克的计算机，这样一个机器人具有真正的理解及其他心理状态。”

对于机器人回应，首先要注意到，它默认了认知并不仅仅事关形式符号的操作，因为这个回应加上了一套与外部世界在物质成因（cause）方面的联系（见Fodor，1980）。但对机器人回应的回应是，增加诸种“感知”“运动”能力，无论是在“理解”这个特殊意义上，还是“意向性”这个一般意义上，都没有为原本的尚克程序增加任何东西。为了看出这一点，请注意同样的思想实验也适用于机器人的情形。设想不是把计算机放入机器人，而是把我放入房间，就像最初的中文情况一样，你给我更多的中文字符和更多的英文指令，好让我匹配中文字符并将它们向外部反馈。试想，有些中文字符是机器人身上的电视摄像机发送给我的，我发出的另一些中文字符则用来让机器人内部的马达驱动它的腿和手臂，而这一切我都不知情。需要强调的是，我所做的一切都只是形式符号的操作，我对上述这些情况都一无所知：我从机器人的“感知”装置接收“信息”，并向它的马达装置发出指令，而对这两方面事实都浑然不知。我是这个机器人内里的“小人儿”，但与传统“小人儿”不同的是，我不知道正在发生什么。除了符号操作的规则之外，我什么也不理解。对这个案例我想说，机器人根本没有意向状态，它四处移动只是其电路和程序的结果。此外，我在充当一个程序实例的过程中也没有相关类型的意向状态。我的所有作为不过是在遵循操作形式符号的形式指令。

3.脑模拟器回应（伯克利和MIT）。“假如我们设计一个程序，它不表征我们关于世界所拥有的信息，例如尚克脚本中的那类信息，但是它模拟汉语母语者理解中文故事并作答时，脑中的突触上真实的神经元发放序列。这台机器接收中文故事和问题作为输入，并且模拟实际懂中文的脑在处理这些故事时的形式结构，然后输出中文的应答。我们甚至可以想象这台机器并非运行单一的串行程序，而是一整套程序并行，即大体以真实的人脑处理自然语言的那种方式运转。那么在这样的情况下，我们当然就得说机器是理解故事的了；我们如果拒绝承认这一点，不也就否认了汉语母语者是理解故事的吗？在突触的层次上，计算机的程序与中文脑的程序会有甚或能有什么区别吗？”

在反驳这个回应之前，我想先扯句题外话：任何人工智能（或功能主义等）的支持者做出这种回应，都是有多奇怪啊！我认为强AI的整体理念就是，我们不需要了解人脑如何工作就能知道心灵如何工作。这里（我曾认为）的基本假设，是有一个心理活动的层次，包含对形式元素的计算处理，而这构成了心理的本质，并且可以由脑的各种不同过程实现，就像任何计算机程序都能由不同的计算机硬件来实现那样。因为根据强AI的假设，心灵之于脑，正如程序之于硬件，因此我们无需神经生理学就能理解心灵。我们如果必须知道脑如何工作才能做AI，那也就不必在AI上费心了。然而，即便使其接近于脑的运作，仍不足以产生理解。要明白这一点，请想象房间里那个只懂一门语言的人不是在摆弄符号，而是在操纵一组通过阀门相连的复杂水管。当这个人收到中文字符时，他查阅用英文写就的程序来看他需要开关哪些阀门。每根水管对应着中文脑中的一个突触，整个系统就此装配而成，等所有正确的神经元发放都发生后，也就是等所有正确的水龙头都打开后，中文的回答也就在这组管道的输出端喷涌而出。

那么，这个系统的理解又发生在何处呢？它以中文作为输入，模拟中文脑中突触的形式结构，并给出中文的输出。但这个人无疑不懂中文，水管也不懂；而如果我们忍不住去接受那个我视为荒谬的观点，即这个人和水管的合取竟然能够理解，那么记得吗，原则上这个人可以将水管的形式结构内化，并在想象中完成所有的“神经元发放”。脑模拟器的问题在于它模拟了脑的错误方面。只要它模拟的仅仅是突触上神经元活动序列的形式结构，它就没有模拟脑的关键之处，即脑的物质成因特性，脑产生意向状态的能力。水管的例子表明，形式特性对成因特性而言是不充分的，我们可以将所有的形式特性都从相关的神经生物性的成因特性中剥离出去。

4.组合回应（伯克利和斯坦福）。“尽管前三个回应单独看来都不太能有力驳斥中文房间这一反例，但如果把它们组合到一起，就会有说服力得多，甚至起决定性的作用。想象一个机器人，颅腔内嵌一个脑形的计算机，这台计算机用人脑的突触来编程，而这个机器人的所有行为与人类的行为无从分辨。这时，我们就要将其视作一个统一的系统，而不仅仅是一个有输入和输出的计算机。显然在这样的情况下，我们势必要把意向性归给这个系统。”

我完全同意，这种情况下，如果我们对这个机器人的了解仅限于此的话，那么接受它具有意向性确实是理性的，甚至难以抗拒。实际上，除了外表和行为，这个组合中的其他元素真的无关紧要。假如我们能够造出一个机器人，其行为在很大范围内与人类无从分辨，我们就会把意向性归给它——想不出有什么理由不这么做。我们无须事先知道它的计算机脑在形式方面模拟了人脑。

但我看不出这对强AI的各种主张有什么帮助，原因是：根据强AI，建立起一个有着正确输入输的形式程序实例，就是意向性的充分条件，甚至就构成了意向性。正如纽厄尔（1979）指出的，心理的本质是物理符号系统的操作。但在这个例子中，我们把意向性归给机器人，与形式程序毫无瓜葛。它们只是基于这样的假设：如果机器人的外表及行为与我们充分相似，那么直到有其他证明之前我们都会设想它一定具有和我们一样的心理状态来引发行为并被行为表现出来，而且机器人还必定具有能产生这些心理状态的内部机制。如果我们能不借助这些假设而独立地解释机器人的行为，尤其如果我们知道它有一个形式系统，那我们就不会把意向性归给它。这也正是我之前答复第二个反对时的要点。

假设我们知道，这个机器人的行为完全取决于这一事实：它内部有人从它的感觉接收器接收未经解释的形式符号，再向它的运动装置发送未经解释的形式符号，且这个人依照一套规则进行符号操作。此外，设想这个人对机器人的这些事实一无所知，他知道的只有对何种无意义的符号要进行何种操作。这种情况下，我们会把机器人视作一个精巧的机器傀儡。这时再假设傀儡具有心灵，就既无根据也无必要了，因为现在再也没有任何理由把意向性归给机器人或它所在的某个系统（当然，除了那个人操作符号时的意向性）。对形式符号的操作在继续，输入和输出正确匹配，但意向性的唯一真正所在是那个人，而他并不知道任何相关的意向状态，比如说，他看不到机器人的眼中看到什么，并非有意移动机器人的手臂，也不理解机器人给出或收到的言论。根据前述原因，这个包括人和机器人在内的系统也做不到这些。

为了看清这一点，请把上述情况与下述情况对比：我们把意向性归给猿、猴等特定的其他灵长类，也归给狗这样的驯养动物，觉得这很自然。我们觉得自然，大致因为两点：如果不把意向性归给动物，我们就无法理解它们的行为；以及我们能看到，组成这些动物的质料与我们自己的类似——那是眼睛，那是鼻子，这是皮肤等等。鉴于动物行为有连贯一致性，也鉴于我们假定动物的基础成因性质料与人类相同，我们就能设想，动物的行为背后一定也有心理状态，并且产生这些心理状态的机制也由类似于我们拥有的质料构成。只要没有反对理由，我们当然也能对机器人做出类似的设想，但只要我们知道这些行为是形式程序的结果，而与物质实际成因方面的特性无关，我们就要放弃意向性的设想。

针对我的例子，这里还有另外两种回应很是常见（因而也值得探讨），但它们实在也是不得要领。

5.他心回应（耶鲁）。“你是怎么知道别人理解中文或其他任何东西？只能通过他们的行为。既然计算机（原则上）也能像别的人类一样通过行为测试，所以如果你把认知归给其他的人，原则上你也必须把它归给计算机。”

这个反驳确实只值一个简短回应。这场讨论中的问题并不在于我是如何知道他人具有认知状态的，而是当我把认知状态归给他们时，我归给他们的究竟是什么东西。论证的要旨在于，认知不可能只是计算过程及其输出，因为计算过程及输出没有认知状态就能存在。假装麻木可回应不了这个论证。“认知科学”的一个前提即预设了心理的实在性和可知性，就像物理科学也必须预设物理对象的实在性和可知性一样。

6.左右逢源（Many Mansions）回应（伯克利）。“你的整个论证预设了AI仅仅关乎模拟和数字计算机。但这碰巧只是科技的现状而已。无论你认为对于意向性来说那些至关重要的成因过程是什么（假设你是对的），我们最终都能制造出具有这些成因过程的设备，而那就是人工智能。因此你的论证并不针对人工智能产生并解释认知的能力。”

这个回应我真是无可反驳，除了说，其实它通过“任何人工产生并解释认知的事物”重新定义了强AI，从而使其变得无足轻重。原先为人工智能所做的那个论断之所以有意思，就在于它是一个精确、清晰的论题：心理过程就是对形式上定义的元素施加计算过程。我一直致力于挑战的是这一论题。如果这一论断被重新定义，不再是同一个论题，我的反驳也就不再适用，因为它要去针对的那个可检验假设已不复存在。





现在让我们回到那个我承诺过会尽力解答的问题上来：在我最初的例子中，我理解英文而不理解中文，而所述机器既不理解英文也不理解中文，那么据此，我这里一定是有点什么，让我理解英文的这种情况成立，而缺了这方面，就让情况变成了我理解不了中文。那么无论这些“什么”是什么，为什么我们不能把它们赋予一台机器呢？

原则上我看不出有任何理由，让我们不能赋予机器理解英文或中文的能力。因为在一个重要的意义上，我们的身体和脑恰好就是这样的机器。但我也确实看得出强有力的论证，论证为什么说我们不能将其赋予机器，因为定义机器的运转，仅仅是根据对形式上定义的元素施加的计算过程；也就是说，机器的运转被定义为计算机程序的实例。我并不是因为充当了计算机程序的实例而能理解英文并具有其他形式的意向性的（我假设我充当了某些计算机程序的实例）；据我所知，是因为我是某种具有特定生物（即化学和物理）结构的有机体，在一定条件下，这个结构作为物质成因，能产生感知、行动、理解、学习及其他意向现象。当前论证的部分关切在于，只有具有这种物质成因力的东西才有可能具有意向性。也许其他的物理、化学过程也能产生这些结果，例如也许火星人的脑由其他材料构成，但也有意向性。这是一个经验性问题，有点像“光合作用能否由所含化学成分不同于叶绿素的物质来完成”这种。

但是，当前论证的主要关切在于，纯形式的模型本身对产生意向性而言绝不是充分的，因为形式特性本身不是意向性的构成要素，本身也并无任何物质成因力，只有在被实例化后才会在机器运转时拥有一种产生下一步形式操作的力量。而形式模型的特定实现所具有的任何其他物质成因特性，都与形式模型无关，因为我们要实现这个形式模型，总能找到一种不同的方式，其中明显没有这些成因特性。纵使出现了奇迹，汉语使用者确实是在实现尚克程序，那我们也能将同一个程序代入英语使用者、水管或计算机，而尽管有程序在，它们也无一理解汉语。

脑的运作，紧要之处不在于突触序列的形式投影，而在于序列的实际特性。我见到的所有强版本人工智能的论证，都坚持要围绕认知的投影划清轮廓，旋即宣称投影是真实的东西。





作为总结，我想力图阐明这个论证隐含的一些一般性哲学观点。明晰起见，我将以问答的方式进行，从那个老掉牙的问题开始：

“机器能思考吗？”

答案显然是，能。我们正是这种机器。

“好，那一台人造的机器能思考吗？”

假设人工制造一台具有神经系统的机器是可能的，神经元上有树突和轴突等等一切，与我们足够相似，那问题的答案就会再次明朗：能。如果你能精准地复制起因，你也就能复制出结果。并且，使用不同于人类的化学原理产生出意识、意向性等等一切，也确实是可能的。正如我所说，这是一个经验性问题。

“好，不过一台数字计算机能思考吗？”

如果我们用“数字计算机”所指的，是在某个层面可以被正确地描述为计算机程序实例的任何东西，那么答案又是，当然能，因为我们就可以是各种计算机程序的实例，而我们能思考。

“但是，某物单凭是具有正确程序的计算机，就能思考、理解等等吗？充当一个程序（当然是正确的程序）的实例，本身是理解的充分条件吗？”

我认为这才算是问对了问题，尽管它经常与前一个或前几个问题混淆。答案是，不能。

“为什么不能？”

因为形式符号的操作本身不具有任何意向性，它们毫无意义；它们甚至不是符号操作，因为那些符号不表示任何东西。用语言学的术语来说，它们只有句法而无语义。计算机看似具有的那种意向性，只存在于编程者、使用者、输入者、输出解释者等人的心里。

中文房间一例的目的就是想表明这一点，方法是表明：一旦我们把某物放入一个真正具有意向性的系统（一个人），并且为其编制形式程序，你就能看到形式程序不具有任何额外的意向性。它什么都没有增加，例如，没有增加一个人理解中文的能力。

确切地说，使AI看上去那么诱人的那个特点，即程序与实现之间的区分，对于“模拟可以是复制”的主张来说至关重要。程序与其硬件实现的区分，似是类似于心灵运作层与脑运作层的区分。而我们如果能把心灵运作层描述为一个形式程序的话，似乎就能不借助内省心理学或脑神经生理学而描述心灵的本质了。但“心灵之于脑相当于程序之于硬件”这一等式会在多处瓦解，以下是其中三点：

第一，程序与实现的区分有这样的后果：同一个程序可被没有任何形式的意向性的各种疯狂方式实现。例如，魏岑鲍姆（1976，第2章）详细展示了如何使用一卷卫生纸和一堆小石子构造一台计算机。同样，理解中文故事的程序可以编入水管序列、风机组或一个只讲英文的人，而它们中无一会因此获得对中文的理解。石头、卫生纸、风和水管一开始就不是具有意向性的那种材料，某物只有具备与脑相同的物质成因力，才能具有意向性。而纵然讲英语的人就意向性而言具有正确种类的材料，但也你能轻易看出，靠记住程序，他没有获得任何额外的意向性，因为记住程序无法教会他汉语。

第二，程序是纯形式的，而意向状态却不具有同种意义上的形式性。定义后者要通过它们的内容而非形式。例如，“下雨了”这个信念的定义，不是特定的形式样貌，而是具有满足条件、匹配方向的特定心理内容（见Searle，1979）。实际上，这样的信念在句法意义上甚至都没有形式样貌，因为同一个信念在不同的语言系统中可以获得无数种不同的句法表达。

第三，如前所述，心理状态和事件，实质上是脑的运作产物，而程序却不是同种意义上计算机的产物。

“好，如果程序绝不是心理过程的构成要素的话，为什么还有那么多人相信？这至少需要做些解释吧。”

我真的不知道该如何回答。“计算机的模拟可能是真实的”，这个观点首先就该怀疑，因为无论如何计算机都不限于模拟心理活动。没人会以为计算机模拟的五级大火会把街区烧毁，或是计算机模拟的暴风雨把我们淋个精湿。那究竟为什么有人会以为计算机模拟的理解会真的理解任何东西？有时会听到这种说法：很难让计算机感到疼痛或坠入爱河；而爱与痛并不比认知或其他什么更困难或更简单。要模拟，你所需要的只是正确的输入输出及一个将前者转化为后者的中间程序。这就是计算机做任何事情所需要的。将模拟和复制混为一谈也是同样的错误，无论是对痛、爱、认知、火焰还是暴风雨。

为什么AI在过去显得一定能以某种方式产生进而解释心理现象，甚至或许对很多人来说至今仍是如此？有这样几条原因，而我认为，只有充分揭露这些引发错觉的原因后，我们才能消除这些错觉。

第一，或许也是最重要的一点，是“信息处理”这一概念的混乱：认知科学的许多人相信，人脑及其心灵会做一些被称作“信息处理”的事情，而类似地，计算机及其程序也做信息处理；另一方面，火焰和暴风雨则根本不做信息处理。因此，即使计算机能模拟任何过程的形式特征，它与心灵和脑也处在一种特殊的关系中，因为计算机被适当编程的话，最理想的就是与脑的程序相同，这时，二者的信息处理就等同了，而这一信息处理实际上就是心理的本质。但是，这个论证的问题在于“信息”这一概念的歧义。如果“处理信息”指的是人类思考算术题或者读故事并回答问题，那计算机处理的就不是“信息”。毋宁说，它处理的是形式符号。编程者和计算机输出的解释者使用符号来代表世界中的对象，这一事实完全超出了计算机的范围。再说一遍，计算机只有句法而无语义。因此，如果你向计算机输入“2+2=”，它会输出“4”。但它完全不知道“4”的意思是4或其他任何什么。重点不在于它缺少一些解释其一阶符号的二阶信息，而是其一阶符号对计算机来说就没有任何解释。计算机所具有的，只是更多的符号。因此，引入“信息处理”这一概念就产生了一个两难：我们要解释“信息处理”这一概念，方式要么就是意向性就隐含在处理之中，要么就不是。如果是前者，那么被编程的计算机处理的不是信息，只是形式符号。如果是后者，那么尽管计算机是在处理信息，但也只是在加法器、打字机、胃、恒温器、暴风骤雨等也是在处理信息的意义上：它们在某个层次上都可以被描述为从一端接收信息，经过转换，再产生输出的信息。但这种情况下，输入和输出有赖于外部观察者解释为日常意义上的信息。因此，无论说的是何种信息处理的相似性，这些相似性都无法在计算机和脑之间建立起来。

第二，大部分AI中仍有行为主义和操作主义的残余。由于适当编程的计算机能有与人类相似的输入输出模式，我们就禁不住设定，计算机也有与人类相似的心理状态。而一旦我们看到，在某些领域中，一个系统在概念上和经验上都可能具有人类的能力而全无意向性，我们也就应该能克服这种冲动了。我的台式加法器有计算能力，但没有意向性；我在本文中也力图表明，一个系统可以具有复制自汉语母语者的输入输出能力，却依然不理解中文，不管它是如何被编程的。图灵测试就当之无愧地属于典型的行为主义和操作主义传统。我相信，如果AI工作者能彻底弃绝行为主义和操作主义，模拟与复制间的许多混淆就能消除。

第三，上述操作主义的残余与某种形式的二元论残余相结合。实际上，强AI要有意义，唯有给定二元论的假设，即对心灵而言，脑无关紧要。在强AI（及功能主义）中，重要的是程序，而程序独立于其机器实现；实际上，对AI而言，实现同一个程序，可以通过电子仪器，笛卡尔式的心理实体，或者黑格尔式的世界精神。讨论这些问题时，最令我吃惊的一个发现是，许多AI工作者都无比震惊于我的观点，即真实的人类心理现象可能依赖于真实人脑的真实物理／化学属性。但你如果花一分钟想想，就会发现我本不该吃惊，因为除非接受某种形式的二元论，否则强AI的企划根本没戏。这个企划就是要通过设计程序来复制和解释心理，但除非心灵在概念上和经验上全都独立于脑，否则这个企划无从施行，因为程序是完全独立于任何实现的。除非你相信心灵与脑在概念和经验上都是可分离的，即相信某种强形式的二元论，否则你就不能指望通过编写和运行程序来复制心理，因为程序一定是独立于脑或其他任何特定形式的实例的。如果各种心理活动是由对形式符号的计算操作构成的，那么就能得出，它们与脑就没有任何值得关注的联系；唯一的联系就是，脑只不过碰巧是能够为程序充当实例的无数种机器之一。这种形式的二元论不是传统的笛卡尔式的，因为后者主张存在两种实体；不过它坚持心灵的特定心理方面与脑的实际特性没有内在的联系，在这个意义上它又是笛卡尔式的二元论。AI文献中经常包含对“二元论”的强烈批判，这一情况把这种深层的二元论掩蔽了起来；这些作者似乎都没意识到，他们自己的立场预设了一种强版本的二元论。

“机器能思考吗？”我自己的看法是，只有一种机器能够思考，它也确实是一种非常特殊的机器，那就是脑，以及与脑具有相同物质成因力的机器。这也是强AI关于思考一直没给我们讲出什么内容的主要原因，因为它不讲任何关于机器的事。根据强AI自身的定义，它事关的是程序，而程序并不是机器。而意向性，无论还会是什么，首先都是一种生物现象，其产生要在物质成因方面依赖于特定的生化特性，就像泌乳、光合作用等任何生物现象一样。没人会认为只要用计算机模拟泌乳和光合作用的形式性事件序列，然后运行这些模拟，我们就能生产出奶和糖；而一旦论及心灵，就有很多人愿意相信这种奇迹，这皆因一种根深蒂固的二元论：他们设想的心灵不同于奶和糖，是一种完全独立于特定物质成因的形式过程。

为维护这种二元论，人们常常流露出这种希望：脑是一台数字计算机（顺便一提，我们以前经常把计算机称为“电脑”）。但这无济于事。脑当然是数字计算机，因为每样东西都是数字计算机，脑也不例外。重点是，脑有产生意向性的物质因能力，而这并不在于它能充当计算机程序的实例，因为无论你想到了什么程序，都有可能找到点什么，既是它的实例，又不具有任何心理状态。无论脑是做了什么而产生了意向性，都不可能是因为它充当了一个程序的实例，因为没有程序就其本身而言是意向性的充分条件。[3]


反思

本文最初是与来自各方的28个回应一同面世的。大多回应都包含精彩的评论，但重刊它们会挤爆这本书，而且有些怎么看都有点太过技术化了。塞尔文章的一大好处是，对没有受过AI、神经学、哲学或其他相关学科特殊训练的人而言，本文非常易懂。

我们的立场和塞尔完全相反。不过我们发现塞尔是一位雄辩的对手。我们不会尝试彻底推翻他的所有论点，而会专注于他所提问题中的几个。对其他观点的回应，则隐含在本书的其他部分中。

塞尔的论文基于他精巧的“中文房间思想实验”，其中，读者被敦促去代入一个人，手动去执行一串某个AI程序会执行的步骤：这程序非常聪明，运行方式与人类充分相似，因而能通过图灵测试，而它阅读中文故事并用中文回答相关问题时，就会执行这串步骤。我们认为塞尔做了一个严重的、根本上的错误表征，给了人这种印象：有理由认为人类能做到这样的事情。接受了这一图景的读者，会不知不觉在智能与符号操作的关系上陷入一种极其不切实际的观念。

塞尔要在读者身上引发这种错觉（当然他自己并不认为这是错觉！），是有先决条件的。不同概念层次的两个系统在复杂性方面存在巨大的差异，而他须得先让读者忽视这一点。一旦他做到了这个，其余的就是小菜一碟。一开始，塞尔邀请读者与之一同手动模拟一个现存的AI程序，它能在有限的几个领域中，以有限的方式回答有限种类的问题。让一个人手动模拟这个或任何现存的AI程序，即让他在计算机那样的细节水平上一步步地把程序实现出来，这项艰巨乏味的工作就算不会耗时几周数月，总也要花上几天。但塞尔没有指出这一点，他像个娴熟的魔术师，巧妙地转移了读者的注意力；相反，他假设出了一个通过了图灵测试的程序，把读者的想象转移到了这个程序上！他已经跨越了若干个能力等级，却对此只字不提。他再次邀请读者将自己代入那个执行亦步亦趋的模拟的人，并去“感受对中文理解的缺失”。这就是塞尔论证的要害。

我们对此的回应基本上是“系统回应”（而且我们过会儿会表明，塞尔的回应在某种程度上也是）：试图将理解归给（偶然）具有生命的模拟器是错误的；理解属于系统整体，其中包括被塞尔随口说成是“草稿纸”的东西。我们觉得，这句轻率的评论揭示了塞尔的想象是如何让他认不清实情的。一台能思考的计算机带给约翰·塞尔的厌恶，就好像非欧几何之于它不经意的发现者杰罗拉莫·萨凯里，后者极力否认自己的杰作。直到18世纪晚期，要人们接受由另一种几何学带来的概念扩展都还为时尚早。而大约50年后，非欧几何重获发现并渐获接受。[4]

或许同样的事情也会发生在“人工意向性”上——如果它什么时候被创造出来的话。假如真出现了一个能通过图灵测试的程序，看样子塞尔不仅不会为它的能力和深度感到惊奇，反而会继续坚称它缺乏某种神奇的“脑的物质成因力”（无论这究竟是什么）。为了指出这一概念的空洞，泽农·佩利申在对塞尔的回应中，想知道下面这段话（很能让人想起选文12，祖波夫《脑的故事》）是否精当地刻画了塞尔的观点：


如果你脑中会有越来越多的细胞被集成电路芯片代替，且被编程为每个单元的输入输出功能都与被代替的单元一致，那么你十有八九还是会像现在这样正常说话，只不过你不再通过说话表达任何意义。我们外部观察者视为言词的东西，对你则成了电路导致你发出的某种噪声。


塞尔立场的弱点在于，他没有提供一个明确的方式来判定真正的意义，或者其实就是真正的“你”，是何时从系统中消失的。他只是坚称，有些系统凭其“物质成因力”具有意向性，而有些没有。这些成因力缘起何处，他也游移不定。有时脑好像是由“正确的材料”构成的，有时好像又不是这样。好像是某个当下，什么方便就是什么：这会儿，这种捉摸不定的本质区分了“形式”与“内容”，过会儿，另一种本质又区分了句法和语义，等等。

对系统回应的支持者，塞尔提出的思想是，房间里的那个人（从现在起我们称之为“塞尔妖”）只须记住或掌握“有限草稿纸”上的所有材料。仿佛真有种什么想得到的想象力发挥方式，能让这么一个人类做到这事似的。那“有限的草稿纸”上的程序囊括了一整个的心灵和性格，它们属于这样一种东西，它对书面材料的反应能力和人类一样复杂，因为它能通过图灵测试。

有哪个人类能轻易“吞下”对另一个人的心灵的完整描述吗？记住一个文段就够让我们觉得难了。但塞尔想象中的妖怪却能掌握很可能数百万甚至数十亿张纸上密密麻麻写满的抽象符号，而且所有这些信息还都是可用的，无论何时需要，都能无碍调取！这剧情中如此不现实的一面全被轻描淡写一笔带过，而且也不在塞尔说服读者的核心论证之中。实际上，刚好相反，他论证的关键部分在于掩盖数量级方面的问题，不然多疑的读者就会意识到，几乎所有的理解都在于纸上的这数十亿符号，而全然不在于妖怪。妖怪有生命这一情况只是枝节，无关紧要，实际上还是误导，而塞尔却误以为它事关重大。

我们要展示塞尔自己其实拥护了系统回应，以此来支持上述论点。为此，我们首先要将塞尔的思想实验置于一个更宽泛的语境中。我们尤其要表明，塞尔的设定只是相关的一大类思想实验之一，其中还颇有几个是本书其他选文的主题。这类思想实验中，每一个都可以在一个思想实验发生器上，通过选定一套特定的“旋钮挡位”而界定出来，目的是为了在你的心灵之眼中创造出各种人类心理活动的虚幻模拟。每个不同的思想实验都是一个“直觉泵”（丹尼特的措辞），它会放大问题的某个方面，旨在将读者推向某些结论。我们发现相关的旋钮大致有5个，当然其他人可能会想到更多。





旋钮1：控制构造模拟的物理“材料”。其挡位包括：神经元和化学物质，水管和水，草稿纸和写在上面的符号，卫生纸和石子，数据结构和流程，等等。

旋钮2：控制要模仿人脑的模拟的精度等级。它可被任意设定为亚原子程度的精细级，像细胞和突触这样的粗糙级，甚或是AI研究者、认知心理学家处理的等级：概念和观念级、表征和过程级。

旋钮3：控制模拟的物理规模。我们假定，微型化技术使我们做得出迷你的水管网络或固态芯片网络，小得可以放在顶针里；反过来，任何化学过程也可以放大到肉眼可见的宏观尺度。

旋钮4：这个旋钮很关键，控制执行模拟的妖怪的尺寸和性质。如果是一个正常体型的人类，我们就叫它“塞尔妖”；如果是一个神经元和粒子就能容纳的小精灵一样的小生灵，我们就以约翰·豪格兰之名叫它“豪格兰妖”，他在对塞尔的回应中强调了这一概念。这个旋钮的挡位也会决定妖怪是否有生命。

旋钮5：控制妖怪的运行速度。妖怪运转的挡位可以设为没命地快（每微秒即运算数百万次）或恼人地慢（可能好几秒才运算一次）。





现在，通过摆弄各旋钮的挡位，我们就能炮制出各种思想实验。一种选择会产生出选文26《对话爱因斯坦的脑》中描绘的情景。再换一种，就会产生塞尔的中文房间，具体而言旋钮挡位设定如下：





旋钮1：纸和符号

旋钮2：概念、观念级

旋钮3：房间大小

旋钮4：人类体型的妖怪

旋钮5：慢速挡（每隔几秒运算一次）





请注意，塞尔原则上并不反对假定一个带有这些参数的模拟能够通过图灵测试。他只是质疑其隐含的东西。

还有最后一个参数，它不是旋钮，而是观察这一思想实验的视角所在。让我们来给这个单调的实验加点颜色：假定模拟出的最终汉语使用者是位人类女性，而其中的妖怪（如果有生命）则总是男性，这样我们就可以在妖怪视角和系统视角之间有所选择了。记住，按照假设，妖怪和模拟出的女士，都有同样的能力清楚表达他们自己是否有理解、他们正在经历什么等方面的观点。塞尔无论如何都坚持，我们只能从妖怪的视角出发来观察这个实验。他坚称，无论模拟女士就自己的理解断言什么（当然是用汉语），我们都要无视她的断言，而把注意力放在里面那个执行符号操作的妖怪身上。塞尔的断言相当于认为，这里只有一个视角，而非两个。如果有人接受塞尔描述整个实验的方式，这一断言在直觉上就对他有极强的吸引力，因为这个妖怪有着我们的体型、讲着我们的语言并以我们的速度运转；而要认同这位“女士”就很难了，毕竟她回答起问题，速度也就每个世纪一次（还得是走运的情况），用的也是“毫无意义的曲里拐弯儿”。

但如果我们改变某些旋钮的挡位，我们改换视角的难度也会改变。尤其是豪格兰的变体，旋钮调整如下：





旋钮1：神经元和化学物质

旋钮2：神经元电信号级

旋钮3：脑子大小

旋钮4：小小的妖怪

旋钮5：快得眼花缭乱





豪格兰想让我们想象的是：有一位真正的女士，脑子不幸有些缺陷，不再能在神经元之间传递神经递质。不过所幸脑中栖居着极其微小又极其迅速的豪格兰妖，每当有神经元要向邻近的神经元释放神经递质时，它就介入进来。它会去“触碰”邻接神经元上适当的突触，而且对这个神经元而言，功能上与收到真正的神经递质无异。而且豪格兰妖十分迅速，能以万亿分之一秒的速度在突触间跳转，从不耽误行程。这样，如果这位女士身体健康，她的脑就会以它该有的样子运转。现在豪格兰问塞尔，这位女士还是在思考吗，亦即她具有意向性吗，或者，回想图灵引用杰斐逊教授的话，她是否只是“发出人工信号”？

你可能以为，塞尔会力劝我们听从并认同妖怪，而对让我们听从并认同这位女士的系统回应敬而远之。但在他对豪格兰的回应中，塞尔让我们大吃一惊：这次，他选择了听从她，无视了那个在它小小的有利位置上咒骂不已的妖怪。它对我们喊的是：“蠢货！别听她的！她只是个傀儡，她的所有行动都产生自我的触碰，产生自嵌入神经元的程序，而我就在这许多神经元中游走着！”但塞尔并没有理会这个豪格兰妖的哭诉，他说：“她的神经元仍然具有正确的物质成因力，只是需要妖怪的一点帮助。”

我们可以在塞尔原初的设定和这个修改后的设定之间建立一个映射。“有限的草稿纸”对应的是现在这位女士脑中的所有突触。写在“草稿纸”上的AI程序对应的是她脑的整体布局，这相当于是告诉妖怪该何时去、如何知道去触碰哪个突触的大型指令。在纸上书写“毫无意义的中文曲里拐弯儿”对应的则是触碰突触。假设我们就采用这样的设定，此外再变动一下尺寸和速度的旋钮。我们将这位女士的脑扩大到地球的尺寸，那么妖怪也就变成了我们这样体型的塞尔妖，而不是迷你豪格兰妖了。并且，我们让塞尔妖以一个对人类而言合理的速度行动，而不是在几微秒间就绕着这个大球体飞越数千英里。这样的话，塞尔会希望我们认同哪个层次呢？我们就不乱猜了，但对我们来说，如果系统回应在刚才的情况下让人难以抗拒，现在亦应如是。

必须承认，塞尔的思想实验生动地提出了什么是真正理解一门语言的问题。我们想姑且说点题外话。思考一下这个问题：“操作一门语言的书面或口头符号的何种能力，才算对这门语言的真正理解？”鹦鹉能学舌英语，但不懂英语。电话报时服务里的女性录音能准确通报一天中的时间，但却也不是一个懂英语的系统的喉舌。那个声音背后没有心思，它的心理底层早被撇去，只留下了一个拟人的特性。或许有小孩会疑惑，怎么会有人愿意做那么枯燥的工作，还做得这么敬业，这会让我们发笑。当然，如果她的声音是由一个能够通过图灵测试的灵活AI程序驱动的，那就另当别论了。

想象你正在中国教书。再想象，你意识到自己用英语形成想法，并意识到自己在最后一刻应用转换规则（实际上它们就是最后一瞬间的规则），以诡异且“毫无意义”的方式将英文想法转换为活动嘴巴和声带的指令，而你所有的学生坐在下面，看上去对你的表现十分满意。当他们举手发言时，尽管他们的异国腔调在你听来完全不知所云，但你毕竟有所准备，迅速地应用了某些反转规则复原了他们话里的英文意思……你会觉得自己是真的在讲汉语吗？会觉得自己对汉语思维有任何领悟吗？或者，你真的能够想象这种场景吗？它现实吗？谁要是用了这种方法，真的能讲好一门外语吗？

标准线是“你必须学会用汉语思考”。不过这在于什么？任何有过这种体验的人都会认可这个说法：外语的声音很快就“听不见”了。你透过语音听懂了意思，而非听见了语音，就像你透过窗户看到了东西，而非看见窗户。当然，如果你非常努力，也可以让一门熟悉的语言听上去像纯粹未经解析的声音，就像如果你愿意，也可以看见窗玻璃；但二者不可得兼：你听声音，不可能同时既听到了意思又没听到。因而，大多数时候，人们听得到意思。那些因着迷于语音而学习一门语言的人不免要失望了——不过，一旦掌握了那些语音，即便不能再天真地聆听它们，也会是美妙而振奋的体验。（把此类分析应用于聆听音乐，会是一件有趣的事情。尽管在其中，仅仅听到声音与听到“意思”之间的区分远未获得透彻的理解，但这种区分似乎确然真实存在。）

学习一门外语，包含着超越一个人自己的母语，包含着将新语言正确地结合到思想产生的介质中去。思想的萌发，在新语言中必须也能像在母语中一样（或几乎一样）容易。一门新语言的习惯一层层渗透，最终被神经元吸纳，其实现方式至今仍是一个巨大的谜。不过有一点可以肯定，掌握一门语言并不是让你的“英文子系统”为你执行一套规则程序，使你能处理不知所云的声音和符号。无论怎样，新语言必须与你内部的表征系统，即你的全套概念、意象等等融合，融合的紧密方式要和母语的情况相同。为了更细致地思考这个问题，必须发展出一个清晰的实施层次的概念，一个强有力的计算机科学概念。

计算机科学家对一个系统“仿真”（emulate）另一个系统的想法是习以为常的。事实上，这来自艾伦·图灵在1936年证明的一个定理：任何一台通用数字计算机都能装扮成另一台通用数字计算机，对外界而言，唯一的差别仅在于速度。“仿真”一词和“模拟”（simulation）意指相反，专指一台计算机对另一台计算机的模拟；而“模拟”则指对其他现象的建模，例如飓风、人口曲线、大选甚至计算机用户。

一个主要的区别在于，模拟几乎总是近似的，取决于所涉现象的模型的本质，而仿真则是严格意义上精确的。它的精确程度是，比如一台Sigma 5计算机仿真一台构造不同的计算机，比如DEC的PDP-10时，Sigma 5的用户丝毫察觉不到自己操作的不是真正的DEC。[5]将一种构造嵌入另一种，就产生了所谓的“虚拟机”，此处就是虚拟的PDP-10。每台虚拟机底层都是一台别的机器。可能是同类机器，甚至还可能是别的虚拟机。安德鲁·塔嫩鲍姆在他的著作《计算机组成：结构化方法》（Structured Computer Oranization）中使用了虚拟机的概念来解释大型的计算机系统如何能被视作依次实施、不断叠加的虚拟机堆栈，而栈底当然是一台真正的机器！不过在任何情况下，层次间是密闭的，滴水不漏，就像塞尔妖无法同由他组成的汉语使用者交谈。（想象他们做何种对话会很有趣，还要假设有口译在场，因为塞尔妖根本不懂汉语。）

理论上，任何两个层次之间都有可能相互沟通，但惯例上认为这是种坏样式：要严禁层次混同。尽管如此，“模糊两个实施层次之间的界限”这个禁果很可能正是一个“人类系统”学习外语时所发生的。外语并不是运行在母语之上的寄生软件，而是与母语（几乎）同等地深植于硬件之中。某种意义上，掌握一门外语会给人的底层“机制”带来深刻的变化，神经元发放的方式发生巨大且连贯的成套变化，从而创造出高层实体，即符号，相互触发的新方式。

Sigma 5前面板（计算机历史博物馆，Marcin Wichary摄，2009）



类比到计算机系统上，一个高层程序必须有办法让执行程序的“妖怪”内部产生变化。这种做法对计算机科学的当前样式而言是完全陌生的，后者是将一个层次严格垂直且密闭地实施于另一个层次之上。我们感觉，高层的环回及影响（作为高层的基础支撑的）低层的能力，是种非常接近于意识核心的戏法。或许有一天，它会被证明是推进更具灵活性的计算机设计、当然也是通向人工智能的关键因素。特别是，“理解”的真正意味是什么，要满意地回答这一问题，无疑需要更为清晰地勾勒出一个符号操作系统中的不同层次之间相互依赖和作用的方式。总之，这些概念已被证明是很难捉摸的，要清晰地理解它们，大概路还很长。

这个对多层次的讨论颇为费解，你可能已经开始疑惑“层次”究竟是什么意思。这可是个绝顶难题。但凡层次之间相互封闭，就像塞尔妖和讲汉语的女士那样，问题就相当清楚。当它们开始混作一团时，当心！塞尔可能会承认他的思想实验中有两个层次，但他不愿承认其中也有两个视角，两个真正有感觉、“有体验”的存在者。他担心，一旦我们承认有些计算系统可能拥有体验，这就会成为潘多拉的盒子，忽然之间“到处都是心灵”：在蠕动的胃中、肝脏中、汽车引擎中，等等。

塞尔似乎相信，无论什么系统都能被赋予信念、感受等等，只要有人能费尽心思地把这个系统描述为AI程序的实例。显然，这个令人不安想法会引向泛心论。塞尔也确实相信，搞AI的人无意中承诺了一个泛心论的世界图景。

为了避开自己布置的陷阱，塞尔坚称，当你开始发现到处都是心灵时，在那些无生命的物体上发现的“信念”“感受”都不是真的，而是“伪的”。它们缺少意向性！它们没有脑的物质成因力！（当然，塞尔会告诫大家当心，别把这些概念与“灵魂”这个朴素二元论的概念混淆。）

我们要避开这个陷阱，方法是彻底否认它的存在。认为到处都是心灵是错误的。就像脑不会潜藏在汽车引擎和肝脏中，心灵也不会。

有必要在这上面多说两句。如果你能在蠕动的胃中看到思想过程的复杂性，那还有什么能阻止你把碳酸饮料的气泡形状解读成对肖邦E小调钢琴协奏曲的编码？瑞士奶酪上的孔洞不也编码了整部美国历史吗？它们当然做到了，用汉语，也用英语。毕竟一切写在一切中！巴赫的《勃兰登堡协奏曲》第2首是以《哈姆雷特》的结构编码而成，而《哈姆雷特》当然也可以用你吞掉的最后一块生日蛋糕的结构来解读（只要你知道编码）。

所有这些事例的问题都在于，你要指定代码，事先却并不知道自己想要解读什么。不然，借助一套任意构造的后天编码，你可以从一场棒球赛或一根草叶中扯出对任何一个人心理活动的描述来。

诚然，不同心灵的精审复杂程度不同，但仅当在精审复杂的表征系统存在时，配叫作“心灵”的心灵才是存在的，而在汽车引擎或肝脏中，描述不出一种映射，指向一个时间上持存且自我更新的表征系统。鉴于人们从大金字塔或巨石阵的结构中、从巴赫的音乐、莎士比亚的戏剧等等中解读出了额外的意义，或许也有人能以类似方式从轰鸣的汽车引擎中读出“心性”，即编织牵强附会的术数式映射体系，有求必应地按解释者的意愿去歪曲捏造。但我们怀疑这是否是塞尔想要的（我们的确认为这就是他想要的）。

心灵存在于脑中，也可能将会存在于被编程的机器中。如果这样的机器能问世，那时，它的成因力并非来自构造它的物质，而是来自它的设计和它所运行的程序。而我们要了解到它们具有这样的成因力，途径是和它们交谈，并认真聆听它们要说的话。

D. R. H.



* * *



[1] 而且，“理解”同时暗含了拥有心理（意向的）状态以及这些状态的真（有效、成功）。鉴于讨论的目的，我们只考虑具有这些状态时的情况。——原注

[2] 根据定义，意向性是特定心理状态的特征，因这一特征，此类心理状态指向或关涉世界中的事物和事态。因此，信念、欲望和意图等都是意向性状态，焦虑、抑郁等无所指的形式则不是。——原注

[3] 承蒙一大批人与我讨论这些问题，也承蒙他们耐心地尝试帮我克服对人工智能的无知。我要特别感谢奈德·布洛克、休伯特·德雷福斯、约翰·豪格兰、罗杰·尚克、罗伯特·威林斯基和特里·维诺格拉德。——原注

[4] 萨凯里（Giovanni Gerolamo Saccheri，1667-1733），意大利耶稣会教士，哲学家，数学家。他生前最后一年发表了一本几何著作，本意是证明欧几里得的平行公设。他从四边形内角和不为360度（实质是假设三角形内角和不等于两个直角）出发，希望得到荒谬的结论，但到最后逻辑却是一贯的；即便如此，他没有正视这一点。但这项工作成为了非欧几何的先驱性工作之一。18世纪晚期到19世纪早期，非欧几何的更多先驱性工作先后诞生，但因思维惯性等原因一直没有确立新体系甚至没有公布。真正的公布和确立，约在19世纪中叶（罗氏几何）。

[5] Sigma是SDS公司（Scientific Data Systems）1966年起推出的第三代计算机系列，以32位Sigma 7最为著名，7型曾是UCLA主机，IBM 360的竞争款。1969年SDS为施乐收购。PDP（programmed data processor，编程数据处理器）是迪吉多公司（Digital Equipment Coorperation，DEC）自1960年起推出的晶体管小型机系列，其中1970年的PDP-11是第一款16位机。但苹果等个人微机出现后，DEC业绩一落千丈，直至1998年为康柏收购（而康柏后为惠普收购）。