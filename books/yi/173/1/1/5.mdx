---
---
---
title: 学习是一种优化的奖励函数
---




还记得可以识别数字的LeNet系统吗？这种人工神经网络学习的前提是为其提供正确答案。对于每个输入的图像，它需要知道它对应的是10个可能数字中的哪一个。人工神经网络只能通过计算其响应和正确答案之间的偏差来进行自我纠正，这个过程被称为“监督学习”（supervised learning）。系统之外的监督者知道正确答案，并试图将其传输给机器。这种方法是有效的，但应该注意的是，这种提前知道正确答案的情况相当罕见。当幼儿学习走路时，没有人确切地告诉他们应该收缩哪块肌肉，他们收到的只有一次又一次的鼓励，直到不再跌倒。幼儿的学习完全是基于对结果的评估：我摔倒了，或者我终于走到了房间的另一边。人工智能面临着同样的“无监督学习”（unsupervised learning）问题。当一台机器学习玩电子游戏时，它被告知的唯一一件事情就是它必须努力获得最高分。没有人事先告诉它可以采取哪些具体行动来实现这一点。那么，它是如何迅速找到正确方法的呢？

科学家们发明了“强化学习”（reinforcement learning）来应对这一挑战。我们不向系统提供任何关于它必须做什么的细节（也没人知道该做什么），只是提供“奖励”，即一种量化分数形式的评估。5但更糟糕的是，机器可能会在延迟很长一段时间后才收到分数，远远滞后于它做决定的那一刻。谷歌的子公司DeepMind研发了一款能够下国际象棋、跳棋和围棋的机器，它使用的就是强化学习的原理。这个任务很艰巨，因为只有到最后一刻，系统才会收到唯一的表明输赢的奖励信号。在棋局进行过程中，系统不会收到任何反馈。那么，系统如何才能计算出每一步要做什么呢？而且，一旦揭晓了最终分数，机器如何回溯评估它前面做的决定呢？

计算机科学家发现的技巧是给机器编程，让它同时做两件事：行动和自我评估。自我评估被称为“评论者”，它的目标是尽可能准确地评估游戏的状态，以便预测最终的奖励。我是赢了还是输了？我处于势均力敌的状态还是快要输了？评论者可以让系统时时刻刻评估自己的行动，而不仅仅是在结束的时候得知结论。“行动者”的目标是使用这个评估来纠正自己。等等，我最好不要这么做，因为评论者认为这会增加我失败的概率。

一次又一次的反复试验后，行动者和评论者一起进步：一个专注于最有效的行动，另一个学习更敏锐地评估这些行动的结果。这个“行动者—评论者”组合被赋予了一种非凡的先见之明：在浩瀚的棋局海洋中，预测哪些行为可能会赢，哪些行为只会导致失败。

行动者与评论者的结合是当代人工智能最有效的策略之一。在层级化的神经网络的支持下，它创造了奇迹。早在20世纪80年代，它就使人工神经网络赢得了五子棋世界杯冠军。最近，DeepMind运用它研发了一个多功能人工神经网络，该网络可以学会所有类型的电子游戏，如超级马里奥和俄罗斯方块。6人们只需将图像的像素作为输入、可能的动作作为输出，并将游戏得分作为奖励函数，机器自己就能学会其他所有的细节。当它玩俄罗斯方块时，它会发现屏幕是由形状组成的，坠落的那个砖块是最重要的元素，各种动作可以改变它的方向和位置，最终将自己变成一个超级人造玩家。当它玩超级马里奥时，输入和奖励的变化教会它注意完全不同的设置：马里奥身体的像素是什么，他是如何移动的，敌人在哪里，墙壁是什么形状的，什么是门，什么是陷阱，什么是金币以及如何与每个元素互动。通过调整其参数，调节将各层级联结在一起的数百万个联结……这个人工神经网络掌握了所有游戏的需求，学习识别俄罗斯方块、吃豆人或刺猬索尼克里的各种形状。

教一台机器玩电子游戏有什么意义？2年后，DeepMind的工程师利用他们从玩游戏中学到的东西解决了一个至关重要的经济问题：谷歌应该如何优化其计算机服务器的管理以获得更大效益？二者所需的人工神经网络的结构基本相似，唯一需要改变的是输入（日期、时间、天气、国际事件、搜索请求、联结到每台服务器的人数等）、输出（在各大洲打开或关闭这个或那个服务器）和奖励功能（消耗更少的能源）。其结果是谷歌的电力消耗得以大幅缩减。在无数专业工程师已经优化了这些服务器的基础上，该神经网络还能替谷歌缩减高达40%的能源消耗，节省数千万美元。人工智能已经真正达到可以颠覆整个行业的水平了。

DeepMind之后又取得了更令人惊叹的成就，它的AlphaGo程序在围棋人机大战中击败了共获得18个世界冠军的李世石。直到现在，该事件依然被认为是人工智能领域的珠穆朗玛峰。7围棋比赛在一个19×19的方格棋盘上进行，有361个位子可以下黑白棋子，它的排列组合的数量如此之大，以至于系统地探索每个棋手未来可以落子的所有行动严格来说根本不可能。然而，强化学习让AlphaGo程序比任何人类棋手都能更好地识别有利和不利的组合。其中一个诀窍就是让程序与自己对打，就像棋手通过同时下白棋和下黑棋来进行训练一样。如此，在每一场比赛结束时，获胜的程序会强化自己的行动，而落败者会削减其步骤，但两种做法都让程序学会了更有效地评估自己的行动。

我们嘲笑电影《吹牛大王历险记》（The Adventures of Baron Munchausen）中的男爵，他愚蠢地试图通过提拉自己靴子上的扣带使自己飞起来。然而，在人工智能领域，蒙克豪森男爵的疯狂催生了一种神奇的“自举法”（bootstrapping）算法策略。该算法从缺乏知识支撑的毫无意义的架构开始，一点一点地慢慢成长为世界冠军，它所做的一切只是与自己较量而已。

通过让两个人工神经网络合作或者竞争来提高学习速度的方法使人工智能获得了长足的进步。最新的一个想法被称为“对抗性学习”（adversarial learning）8，通过训练两个系统对抗让其中的一个系统成为专家（比如凡·高的画的鉴定专家），而另一个的唯一目标是让第一个系统失败（通过学习成为一个出色的伪造凡·高的画作的赝品大师）。第一个系统只要成功识别出凡·高的真迹就会获得奖励，而第二个系统只要成功骗过了第一个系统的“眼睛”就会获得奖励。这种对抗性学习算法同时产生了两种人工智能，一种是研究凡·高的权威，擅长通过最小的细节来鉴定一幅真正的画作；另一种是天才伪造者，能够绘制能愚弄最好的鉴赏专家的画作。

这种方法适用于人脑吗？我们的两个脑半球和众多的皮层下基底神经核也聚集了一大批“专家”，他们相互争斗、协调和评估。脑中的某些区域会模拟别人在做什么，它使我们可以预见和想象自己行为产生的结果，这些预想无比真实，堪比最好的伪造者，比如我们的记忆和想象可以让我们看到去年夏天游泳的海湾，或者我们在黑暗中抓住的门把手。有些脑区会学习批评其他脑区，它们不断地评估我们的能力，预测我们可能得到的奖惩，促使我们采取行动或保持沉默。我们还将看到元认知这个认识自我、评估自我、在心理上模拟如果我们这样做或那样做会发生什么的能力，元认知在人类学习中发挥着根本作用。我们自己形成的观点帮助我们进步，在某些情况下又会将我们锁在一个失败的恶性循环中。因此，将脑视为一群协作和竞争的专家的集合并无不妥。