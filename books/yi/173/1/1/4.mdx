---
---
---
title: 学习就是探索各种可能性
---




前面讲述的纠错过程的问题之一是，它可能会卡在一组不是最优的参数上。想象一下，一颗高尔夫球在草坪上沿着最陡峭的斜坡滚动，它可能会被卡在草地上的一个小凹陷里，无法到达整个场地的最低点，这个最低点就是全局最优解（absolute optimum）。同样，梯度下降算法有时会卡在一个它无法退出的点上，这被称为“局部最小值”（local minimum），它是参数空间中的一口井，是学习算法被困住后无法逃脱的陷阱，此时，学习陷入停滞，因为所有的改变似乎都适得其反，每一次的改变都会增加错误率。这个系统觉得它已经学到了所有它能学到的东西，以至于对仍然存在的更好的设置视而不见，哪怕这些设置在参数空间中近在咫尺。梯度下降算法无法“看到”它们，因为它拒绝上坡，不愿意越过眼前的山到达另一个下坡。由于“目光短浅”，它只在距离起点一小段距离的地方探险，因此错过了更遥远但更好的参数配置。

这个问题对你来说是不是太抽象了？来看一下具体的情况。你去食品市场购物，在那里你花了一些时间寻找最便宜的特卖品。沿着过道，你走过第一个卖家摊位（他家产品的价格似乎过高），避开第二家（他家产品一直都很贵），最后停在了第三家，他家产品似乎比前两家便宜得多。但是，谁能说隔着一条过道，别家产品的价格会不会更诱人呢？专注于当下最优价格并不能保证找到全市场最优价格。

计算机科学家经常面临这一困难，他们会使用一整套的算法解决它。这些算法中的大多数都是在寻找最佳参数的过程中引入一点随机性（randomness）。原理很简单，与其只关注市场的一条通道，何不随意逛一逛；与其让高尔夫球随意地沿着斜坡滚下来，何不拿起来摇晃一下再扔下去，以减少落入陷阱的概率。有时，随机搜索算法（stochastic search algorithms）会尝试一个更远且部分随机的设置，这样，如果有更好的解决方案可以触及，就会被算法找到。在实践中，人们可以通过各种方式引入一定程度的随机性到设定中，或更新参数使训练范例的呈现顺序多样化，或在数据中添加一些噪声，或随机选择一部分联结来用，所有这些方法都会使学习的算法更加健全。

一些机器学习算法也会从达尔文的进化算法中去寻找灵感。比如在参数优化的过程中，基于之前的解决方案引入突变和随机交叉。在生物学中，必须小心控制突变的速度，才不会浪费时间或制造出危险的物种来。

机器学习算法亦是如此，灵感来自锻造技术。为了锻造一把坚不可摧的剑，铁匠会通过退火来优化金属的性能。他们将金属多次加热，每次加热的温度依次降低，以增加原子按规则结构排列的机会。这个技术现在已经被用到了计算机科学中，研究者采用虚拟的“温度”逐渐降低的方式，模拟退火的算法，在参数中引入随机性。随机性的概率起初会很高，但会逐渐下降，直到系统固定在一个最佳设置上。

计算机科学家发现这些算法非常有效，所以一些算法在进化过程中被内化到我们脑中也就不足为奇了。随机的探索、随机产生的好奇心和频繁的神经元放电都在智人的学习中扮演着重要的角色。无论我们是在玩“石头剪刀布”、在即兴创作爵士乐，还是在研究数学问题的可能解法，随机性都是至关重要的。孩子玩耍时就是在用大量的随机性在探索各种的可能性，那时他们已经进入学习模式了。而在晚上睡觉时，他们的脑会继续胡思乱想，直到突然想出一个最能解释他们白天经历的说法。在本书的第三部分，我将讨论半随机算法（semi-random algorithm），这种算法控制着孩子非凡的好奇心以及少数成年人的赤子之心。