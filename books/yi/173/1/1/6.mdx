---
---
---
title: 学习限定了搜索空间
---




当代人工智能仍然面临着一个重大问题，那就是内部模型的参数越多，系统就越难找到最佳的调整方法。而在目前的人工神经网络中，搜索空间是巨大的。因此，计算机科学家不得不处理大规模的组合爆炸：在每个层级都有数以百万计的选择，而它们的组合是如此之多，系统不可能探索所有的选择。因此，学习有时会非常缓慢，需要在这片广阔的可能性地图中进行数十亿次尝试才能将系统推向正确的方向。在巨大的空间中，无论数据有多少，都会变得稀缺。这个问题被称为“维度诅咒”（curse of dimensionality），当你有数百万个潜在的杠杆需要撬动时，学习就会变得非常困难。

神经网络拥有的大量参数往往还会导致另外一个问题，这就是所谓的“过度拟合”（overfitting）或“过度学习”：系统拥有如此多的自由度，以至于它发现记住每个例子的所有细节比找出一个更普遍的规则来解释这些细节更容易。

正如计算机科学之父约翰·冯·诺伊曼（John von Neumann）的名言：“用4个参数我可以画出一头大象，用5个参数我可以让它甩动鼻子。”他认为拥有太多的自由参数可能是一种诅咒，因为仅仅通过记住每个细节来“过度拟合”各种数据太容易了，你不需要对大象这个物种有太多深入的了解，就可以将其归类为皮厚的动物。但这并不意味着系统捕捉到了所有重要的东西，自由参数过多不利于抽象化。虽然这个系统很容易学习，但它不能类化到新的情境中去，实现举一反三。然而，这种泛化正是学习的关键。一台机器能够识别它之前看过的画面，或者赢得它曾经下过的围棋游戏，但这有什么意义呢？显然，真正的目标是它可以识别任何一张照片、战胜任何一名围棋玩家，无论情境是熟悉的还是陌生的。

当然，计算机科学家正在研究这些问题的各种解决方案。简化模型是既能加速学习又能提高泛化能力的最有效的干预措施之一。只要需要调整的参数数量减少，就可以迫使系统寻找更普遍的解决方案。这是杨立昆发明卷积神经网络的关键灵感，卷积神经网络是一种人工智能学习设备，在图像识别领域中应用颇广。9他的想法很简单，为了识别图片中的物体，需要在图片的各个位置进行同样的加工。例如，要识别一张照片中的所有人脸，就应该对图片的每个部分应用相同的算法，包括寻找椭圆形、一双眼睛等。没有必要在视网膜的每个点上学习不同的模型，在一个地方学到的东西应该可以在其他地方重复使用才对。

在学习过程中，杨立昆的卷积神经网络将它们从给定区域学到的东西应用到了整个网络的每个层级，以及更大的范围。因此，需要学习的参数数量就少了很多。总的来说，系统只需调整它应用于所有地方的单个过滤器，而无需为图像中的每个位置调整过多不同的联结。这个简单的技巧极大地提高了图像识别系统的性能，特别是对新图像的泛化。原因很简单，在一张新图像上运行的算法得益于它从所见过的每一张照片的每个点上获得的庞大经验。泛化还加快了学习的速度，因为机器只需要探索视觉模型的一个子集即可。在学习之前，它已经知道关于这个世界的一个重要原则：相同的对象可以出现在图像的任何位置。

此技巧可推广到许多其他领域。比如，要识别语音，就必须对说话人的具体声音进行抽象化。实现方法是通过强制人工神经网络在不同声音频段使用相同的联结，不管这个声音是低还是高。减少必须调整的参数数量有两个好处，即加快学习速度和更好地对新语音的泛化。这就是你的智能手机能够响应你的声音的原因。