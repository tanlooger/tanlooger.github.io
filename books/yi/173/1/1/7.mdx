---
---
---
title: 学习是投射先验假设
---




杨立昆的策略诠释了一个更普遍的概念：利用先天知识。卷积神经网络之所以比其他类型的神经网络学习得更好更快，是因为它不会学习所有东西。它在自己的架构中融入了一个强有力的假设，那就是在一个地方学到的东西可以推广到其他地方。

图像识别的主要问题在于它的不变性：我要识别一张脸，不管它的位置和大小，也不管它移动到了右边或左边、更远或更近。这是一个挑战，但同时也是一个非常严格的限制，即我可以期待同样的线索帮助我在空间的任何地方识别出一张脸。通过在任何地方复制相同的算法，卷积神经网络有效地利用了这一限制，并将其集成到了自己的结构中。在开始学习之前，系统就已经“知道”了视觉世界的关键属性。它无须学习不变性，而是将其作为先验假设，并以此来缩小学习空间，真的很聪明！

这里我想说的是，先天和后天本不应该对立。纯粹的、没有任何先天限制的情况下的学习是根本不存在的。任何学习的算法都或多或少包含着一些关于要学习的领域的先验假设。与其试图从头开始学习所有东西，不如依赖先验假设，这些假设清楚地描述了必须探索的领域的基本规律，并将这些规律集成到系统的体系结构中，这要高效得多。先验假设越多，学习速度就越快（当然，前提是这些假设是正确的），这是普遍真理。如果你认为AlphaGo程序是从零开始训练自己跟自己下围棋的，那就大错特错了。它最初的表征包括棋盘的形貌和对称性，将搜索空间缩小到了原有的1/8。

我们的脑也是由各种各样的假设塑造的。我们在后面将看到，婴儿出生时，脑已经井然有序，而且储备了渊博的知识。他们隐约地知道，物体只有在被推动时才会移动、固体之间不会相互穿透，他们还知道世界上有许多陌生的实体，如会说话和移动的人。这些知识不需要学习，因为它们在人类生活的任何地方都是如此，我们的基因组将它们作为固定回路置入脑，从而规范和加快了学习。婴儿不必学习世界的一切，因为他们的脑中充满了先天的规范，只有无法预测的具体参数（如脸型、虹膜颜色、语调和个人品位）才有待后天习得。

再说一次，先天和后天不必对立起来。婴儿的脑之所以知道人和无生命物体的区别，那是因为它已经学会了这一点。不是在出生的最初几天学会的，而是在数百万年的进化过程中学会的。自然选择实际上是一种学习算法，一个令人难以置信的强大程序，它已经运行了数亿年，置入了数十亿台学习机器（即曾经存在过的每一种生物）。10我们是这个深不可测的智慧的继承者。通过进化的试验和失败，我们的基因组已经将我们之前几代人的知识内化。这种与生俱来的知识与我们在有生之年学到的具体事实是不同的，它更抽象，因为它使我们的神经网络偏向于尊重基本的自然规律。

简而言之，在怀孕期间，我们的基因奠定了脑架构的基础，通过对探索的空间大小施加限制来指导和加速随后的学习。在计算机科学术语中，我们可以说基因预置了脑的“超参数”：指定层数、神经元类型、它们相互联结的一般形状、它们是否在视网膜上的任何一点复制等高水平变量。因为这些变量中的许多都储存在我们的基因组中，我们不再需要学习它们：我们的物种在进化过程中已将它们内化。

因此，我们的脑不是简单被动地接受感官输入。从一开始，它就已经拥有了一套抽象的假设，一套通过达尔文进化算法筛选积累起来的智慧，现在它把这些智慧投射到外部世界。并非所有的科学家都同意这一观点，但我认为这是一个中心论点：当今许多人工神经网络背后的幼稚经验主义哲学是错误的。他们认为我们的脑回路生来就是杂乱无章的，没有任何知识，这些回路之后会受到环境的影响，这根本是不正确的。在人和机器中，学习总是从一组先验假设开始，这些假设被投射到输入的数据上，系统从这些假设中选择最适合当前环境的假设。正如让-皮埃尔·尚热（Jean-Pierre Changeux）在他1985年出版的畅销书《神经人》（Neuronal Man）中所说：“学习就是消除不对的信息。”