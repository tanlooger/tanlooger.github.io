---
---
---
title: 学习就是将错误降到最低
---




被我们称为“人工神经网络”的计算机算法，其灵感源自大脑皮层级性组织。它有着和大脑皮层一样的金字塔般的连续层级结构，每一层都具有比前一层更深层次的规律。这些连续的层级以越来越深的方式处理输入的数据，因此它们也被称为“深度网络”。每一层级本身只能侦察外部世界的极其微小的一部分，比如在数学家们经常谈论的一个线性分类的问题中，每1个神经元只能将数据分成A和B两类，通过画一条直线串起它们。然而，当你将各个层级组合在一起时，你就会得到一个非常强大的学习工具，它能够侦察复杂的结构并通过调节自己以处理各种不同的问题。从这个意义上说，计算机芯片的进步使得今天的人工神经网络变成了深度网络，它们包含了几十个连续的层级。而距离感官输入越远的层级，其洞察力就越强，识别抽象本质的能力也就越强。

让我们来看一下法国卷积神经网络之父杨立昆（Yann LeCun）创造的LeNet算法（见彩图2）2。早在20世纪90年代，卷积神经网络在识别手写字符方面就取得了非凡的成绩。多年来，加拿大邮政部门用它实现了手写邮政编码的智能化处理。那它是如何工作的呢？该算法将书面字符以像素影像的形式输入其中，再以10个可能数字或26个可能字母中的最佳选择输出。这个人工神经网络包含了许多联结在一起的层级结构，类似我们脑的神经网络。第一层级直接与影像相连，应用简单的过滤器识别直线和曲线。更高的层级则包含更广泛和更复杂的过滤器，可以识别图像中越来越多的构成，比如：2的曲线、O的环、Z的平行线……如此直至输出层。LeNet算法可以对字符做出反应，但它不管位置、字体或大小写，只管神经元之间的联结，而且这些过程是自动化的。一旦这些联结被算法自动调整，每个神经元就会用它的过滤器去界定要处理的内容。这就是为什么一个神经元对数字2有反应而另一个对数字3有反应。

彩图2

学习是找出当下问题所适合的表征层级，在卷积神经网络LeNet算法中，它通过调整每个层级的几百万个参数来辨识真实世界的一部分。在最低的层级，神经元只对最基本的特质敏感，如线条、方向和质地等。当顺着层级逐次延伸向上，神经元就会对比较复杂的形状做出响应，包括房子、眼睛和昆虫等。



那么，这数百万个联结是如何调整的呢？它就像前面提过的棱镜的例子一样。在每一次试验中，网络都会给出一个是否出错的反馈，它再尝试调整参数，以便在下一次试验中避免同样的错误。每一个错误的答案都提供了有价值的信息。通过类似前文提到的手太靠右或太靠左这样的正负反馈，系统会逐步确认怎么做才能成功。通过回溯错误的根源，机器会校正参数的设置，以避免错误的发生。

让我们再来看看猎人是如何调整步枪瞄准镜的，这个过程简单原始。猎人射击，发现他瞄准的方向距目标向右偏离了5厘米，由此他获得了关于误差幅度（5厘米）和误差方向（向右偏）的重要信息。这些信息给了他调整步枪瞄准镜的方向。只要他稍微聪明一点儿，就能推断出修正方向：子弹偏右了，他应该把瞄准镜向左偏移一点儿。他也可以随意调整，看看把瞄准镜向右调时偏差是增加还是减少。以这种方式反复试错，猎人就会逐渐发现减小目标和实际射击点之间的差距的规律。为了最大限度地提高准度，猎人在调整瞄准镜的过程中不自知地应用了一种学习算法，他的脑计算了数学家所说的系统的“导数”或“梯度”，即“梯度下降算法”（gradient descent algorithm）：将步枪的瞄准镜朝最有效的方向移动，以降低出错的概率。

尽管现代人工智能使用的人工神经网络大多有数百万的输入、输出和可调整的参数，但其运行模式跟前面提到的猎人一样：先观察到自己的错误，然后用错误校正内在模式，使之朝着它们认为错误减少的方向发展。在许多情况下，这样的学习受到严密的监控。我们明确地告诉人工神经网络它应该在输出端激活哪个答案（比如“这是1，而不是7”），我们也很清楚如果参数引发错误，应该朝哪个方向调整它们（通过数学计算，可以准确地知道当人工神经网络频繁地将数字1激活输出为7时，应该修改哪些联结）。在机器学习的术语中，这种情况被称为“监督学习”（人被看作机器的监督者，知道机器必须给出的正确答案）和“误差反向传播”（错误信号会被发送回人工神经网络得以修改其参数）。

程序很简单：我试着回答，被告知我的答案是错误的，我测算错误偏差，然后调整参数以纠正错误，逐步调整，每一步只往正确的方向上做一点小小的修正。这就是为什么计算机的学习会很慢：学习一项复杂的活动，比如玩“俄罗斯方块”，计算机需要运行这套程序数千次、数百万次，甚至数十亿次才能学会。在一个包含大量可调参数的空间中，计算机可能需要很长时间才能找到每个“螺母”和“螺栓”的最佳设置。

最早的人工神经网络出现在20世纪80年代，那时的网络就已经按照这种逐步纠错的规则运行了。现在，计算技术的进步已经将这一运行规则扩展到了包含数亿个可调联结的深度神经网络。这些深度神经网络由一系列连续的层级组成，每个层级对应处理它的问题。例如彩图2展示的LeNet系统，该系统派生于杨立昆最先提出的LeNet体系结构，杨立昆因此赢得了最知名的国际影像识别大赛的冠军。在接触数十亿张影像后，这个系统将它们分成了1 000个不同的类别，包括脸、风景、船、车、狗、昆虫、花、路标等。系统结构周边的每个层级都与真实世界的某个具体的层面相对应：低层级的单位会选择性地对线条或质地做出响应，层级越高，就会有越多的神经元对复杂的输入起反应，如对几何图形（圆形、曲线、星形……）、物体的部件（裤子口袋、车门把手、一双眼睛……），甚至对整个物体（建筑物、脸、蜘蛛……）做出响应。

梯度下降算法的研究者发现，将误差最小化这些模式对影像分类是最有用的。但是，如果同样的网络接触到书本段落或乐谱，它就要重新学习识别字母、音符或任何在新环境中出现的形状。彩图3展示了这种类型的网络如何自发调整以识别数千个手写数字。4在最低级别，数据是混合的：有些影像表面上相似，但应该加以区分，如3和8；而一些看起来完全不同的影像最终却必须放在同一个分类中，如数字8的许多不同字体（最上面的环有些是打开的，有些是闭合的）。人工神经网络在每个层级运行过程中不断抽象化，直到同种字符的所有实例被准确地分组。通过将误差降低，它发现了与手写数字识别问题最相关的表征的层级结构。事实上，仅仅通过纠正自己的错误，就能发现一整套解决周边问题的侦察方法，这是相当了不起的。

彩图3

一个高层级神经网络如何学会将我们手写的数字进行分类？这是一项困难的工作，因为每个数字可以有几百种不同的写法。在这个网络的初级层级（右下图），人工网络神经元会混淆看起来很相似的数字，如9和4。随着层级的升高，神经元可以成功地将同一个数字的图像集中在一起并区分出来。



今天，通过误差反向传播进行学习仍然是许多计算机应用程序的核心。它是驱动智能手机轻松识别声音、智能汽车快速感知行人和路标的引擎。在这方面，人脑也在使用类似的算法。但是，误差反向传播有多种形态。人工智能领域在过去30多年里取得了巨大的进步，研究人员发现了许多促进计算机学习的技巧。我们现在就来回顾它们——正如我们将看到的，它们也告诉了我们很多关于我们自身及我们的学习方式的知识。